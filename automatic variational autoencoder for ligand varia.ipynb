{"cells":[{"metadata":{},"cell_type":"markdown","source":"## 1. Introduction - Overview of problem and approach"},{"metadata":{},"cell_type":"markdown","source":"## 2. Disease and Background - Visualizing the Target"},{"metadata":{},"cell_type":"markdown","source":"## 3. Dataset - Finding relevant compounds with activities"},{"metadata":{},"cell_type":"markdown","source":"### General Dataset Preparation"},{"metadata":{"trusted":true},"cell_type":"code","source":"!conda install -c rdkit rdkit -y\n!git clone https://github.com/tmacdou4/2019-nCov.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nos.listdir(\"/kaggle/working/2019-nCov\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! grep AID /kaggle/working/2019-nCov/Data/SARS_C3_Assays.txt > /kaggle/working/2019-nCov/Data/SARS_C3_Assays_AID_only.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! sed -i 's/[^0-9]//g' /kaggle/working/2019-nCov/Data/SARS_C3_Assays_AID_only.csv","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imports\nimport rdkit\nfrom rdkit.Chem import AllChem as Chem\nfrom rdkit.DataStructs import cDataStructs\nimport numpy as np\nimport pandas as pd\nfrom rdkit.Chem.Draw import IPythonConsole\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport pickle\nimport csv\nfrom rdkit.Chem import QED\nimport random\nimport json\nfrom sklearn.preprocessing import StandardScaler","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_assays(assay_path, assay_pickle_path):\n    with open(str(assay_path)) as f:\n        r = csv.reader(f)\n        AIDs = list(r)\n    assays = []\n    for i, AID in zip(range(len(AIDs)), AIDs):\n        #This needs to be changed to \n        #os.system('curl https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/%s/sdf -o cmp.sdf' %CID)\n        #if you run it on a mac\n        os.system(f'wget https://pubchem.ncbi.nlm.nih.gov/rest/pug/assay/aid/{str(AID[0])}/csv -O Data/assay.csv')\n        if os.stat(f'/kaggle/working/2019-nCov/Data/assay.csv').st_size != 0:\n            assays.append(pd.read_csv(f'/kaggle/working/2019-nCov/Data/assay.csv'))\n\n    pickle.dump(assays, open(str(assay_pickle_path), \"wb\"))\n\ndef get_mols_for_assays(assays_no_mol_path, assays_with_mol_path):\n    assays = pickle.load(open(str(assays_no_mol_path), \"rb\"))\n    for assay in assays:\n        if len(assay) != 1:\n            cids = list(assay[['PUBCHEM_CID']].values.astype(\"int32\").squeeze())\n            nan_counter = 0\n            for i in range(len(cids)):\n                if cids[i] < 0:\n                    nan_counter += 1\n                else:\n                    break\n            cids = cids[nan_counter:]\n            mols = []\n            for CID in cids:\n                #os.system('curl https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/%s/sdf -o cmp.sdf' %CID)\n                os.system('wget https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/%s/sdf -O cmp.sdf' %CID)\n                if os.stat(f'/kaggle/working/2019-nCov/Data/cmp.sdf').st_size != 0:\n                    mols.append(Chem.SDMolSupplier(\"/kaggle/working/2019-nCov/Data/cmp.sdf\")[0])\n                else:\n                    mols.append(None)\n\n            for i in range(nan_counter):\n                mols.insert(0,None)\n\n            assay.insert(3, \"Mol Object\", mols)\n\n    pickle.dump(assays, open(str(assays_with_mol_path), \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_assays(\"/kaggle/working/2019-nCov/Data/SARS_C3_Assays_AID_only.csv\", \n           \"/kaggle/working/2019-nCov/Data/sars/sars_assays_no_mol.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"get_mols_for_assays(\"/kaggle/working/2019-nCov/Data/sars/sars_assays_no_mol.pkl\",\n                    \"/kaggle/working/2019-nCov/Data/sars/sars_assays.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This goes an HTTP get on EVERY compound and takes a WHILE. Might be better to just use the pickled datasets\n# get_assays_no_mol(\"/kaggle/working/2019-nCov/Data/MERS_Protease_Assays_AID_only.csv\",\n#                   \"/kaggle/working/2019-nCov/Data/mers/mers_assays_no_mol.pkl\")\n# get_mols_for_assay(\"/kaggle/working/2019-nCov/Data/mers/mers_assays_no_mol.pkl\", \n#                    \"/kaggle/working/2019-nCov/Data/mers/mers_assays.pkl\")\n# get_assays_no_mol(\"/kaggle/working/2019-nCov/Data/NS3_Protease_Assays_AID_only.csv\", \n#                   \"/kaggle/working/2019-nCov/Data/ns3/ns3_assays_no_mol.pkl\")\n# get_mols_for_assay(\"/kaggle/working/2019-nCov/Data/ns3/ns3_assays_no_mol.pkl\",\n#                    \"/kaggle/working/2019-nCov/Data/ns3/ns3_assays.pkl\")\n# get_assays_no_mol(\"/kaggle/working/2019-nCov/Data/HIV_Protease_Assays_AID_only.csv\",\n#                   \"/kaggle/working/2019-nCov/Data/hiv/hiv_assays_no_mol.pkl\")\n# get_mols_for_assay(\"/kaggle/working/2019-nCov/Data/hiv/hiv_assays_no_mol.pkl\", \n#                    \"/kaggle/working/2019-nCov/Data/hiv/hiv_assays.pkl\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#This datastructure is a dictionary of lists of dataframe. \nassays = {}\nassays[\"sars\"] = pickle.load(open(\"/kaggle/working/2019-nCov/Data/sars/sars_assays.pkl\", \"rb\"))\nassays[\"mers\"] = pickle.load(open(\"/kaggle/working/2019-nCov/Data/mers/mers_assays.pkl\", \"rb\"))\nassays[\"ns3\"] = pickle.load(open(\"/kaggle/working/2019-nCov/Data/ns3/ns3_assays.pkl\", \"rb\"))\nassays[\"hiv\"] = pickle.load(open(\"/kaggle/working/2019-nCov/Data/hiv/hiv_assays.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It is worth mentioning here the different kinds of Bioactivities that an assay can report. Depending on what was relevant to the scientists involved in the study, various values can be used. Possibly most importantly for generating this dataset though is to not confuse the different kinds of activities. We will focus on IC50, which is the concentration of the compound at which 50% inhibition is observed. The value is normal reported as a \"Micromolar concentration\". The lower the value, the better the compound is at inhibiting the protein. It is important to not be tempted to use the \"Activity\" reported in some assays, which is normally a % and corresponds to how much that compound inhibits the protein at a given concentration. We're sticking with IC50 because this value is very information rich and actually many \"Activity\" experiments go into producing 1 IC50 value. Also they are more easily comparable, as we don't need to standardize concentration across the assays."},{"metadata":{},"cell_type":"markdown","source":"For this report we will focus on the \"PubChem Standard Value\" which is normally a standardized value using some metric (we will further narrow to only the metrics we want)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #This removes all the assays that do not have a column called \"PubChem Standard Value\"\n# for a in [\"sars\", \"mers\", \"ns3\", \"hiv\"]:\n#     print(\"Length of\",str(a),\"before removing\")\n#     print(len(assays[a]))\n#     assays[a] = np.array(assays[a])\n#     bad_list = []\n#     good_list = []\n#     for i in range(len(assays[a])):\n#         ic50_cols = [col for col in assays[a][i].columns if 'PubChem Standard Value' in col]\n#         if not ic50_cols:\n#             bad_list.append(i)\n#         else:\n#             good_list.append(int(i))\n\n#     bad_list = np.array(bad_list)\n#     good_list = np.array(good_list, dtype='int32')\n\n#     assays[a] = assays[a][good_list]\n#     print(\"Length of\",str(a),\"after removing\")\n#     print(len(assays[a]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Remove unnesessary columns\n# for a in [\"sars\", \"mers\", \"ns3\", \"hiv\"]:\n#     for i in range(len(assays[a])):\n#         assays[a][i] = assays[a][i][[\"Mol Object\", \"PubChem Standard Value\", \"Standard Type\"]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Look at what different kind of metrics were used\n# for a in [\"sars\", \"mers\", \"ns3\", \"hiv\"]:\n#     for i in range(len(assays[a])):\n#         print(assays[a][i][[\"Standard Type\"]].values[-1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #concatenate all of the dataframe in the dictionary into a single list.\n# #We lose the notion that they were once for different targets\n# all_dfs = []\n# for a in [\"sars\", \"mers\", \"ns3\", \"hiv\"]:\n#     for i in range(len(assays[a])):\n#         if assays[a][i][[\"Standard Type\"]].values[-1][0] in {\"IC50\", \"Ki\", \"Kd\", \"IC90\"}:\n#             all_dfs.append(assays[a][i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Remove header info and concatenate them\n# for i in range(len(all_dfs)):\n#     all_dfs[i] = all_dfs[i].iloc[4:]\n# final_df = pd.concat(all_dfs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Take only the compounds with activites below 0.1 (all will be relatively active)\n# final_df['PubChem Standard Value'] = final_df['PubChem Standard Value'].astype(float)\n# df = final_df[final_df[\"PubChem Standard Value\"] < 0.1]\n\n# pickle.dump(df, open(\"/kaggle/working/2019-nCov/Data/final_df.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Method Specific-preparation"},{"metadata":{},"cell_type":"markdown","source":"Now moving on to preparing the dataset for use in the predictive model as well as the generative model"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pickle.load(open(\"/kaggle/working/2019-nCov/Data/final_df.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.insert(3, 'smiles', [Chem.MolToSmiles(x) for x in df[['Mol Object']].values[:,0]], True)\ndf.insert(4, 'qed', [QED.qed(x) for x in df[['Mol Object']].values[:,0]], True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"salt_indexes = []\nfor i in range(len(df)):\n    if \".\" in df[[\"smiles\"]].values[i][0]:\n        salt_indexes.append(i)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df.reset_index()\ndf = df.drop(df.index[salt_indexes])\n\n#This is the format for the generative model, the cgvae\n#smiles string and QED values, with validation id's defined in a separate json file\ndf[[\"smiles\", \"qed\"]].to_csv(\"/kaggle/working/2019-nCov/Data/250k_rndm_zinc_drugs_clean_3.csv\", index=False)\nnew_valid_idx = random.sample(range(len(df)), int(len(df)*0.1))\nnew_valid_idx.sort()\nwith open(\"/kaggle/working/2019-nCov/Data/valid_idx_zinc.json\", 'w') as f:\n    json.dump(new_valid_idx, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#The values are very small, so it's more effective to work in log-space\ndf.insert(2, 'log_std', [-np.log10(x) for x in df[['PubChem Standard Value']].values[:,0]], True)\n#plt.hist(df[['log_std']].values[:,0], bins=25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Scale the values to have 0 mean and unit variance\nscaler = StandardScaler()\nscaler.fit(df[['log_std']].values[:,0].reshape(-1, 1))\ndf.insert(2, 'log_std_scaled', scaler.transform(df[['log_std']].values[:,0].reshape(-1, 1)), True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Format the data for the predictive model\ndata = df[[\"smiles\", \"log_std_scaled\"]].values\nnp.random.shuffle(data)\ntrain, valid, test = np.split(data, [int(.8*data.shape[0]), int(.9*data.shape[0])])\ntrain = np.insert(train, 0, [None]*train.shape[0], 1)\nvalid = np.insert(valid, 0, [None]*valid.shape[0], 1)\ntest = np.insert(test, 0, [None]*test.shape[0], 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the data for the predicitive model\npd.DataFrame(train).to_csv(\"/kaggle/working/2019-nCov/Data/protease_train.csv.gz\",\n                           index=False, compression='gzip', sep='\\t')\npd.DataFrame(valid).to_csv(\"/kaggle/working/2019-nCov/Data/protease_valid.csv.gz\",\n                           index=False, compression='gzip', sep='\\t')\npd.DataFrame(test).to_csv(\"/kaggle/working/2019-nCov/Data/protease_test.csv.gz\",\n                          index=False, compression='gzip', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 4. Bioactivity Prediction - Edge Memory Neural Network"},{"metadata":{},"cell_type":"markdown","source":"This approach is detailed in the paper: \"Building Attention and Edge Convolution Neural Networks for Bioactivity and Physical-Chemical Property Prediction\" available here: https://chemrxiv.org/articles/Building_Attention_and_Edge_Convolution_Neural_Networks_for_Bioactivity_and_Physical-Chemical_Property_Prediction/9873599 and with code available here: https://github.com/edvardlindelof/graph-neural-networks-for-drug-discovery. Other than moving it all to this notebook, the code is only lightly modified coming from that repo"},{"metadata":{},"cell_type":"markdown","source":"They describe efforts to make several new network architectures to better learn from chemical graph data. Their implementation is written using pytorch and i'm got it working with the requirements described in the install.sh script."},{"metadata":{},"cell_type":"markdown","source":"Outline of this section: First we train the model using the dataset obtained in section 3. Then we can use the trained model to look at commercially available libraries of molecules where it would be impossible to do docking studies on each compound. In this way, the machine learning model serves to \"thin the herd\" of potential compounds so that we can identify candidates for docking studies."},{"metadata":{},"cell_type":"markdown","source":"### Training the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"##################  aggregation.py\nimport torch\nfrom torch import nn\n\n\nclass AggregationMPNN(nn.Module):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features):\n        super(AggregationMPNN, self).__init__()\n        self.node_features = node_features\n        self.edge_features = edge_features\n        self.message_size = message_size\n        self.message_passes = message_passes\n        self.out_features = out_features\n\n    # nodes (total number of nodes in batch, number of features)\n    # node_neighbours (total number of nodes in batch, max node degree, number of features)\n    # node_neighbours (total number of nodes in batch, max node degree, number of edge features)\n    # mask (total number of nodes in batch, max node degree) elements are 1 if corresponding neighbour exist\n    def aggregate_message(self, nodes, node_neighbours, edges, mask):\n        raise NotImplementedError\n\n    # inputs are \"batches\" of shape (maximum number of nodes in batch, number of features)\n    def update(self, nodes, messages):\n        raise NotImplementedError\n\n    # inputs are \"batches\" of same shape as the nodes passed to update\n    # node_mask is same shape as inputs and is 1 if elements corresponding exists, otherwise 0\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        raise NotImplementedError\n\n    def forward(self, adjacency, nodes, edges):\n        edge_batch_batch_indices, edge_batch_node_indices, edge_batch_neighbour_indices = adjacency.nonzero().unbind(-1)\n\n        node_batch_batch_indices, node_batch_node_indices = adjacency.sum(-1).nonzero().unbind(-1)\n        node_batch_adj = adjacency[node_batch_batch_indices, node_batch_node_indices, :]\n\n        node_batch_size = node_batch_batch_indices.shape[0]\n        node_degrees = node_batch_adj.sum(-1).long()\n        max_node_degree = node_degrees.max()\n        node_batch_node_neighbours = torch.zeros(node_batch_size, max_node_degree, self.node_features)\n        node_batch_edges = torch.zeros(node_batch_size, max_node_degree, self.edge_features)\n\n        node_batch_neighbour_neighbour_indices = torch.cat([torch.arange(i) for i in node_degrees])\n\n        edge_batch_node_batch_indices = torch.cat(\n            [i * torch.ones(degree) for i, degree in enumerate(node_degrees)]\n        ).long()\n\n        node_batch_node_neighbour_mask = torch.zeros(node_batch_size, max_node_degree)\n\n        if next(self.parameters()).is_cuda:\n            node_batch_node_neighbours = node_batch_node_neighbours.cuda()\n            node_batch_edges = node_batch_edges.cuda()\n            node_batch_neighbour_neighbour_indices = node_batch_neighbour_neighbour_indices.cuda()\n            edge_batch_node_batch_indices = edge_batch_node_batch_indices.cuda()\n            node_batch_node_neighbour_mask = node_batch_node_neighbour_mask.cuda()\n\n        node_batch_node_neighbour_mask[edge_batch_node_batch_indices, node_batch_neighbour_neighbour_indices] = 1\n\n        node_batch_edges[edge_batch_node_batch_indices, node_batch_neighbour_neighbour_indices, :] = \\\n            edges[edge_batch_batch_indices, edge_batch_node_indices, edge_batch_neighbour_indices, :]\n\n        hidden_nodes = nodes.clone()\n\n        for i in range(self.message_passes):\n            node_batch_nodes = hidden_nodes[node_batch_batch_indices, node_batch_node_indices, :]\n            node_batch_node_neighbours[edge_batch_node_batch_indices, node_batch_neighbour_neighbour_indices, :] = \\\n                hidden_nodes[edge_batch_batch_indices, edge_batch_neighbour_indices, :]\n\n            messages = self.aggregate_message(\n                node_batch_nodes, node_batch_node_neighbours.clone(), node_batch_edges, node_batch_node_neighbour_mask\n            )\n            hidden_nodes[node_batch_batch_indices, node_batch_node_indices, :] = self.update(node_batch_nodes, messages)\n\n        node_mask = (adjacency.sum(-1) != 0)  # .unsqueeze(-1).expand_as(nodes)\n        output = self.readout(hidden_nodes, nodes, node_mask)\n        return output\n\n\n#############  aggregation_mpnn_implementation.py\n\n\nclass AttentionENNS2V(AggregationMPNN):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features,\n                 enn_depth=3, enn_hidden_dim=200, enn_dropout_p=0,\n                 att_depth=3, att_hidden_dim=200, att_dropout_p=0,\n                 s2v_lstm_computations=12, s2v_memory_size=50,\n                 out_depth=1, out_hidden_dim=200, out_dropout_p=0):\n        super(AttentionENNS2V, self).__init__(\n            node_features, edge_features, message_size, message_passes, out_features\n        )\n        self.enn = FeedForwardNetwork(\n            edge_features, [enn_hidden_dim] * enn_depth, node_features * message_size, dropout_p=enn_dropout_p\n        )\n        self.att_enn = FeedForwardNetwork(\n            node_features + edge_features, [att_hidden_dim] * att_depth, message_size, dropout_p=att_dropout_p\n        )\n        self.gru = nn.GRUCell(input_size=message_size, hidden_size=node_features, bias=False)\n        self.s2v = Set2Vec(node_features, s2v_lstm_computations, s2v_memory_size)\n        self.out_nn = FeedForwardNetwork(\n            s2v_memory_size * 2, [out_hidden_dim] * out_depth, out_features, dropout_p=out_dropout_p, bias=False\n        )\n\n    def aggregate_message(self, nodes, node_neighbours, edges, mask):\n        BIG_NEGATIVE = -1e6\n        max_node_degree = node_neighbours.shape[1]\n\n        enn_output = self.enn(edges)\n        matrices = enn_output.view(-1, max_node_degree, self.message_size, self.node_features)\n        message_terms = torch.matmul(matrices, node_neighbours.unsqueeze(-1)).squeeze()\n\n        att_enn_output = self.att_enn(torch.cat([edges, node_neighbours], dim=2))\n        energies = att_enn_output.view(-1, max_node_degree, self.message_size)\n        energy_mask = (1 - mask).float() * BIG_NEGATIVE\n        weights = torch.softmax(energies + energy_mask.unsqueeze(-1), dim=1)\n\n        return (weights * message_terms).sum(1)\n\n    def update(self, nodes, messages):\n        return self.gru(messages, nodes)\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        graph_embeddings = self.s2v(hidden_nodes, input_nodes, node_mask)\n        return self.out_nn(graph_embeddings)\n\n\nclass AttentionGGNN(AggregationMPNN):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features,\n                 msg_depth=4, msg_hidden_dim=200, msg_dropout_p=0.0,\n                 att_depth=3, att_hidden_dim=200, att_dropout_p=0,\n                 gather_width=100,\n                 gather_att_depth=3, gather_att_hidden_dim=100, gather_att_dropout_p=0.0,\n                 gather_emb_depth=3, gather_emb_hidden_dim=100, gather_emb_dropout_p=0.0,\n                 out_depth=2, out_hidden_dim=100, out_dropout_p=0.0, out_layer_shrinkage=1.0):\n        super(AttentionGGNN, self).__init__(node_features, edge_features, message_size, message_passes, out_features)\n\n        self.msg_nns = nn.ModuleList()\n        self.att_nns = nn.ModuleList()\n        for _ in range(edge_features):\n            self.msg_nns.append(\n                FeedForwardNetwork(node_features, [msg_hidden_dim] * msg_depth, message_size, dropout_p=msg_dropout_p,\n                                   bias=False)\n            )\n            self.att_nns.append(\n                FeedForwardNetwork(node_features, [att_hidden_dim] * att_depth, message_size, dropout_p=att_dropout_p,\n                                   bias=False)\n            )\n        self.gru = nn.GRUCell(input_size=message_size, hidden_size=node_features, bias=False)\n        self.gather = GraphGather(\n            node_features, gather_width,\n            gather_att_depth, gather_att_hidden_dim, gather_att_dropout_p,\n            gather_emb_depth, gather_emb_hidden_dim, gather_emb_dropout_p\n        )\n        out_layer_sizes = [  # example: depth 5, dim 50, shrinkage 0.5 => out_layer_sizes [50, 42, 35, 30, 25]\n            round(out_hidden_dim * (out_layer_shrinkage ** (i / (out_depth - 1 + 1e-9)))) for i in range(out_depth)\n        ]\n        self.out_nn = FeedForwardNetwork(gather_width, out_layer_sizes, out_features, dropout_p=out_dropout_p)\n\n    def aggregate_message(self, nodes, node_neighbours, edges, node_neighbour_mask):\n        energy_mask = (node_neighbour_mask == 0).float() * 1e6\n        # xxs_masked_per_edge contains (batch_size, max_n_neighbours, message_size)-shape tensors, that has 0s in all rows except\n        # the ones corresponding to the edge type indicated by the list index\n        # intuitive way of writing this involves a torch.stack along batch dimension and is immensely slow\n        embeddings_masked_per_edge = [\n            edges[:, :, i].unsqueeze(-1) * self.msg_nns[i](node_neighbours) for i in range(self.edge_features)\n        ]\n        embedding = sum(embeddings_masked_per_edge)\n        energies_masked_per_edge = [\n            edges[:, :, i].unsqueeze(-1) * self.att_nns[i](node_neighbours) for i in range(self.edge_features)\n        ]\n        energies = sum(energies_masked_per_edge) - energy_mask.unsqueeze(-1)\n        attention = torch.softmax(energies, dim=1)\n        return torch.sum(attention * embedding, dim=1)\n\n    def update(self, nodes, messages):\n        return self.gru(messages, nodes)\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        graph_embeddings = self.gather(hidden_nodes, input_nodes, node_mask)\n        return self.out_nn(graph_embeddings)\n\n\n################  emn.py\n\nclass EMN(nn.Module):\n\n    def __init__(self, edge_features, edge_embedding_size, message_passes, out_features):\n        super(EMN, self).__init__()\n        self.edge_features = edge_features\n        self.edge_embedding_size = edge_embedding_size\n        self.message_passes = message_passes\n        self.out_features = out_features\n\n    def preprocess_edges(self, nodes, node_neighbours, edges):\n        raise NotImplementedError\n\n    # (total number of edges in batch, edge_features) and (total number of edges in batch, max_node_degree, edge_features)\n    def propagate_edges(self, edges, ingoing_edge_memories, ingoing_edges_mask):\n        raise NotImplementedError\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        raise NotImplementedError\n\n    # adjacency (N, n_nodes, n_nodes); edges (N, n_nodes, n_nodes, edge_features)\n    def forward(self, adjacency, nodes, edges):\n        # indices for finding edges in batch\n        edges_b_idx, edges_n_idx, edges_nhb_idx = adjacency.nonzero().unbind(-1)\n\n        n_edges = edges_n_idx.shape[0]\n        adj_of_edge_batch_indices = adjacency.clone().long()\n        r = torch.arange(n_edges) + 1  # +1 to distinguish the index 0 from 'empty' elements, subtracted few lines down\n        if next(self.parameters()).is_cuda:\n            r = r.cuda()\n        adj_of_edge_batch_indices[edges_b_idx, edges_n_idx, edges_nhb_idx] = r\n\n        ingoing_edges_eb_idx = (torch.cat([\n            row[row.nonzero()] for row in adj_of_edge_batch_indices[edges_b_idx, edges_nhb_idx, :]\n        ]) - 1).squeeze()\n\n        edge_degrees = adjacency[edges_b_idx, edges_nhb_idx, :].sum(-1).long()\n        ingoing_edges_igeb_idx = torch.cat([i * torch.ones(d) for i, d in enumerate(edge_degrees)]).long()\n        ingoing_edges_ige_idx = torch.cat([torch.arange(i) for i in edge_degrees]).long()\n\n        batch_size = adjacency.shape[0]\n        n_nodes = adjacency.shape[1]\n        max_node_degree = adjacency.sum(-1).max().int()\n        edge_memories = torch.zeros(n_edges, self.edge_embedding_size)\n        ingoing_edge_memories = torch.zeros(n_edges, max_node_degree, self.edge_embedding_size)\n        ingoing_edges_mask = torch.zeros(n_edges, max_node_degree)\n        if next(self.parameters()).is_cuda:\n            edge_memories = edge_memories.cuda()\n            ingoing_edge_memories = ingoing_edge_memories.cuda()\n            ingoing_edges_mask = ingoing_edges_mask.cuda()\n\n        edge_batch_nodes = nodes[edges_b_idx, edges_n_idx, :]\n        edge_batch_neighbours = nodes[edges_b_idx, edges_nhb_idx, :]\n        edge_batch_edges = edges[edges_b_idx, edges_n_idx, edges_nhb_idx, :]\n        edge_batch_edges = self.preprocess_edges(edge_batch_nodes, edge_batch_neighbours, edge_batch_edges)\n\n        # remove h_ji:s influence on h_ij\n        ingoing_edges_nhb_idx = edges_nhb_idx[ingoing_edges_eb_idx]\n        ingoing_edges_receiving_edge_n_idx = edges_n_idx[ingoing_edges_igeb_idx]\n        not_same_idx = (ingoing_edges_receiving_edge_n_idx != ingoing_edges_nhb_idx).nonzero()\n        ingoing_edges_eb_idx = ingoing_edges_eb_idx[not_same_idx].squeeze()\n        ingoing_edges_ige_idx = ingoing_edges_ige_idx[not_same_idx].squeeze()\n        ingoing_edges_igeb_idx = ingoing_edges_igeb_idx[not_same_idx].squeeze()\n\n        ingoing_edges_mask[ingoing_edges_igeb_idx, ingoing_edges_ige_idx] = 1\n\n        for i in range(self.message_passes):\n            ingoing_edge_memories[ingoing_edges_igeb_idx, ingoing_edges_ige_idx, :] = \\\n                edge_memories[ingoing_edges_eb_idx, :]\n            edge_memories = self.propagate_edges(edge_batch_edges, ingoing_edge_memories.clone(), ingoing_edges_mask)\n\n        node_mask = (adjacency.sum(-1) != 0)\n\n        node_sets = torch.zeros(batch_size, n_nodes, max_node_degree, self.edge_embedding_size)\n        if next(self.parameters()).is_cuda:\n            node_sets = node_sets.cuda()\n\n        edge_batch_edge_memory_indices = torch.cat(\n            [torch.arange(row.sum()) for row in adjacency.view(-1, n_nodes)]\n        ).long()\n\n        node_sets[edges_b_idx, edges_n_idx, edge_batch_edge_memory_indices, :] = edge_memories\n        graph_sets = node_sets.sum(2)\n        output = self.readout(graph_sets, graph_sets, node_mask)\n\n        return output\n\n\n#####################  emn_implementation.py\n\nclass EMNImplementation(EMN):\n\n    def __init__(self, node_features, edge_features, message_passes, out_features,\n                 edge_embedding_size,\n                 edge_emb_depth=3, edge_emb_hidden_dim=150, edge_emb_dropout_p=0.0,\n                 att_depth=3, att_hidden_dim=80, att_dropout_p=0.0,\n                 msg_depth=3, msg_hidden_dim=80, msg_dropout_p=0.0,\n                 gather_width=100,\n                 gather_att_depth=3, gather_att_hidden_dim=100, gather_att_dropout_p=0.0,\n                 gather_emb_depth=3, gather_emb_hidden_dim=100, gather_emb_dropout_p=0.0,\n                 out_depth=2, out_hidden_dim=100, out_dropout_p=0, out_layer_shrinkage=1.0):\n        super(EMNImplementation, self).__init__(\n            edge_features, edge_embedding_size, message_passes, out_features\n        )\n        self.embedding_nn = FeedForwardNetwork(\n            node_features * 2 + edge_features, [edge_emb_hidden_dim] * edge_emb_depth, edge_embedding_size,\n            dropout_p=edge_emb_dropout_p\n        )\n\n        self.emb_msg_nn = FeedForwardNetwork(\n            edge_embedding_size, [msg_hidden_dim] * msg_depth, edge_embedding_size, dropout_p=msg_dropout_p\n        )\n        self.att_msg_nn = FeedForwardNetwork(\n            edge_embedding_size, [att_hidden_dim] * att_depth, edge_embedding_size, dropout_p=att_dropout_p\n        )\n\n        # self.extra_gru_layer = nn.Linear(edge_embedding_size, edge_embedding_size, bias=False)\n        self.gru = nn.GRUCell(edge_embedding_size, edge_embedding_size, bias=False)\n        self.gather = GraphGather(\n            edge_embedding_size, gather_width,\n            gather_att_depth, gather_att_hidden_dim, gather_att_dropout_p,\n            gather_emb_depth, gather_emb_hidden_dim, gather_emb_dropout_p\n        )\n        out_layer_sizes = [  # example: depth 5, dim 50, shrinkage 0.5 => out_layer_sizes [50, 42, 35, 30, 25]\n            round(out_hidden_dim * (out_layer_shrinkage ** (i / (out_depth - 1 + 1e-9)))) for i in range(out_depth)\n        ]\n        self.out_nn = FeedForwardNetwork(gather_width, out_layer_sizes, out_features, dropout_p=out_dropout_p)\n\n    def preprocess_edges(self, nodes, node_neighbours, edges):\n        cat = torch.cat([nodes, node_neighbours, edges], dim=1)\n        return torch.tanh(self.embedding_nn(cat))\n\n    def propagate_edges(self, edges, ingoing_edge_memories, ingoing_edges_mask):\n        BIG_NEGATIVE = -1e6\n        energy_mask = ((1 - ingoing_edges_mask).float() * BIG_NEGATIVE).unsqueeze(-1)\n\n        cat = torch.cat([edges.unsqueeze(1), ingoing_edge_memories], dim=1)\n        embeddings = self.emb_msg_nn(cat)\n\n        edge_energy = self.att_msg_nn(edges)\n        ing_memory_energies = self.att_msg_nn(ingoing_edge_memories) + energy_mask\n        energies = torch.cat([edge_energy.unsqueeze(1), ing_memory_energies], dim=1)\n        attention = torch.softmax(energies, dim=1)\n\n        # set aggregation of the set of the given edge feature and ingoing edge memories\n        message = (attention * embeddings).sum(dim=1)\n        return self.gru(message)  # returning hidden state but it is also set internally I think.. hm\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        graph_embeddings = self.gather(hidden_nodes, input_nodes, node_mask)\n        return self.out_nn(graph_embeddings)\n\n\n################  graphh_features.py\n\n# this is deepchems source file with unused stuff removed, for faster import\nimport numpy as np\nfrom rdkit import Chem\n\n\ndef one_of_k_encoding(x, allowable_set):\n    if x not in allowable_set:\n        raise Exception(\"input {0} not in allowable set{1}:\".format(\n            x, allowable_set))\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef one_of_k_encoding_unk(x, allowable_set):\n    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n    if x not in allowable_set:\n        x = allowable_set[-1]\n    return list(map(lambda s: x == s, allowable_set))\n\n\ndef get_intervals(l):\n    \"\"\"For list of lists, gets the cumulative products of the lengths\"\"\"\n    intervals = len(l) * [0]\n    # Initalize with 1\n    intervals[0] = 1\n    for k in range(1, len(l)):\n        intervals[k] = (len(l[k]) + 1) * intervals[k - 1]\n\n    return intervals\n\n\ndef safe_index(l, e):\n    \"\"\"Gets the index of e in l, providing an index of len(l) if not found\"\"\"\n    try:\n        return l.index(e)\n    except:\n        return len(l)\n\n\npossible_atom_list = [\n    'C', 'N', 'O', 'S', 'F', 'P', 'Cl', 'Mg', 'Na', 'Br', 'Fe', 'Ca', 'Cu',\n    'Mc', 'Pd', 'Pb', 'K', 'I', 'Al', 'Ni', 'Mn'\n]\npossible_numH_list = [0, 1, 2, 3, 4]\npossible_valence_list = [0, 1, 2, 3, 4, 5, 6]\npossible_formal_charge_list = [-3, -2, -1, 0, 1, 2, 3]\npossible_hybridization_list = [\n    Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n    Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n    Chem.rdchem.HybridizationType.SP3D2\n]\npossible_number_radical_e_list = [0, 1, 2]\npossible_chirality_list = ['R', 'S']\n\nreference_lists = [\n    possible_atom_list, possible_numH_list, possible_valence_list,\n    possible_formal_charge_list, possible_number_radical_e_list,\n    possible_hybridization_list, possible_chirality_list\n]\n\nintervals = get_intervals(reference_lists)\n\n\ndef get_feature_list(atom):\n    features = 6 * [0]\n    features[0] = safe_index(possible_atom_list, atom.GetSymbol())\n    features[1] = safe_index(possible_numH_list, atom.GetTotalNumHs())\n    features[2] = safe_index(possible_valence_list, atom.GetImplicitValence())\n    features[3] = safe_index(possible_formal_charge_list, atom.GetFormalCharge())\n    features[4] = safe_index(possible_number_radical_e_list,\n                             atom.GetNumRadicalElectrons())\n    features[5] = safe_index(possible_hybridization_list, atom.GetHybridization())\n    return features\n\n\ndef features_to_id(features, intervals):\n    \"\"\"Convert list of features into index using spacings provided in intervals\"\"\"\n    id = 0\n    for k in range(len(intervals)):\n        id += features[k] * intervals[k]\n\n    # Allow 0 index to correspond to null molecule 1\n    id = id + 1\n    return id\n\n\ndef atom_to_id(atom):\n    \"\"\"Return a unique id corresponding to the atom type\"\"\"\n    features = get_feature_list(atom)\n    return features_to_id(features, intervals)\n\n\ndef atom_features(atom,\n                  bool_id_feat=False,\n                  explicit_H=False,\n                  use_chirality=False):\n    if bool_id_feat:\n        return np.array([atom_to_id(atom)])\n    else:\n        results = one_of_k_encoding_unk(\n            atom.GetSymbol(),\n            [\n                'C',\n                'N',\n                'O',\n                'S',\n                'F',\n                'Si',\n                'P',\n                'Cl',\n                'Br',\n                'Mg',\n                'Na',\n                'Ca',\n                'Fe',\n                'As',\n                'Al',\n                'I',\n                'B',\n                'V',\n                'K',\n                'Tl',\n                'Yb',\n                'Sb',\n                'Sn',\n                'Ag',\n                'Pd',\n                'Co',\n                'Se',\n                'Ti',\n                'Zn',\n                'H',  # H?\n                'Li',\n                'Ge',\n                'Cu',\n                'Au',\n                'Ni',\n                'Cd',\n                'In',\n                'Mn',\n                'Zr',\n                'Cr',\n                'Pt',\n                'Hg',\n                'Pb',\n                'Unknown'\n            ]) + one_of_k_encoding(atom.GetDegree(),\n                                   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]) + \\\n                  one_of_k_encoding_unk(atom.GetImplicitValence(), [0, 1, 2, 3, 4, 5, 6]) + \\\n                  [atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \\\n                  one_of_k_encoding_unk(atom.GetHybridization(), [\n                      Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n                      Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.\n                                        SP3D, Chem.rdchem.HybridizationType.SP3D2\n                  ]) + [atom.GetIsAromatic()]\n        # In case of explicit hydrogen(QM8, QM9), avoid calling `GetTotalNumHs`\n        if not explicit_H:\n            results = results + one_of_k_encoding_unk(atom.GetTotalNumHs(),\n                                                      [0, 1, 2, 3, 4])\n        if use_chirality:\n            try:\n                results = results + one_of_k_encoding_unk(\n                    atom.GetProp('_CIPCode'),\n                    ['R', 'S']) + [atom.HasProp('_ChiralityPossible')]\n            except:\n                results = results + [False, False\n                                     ] + [atom.HasProp('_ChiralityPossible')]\n\n        return np.array(results)\n\n\n####################    modules.py\nimport math\n\n\nclass GraphGather(nn.Module):\n    r\"\"\"The GGNN readout function\n    \"\"\"\n\n    def __init__(self, node_features, out_features,\n                 att_depth=2, att_hidden_dim=100, att_dropout_p=0.0,\n                 emb_depth=2, emb_hidden_dim=100, emb_dropout_p=0.0):\n        super(GraphGather, self).__init__()\n\n        # denoted i and j in GGNN, MPNN and PotentialNet papers\n        self.att_nn = FeedForwardNetwork(\n            node_features * 2, [att_hidden_dim] * att_depth, out_features, dropout_p=att_dropout_p, bias=False\n        )\n        self.emb_nn = FeedForwardNetwork(\n            node_features, [emb_hidden_dim] * emb_depth, out_features, dropout_p=emb_dropout_p, bias=False\n        )\n\n    def forward(self, hidden_nodes, input_nodes, node_mask):\n        cat = torch.cat([hidden_nodes, input_nodes], dim=2)\n        energy_mask = (node_mask == 0).float() * 1e6\n        energies = self.att_nn(cat) - energy_mask.unsqueeze(-1)\n        attention = torch.sigmoid(energies)\n        # attention = torch.softmax(energies, dim=1)\n        embedding = self.emb_nn(hidden_nodes)\n        return torch.sum(attention * embedding, dim=1)\n\n\nclass Set2Vec(nn.Module):\n    r\"\"\"The readout function of MPNN paper's best network\n    \"\"\"\n\n    # used to set attention terms to 0 when passing energies to softmax\n    # tf code uses same trick\n    BIG_NEGATIVE = -1e6\n\n    def __init__(self, node_features, lstm_computations, memory_size):\n        super(Set2Vec, self).__init__()\n\n        self.lstm_computations = lstm_computations\n        self.memory_size = memory_size\n\n        self.embedding_matrix = nn.Linear(node_features * 2, self.memory_size, bias=False)\n        self.lstm = nn.LSTMCell(self.memory_size, self.memory_size, bias=False)\n\n    def forward(self, hidden_output_nodes, input_nodes, node_mask):\n        batch_size = input_nodes.shape[0]\n        energy_mask = (1 - node_mask).float() * self.BIG_NEGATIVE\n\n        lstm_input = torch.zeros(batch_size, self.memory_size)\n\n        cat = torch.cat([hidden_output_nodes, input_nodes], dim=2)\n        memory = self.embedding_matrix(cat)\n\n        hidden_state = torch.zeros(batch_size, self.memory_size)\n        cell_state = torch.zeros(batch_size, self.memory_size)\n\n        if next(self.parameters()).is_cuda:\n            lstm_input = lstm_input.cuda()\n            hidden_state = hidden_state.cuda()\n            cell_state = cell_state.cuda()\n\n        for i in range(self.lstm_computations):\n            query, cell_state = self.lstm(lstm_input, (hidden_state, cell_state))\n            # dot product query x memory\n            energies = (query.view(batch_size, 1, self.memory_size) * memory).sum(dim=-1)\n            attention = torch.softmax(energies + energy_mask, dim=1)\n            read = (attention.unsqueeze(-1) * memory).sum(dim=1)\n\n            hidden_state = query\n            lstm_input = read\n\n        cat = torch.cat([query, read], dim=1)\n        return cat\n\n\nclass FeedForwardNetwork(nn.Module):\n    r\"\"\"Convenience class to create network composed of linear layers with an activation function\n    applied between them\n\n    Args:\n        in_features: size of each input sample\n        hidden_layer_sizes: list of hidden layer sizes\n        out_features: size of each output sample\n        activation: 'SELU' or 'ReLU'\n        bias: If set to False, the layers will not learn an additive bias.\n            Default: ``False``\n    \"\"\"\n\n    def __init__(self, in_features, hidden_layer_sizes, out_features, activation='SELU', bias=False, dropout_p=0.0):\n        super(FeedForwardNetwork, self).__init__()\n\n        if activation == 'SELU':\n            Activation = nn.SELU\n            Dropout = nn.AlphaDropout\n            init_constant = 1.0\n        elif activation == 'ReLU':\n            Activation = nn.ReLU\n            Dropout = nn.Dropout\n            init_constant = 2.0\n\n        layer_sizes = [in_features] + hidden_layer_sizes + [out_features]\n\n        layers = []\n        for i in range(len(layer_sizes) - 2):\n            layers.append(Dropout(dropout_p))\n            layers.append(nn.Linear(layer_sizes[i], layer_sizes[i + 1], bias))\n            layers.append(Activation())\n        layers.append(Dropout(dropout_p))\n        layers.append(nn.Linear(layer_sizes[-2], layer_sizes[-1], bias))\n\n        self.seq = nn.Sequential(*layers)\n\n        for i in range(1, len(layers), 3):\n            # initialization recommended in SELU paper\n            nn.init.normal_(layers[i].weight, std=math.sqrt(init_constant / layers[i].weight.size(1)))\n\n    def forward(self, input):\n        return self.seq(input)\n\n    # I'm probably *supposed to* override extra_repr but then self.seq (unreadable) will be printed too\n    def __repr__(self):\n        ffnn = type(self).__name__\n        in_features = self.seq[1].in_features\n        hidden_layer_sizes = [linear.out_features for linear in self.seq[1:-1:3]]\n        out_features = self.seq[-1].out_features\n        if len(self.seq) > 2:\n            activation = str(self.seq[2])\n        else:\n            activation = 'None'\n        bias = self.seq[1].bias is not None\n        dropout_p = self.seq[0].p\n        return '{}(in_features={}, hidden_layer_sizes={}, out_features={}, activation={}, bias={}, dropout_p={})'.format(\n            ffnn, in_features, hidden_layer_sizes, out_features, activation, bias, dropout_p\n        )\n\n\n#############  molgraph_dataset.py\n\nimport gzip\nimport rdkit\nfrom rdkit import Chem\nfrom rdkit.Chem.rdchem import BondType\nfrom torch.utils import data\n\nfrom collections import defaultdict\n\n\nclass MolGraphDataset(data.Dataset):\n    r\"\"\"For datasets consisting of SMILES strings and target values.\n\n    Expects a csv file formatted as:\n    comment,smiles,targetName1,targetName2\n    Some Comment,CN=C=O,0,1\n    ,CC(=O)NCCC1=CNc2c1cc(OC)cc2,1,1\n\n    Args:\n        path\n        prediction: set to True if dataset contains no target values\n    \"\"\"\n\n    def __init__(self, path, prediction=False):\n        with gzip.open(path, 'r') as file:\n            self.header_cols = file.readline().decode('utf-8')[:-2].split('\\t')\n        n_cols = len(self.header_cols)\n\n        self.target_names = self.header_cols[2:]\n        self.comments = np.genfromtxt(path, delimiter='\\t', skip_header=1, usecols=[0], dtype=np.str, comments=None)\n        # comments=None because default is \"#\", that some smiles contain\n        self.smiles = np.genfromtxt(path, delimiter='\\t', skip_header=1, usecols=[1], dtype=np.str, comments=None)\n        if prediction:\n            self.targets = np.empty((len(self.smiles), n_cols - 2))  # may be used to figure out number of targets etc\n        else:\n            self.targets = np.genfromtxt(path, delimiter='\\t', skip_header=1, usecols=range(2, n_cols),\n                                         comments=None).reshape(-1, n_cols - 2)\n\n    def __getitem__(self, index):\n        adjacency, nodes, edges = smile_to_graph(self.smiles[index])\n        targets = self.targets[index, :]\n        return (adjacency, nodes, edges), targets\n\n    def __len__(self):\n        return len(self.smiles)\n\n\nrdLogger = rdkit.RDLogger.logger()\nrdLogger.setLevel(rdkit.RDLogger.ERROR)\n\n\ndef smile_to_graph(smile):\n    molecule = Chem.MolFromSmiles(smile)\n    n_atoms = molecule.GetNumAtoms()\n    atoms = [molecule.GetAtomWithIdx(i) for i in range(n_atoms)]\n\n    adjacency = Chem.rdmolops.GetAdjacencyMatrix(molecule)\n    node_features = np.array([atom_features(atom) for atom in atoms])\n\n    n_edge_features = 4\n    edge_features = np.zeros([n_atoms, n_atoms, n_edge_features])\n    for bond in molecule.GetBonds():\n        i = bond.GetBeginAtomIdx()\n        j = bond.GetEndAtomIdx()\n        bond_type = BONDTYPE_TO_INT[bond.GetBondType()]\n        edge_features[i, j, bond_type] = 1\n        edge_features[j, i, bond_type] = 1\n\n    return adjacency, node_features, edge_features\n\n\n# rdkit GetBondType() result -> int\nBONDTYPE_TO_INT = defaultdict(\n    lambda: 0,\n    {\n        BondType.SINGLE: 0,\n        BondType.DOUBLE: 1,\n        BondType.TRIPLE: 2,\n        BondType.AROMATIC: 3\n    }\n)\n\n\nclass MolGraphDatasetSubset(MolGraphDataset):\n    r\"\"\"Takes a subset of MolGraphDataset.\n\n    The \"Subset\" class of pytorch does not allow column selection\n    \"\"\"\n\n    def __init__(self, path, indices=None, columns=None):\n        super(MolGraphDatasetSubset, self).__init__(path)\n        if indices:\n            self.smiles = self.smiles[indices]\n            self.targets = self.targets[indices]\n        if columns:\n            self.target_names = [self.target_names[col] for col in columns]\n            self.targets = self.targets[:, columns]\n\n\n# data is list of ((g,h,e), [targets])\n# to be passable to DataLoader it needs to have this signature,\n# where the outer tuple is that which is returned by Dataset's __getitem__\ndef molgraph_collate_fn(data):\n    n_samples = len(data)\n    (adjacency_0, node_features_0, edge_features_0), targets_0 = data[0]\n    n_nodes_largest_graph = max(map(lambda sample: sample[0][0].shape[0], data))\n    n_node_features = node_features_0.shape[1]\n    n_edge_features = edge_features_0.shape[2]\n    n_targets = len(targets_0)\n\n    adjacency_tensor = torch.zeros(n_samples, n_nodes_largest_graph, n_nodes_largest_graph)\n    node_tensor = torch.zeros(n_samples, n_nodes_largest_graph, n_node_features)\n    edge_tensor = torch.zeros(n_samples, n_nodes_largest_graph, n_nodes_largest_graph, n_edge_features)\n    target_tensor = torch.zeros(n_samples, n_targets)\n\n    for i in range(n_samples):\n        (adjacency, node_features, edge_features), target = data[i]\n        n_nodes = adjacency.shape[0]\n\n        adjacency_tensor[i, :n_nodes, :n_nodes] = torch.Tensor(adjacency)\n        node_tensor[i, :n_nodes, :] = torch.Tensor(node_features)\n        edge_tensor[i, :n_nodes, :n_nodes, :] = torch.Tensor(edge_features)\n\n        target_tensor[i] = torch.Tensor(target)\n\n    return adjacency_tensor, node_tensor, edge_tensor, target_tensor\n\n\n#################  summation_mpnn.py\n\nclass SummationMPNN(nn.Module):\n    r\"\"\"Abstract MPNN class, ExampleMPNN demonstrates how to extend it\n\n    Args:\n        node_features (int)\n        edge_features (int)\n        message_size (int)\n        message_passes (int)\n        out_features (int)\n    \"\"\"\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features):\n        super(SummationMPNN, self).__init__()\n        self.node_features = node_features\n        self.edge_features = edge_features\n        self.message_size = message_size\n        self.message_passes = message_passes\n        self.out_features = out_features\n\n    # inputs are \"batches\" of shape (total number of edges in batch, number of features)\n    def message_terms(self, nodes, node_neighbours, edges):\n        raise NotImplementedError\n\n    # inputs are \"batches\" of shape (maximum number of nodes in batch, number of features)\n    def update(self, nodes, messages):\n        raise NotImplementedError\n\n    # inputs are \"batches\" of same shape as the nodes passed to update\n    # node_mask is same shape as inputs and is 1 if elements corresponding exists, otherwise 0\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        raise NotImplementedError\n\n    def forward(self, adjacency, nodes, edges):\n        edge_batch_batch_indices, edge_batch_node_indices, edge_batch_neighbour_indices = adjacency.nonzero().unbind(-1)\n        node_batch_batch_indices, node_batch_node_indices = adjacency.sum(-1).nonzero().unbind(-1)\n\n        same_batch = node_batch_batch_indices.view(-1, 1) == edge_batch_batch_indices\n        same_node = node_batch_node_indices.view(-1, 1) == edge_batch_node_indices\n        # element_ij = 1 if edge_batch_edges[j] is connected with node_batch_nodes[i], else 0\n        message_summation_matrix = (same_batch * same_node).float()\n\n        edge_batch_edges = edges[edge_batch_batch_indices, edge_batch_node_indices, edge_batch_neighbour_indices, :]\n        hidden_nodes = nodes.clone()\n        node_batch_nodes = hidden_nodes[node_batch_batch_indices, node_batch_node_indices, :]\n\n        for i in range(self.message_passes):\n            edge_batch_nodes = hidden_nodes[edge_batch_batch_indices, edge_batch_node_indices, :]\n            edge_batch_neighbours = hidden_nodes[edge_batch_batch_indices, edge_batch_neighbour_indices, :]\n\n            message_terms = self.message_terms(edge_batch_nodes, edge_batch_neighbours, edge_batch_edges)\n            # the summation in eq. 1 of the NMPQC paper happens here\n            messages = torch.matmul(message_summation_matrix, message_terms)\n            node_batch_nodes = self.update(node_batch_nodes, messages)\n\n            hidden_nodes[node_batch_batch_indices, node_batch_node_indices, :] = node_batch_nodes\n\n        node_mask = (adjacency.sum(-1) != 0)  # .unsqueeze(-1).expand_as(nodes)\n        output = self.readout(hidden_nodes, nodes, node_mask)\n        return output\n\n\n##############  summation_mpnn_implementaion.py\n\n\nclass ENNS2V(SummationMPNN):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features,\n                 enn_depth=4, enn_hidden_dim=200, enn_dropout_p=0,\n                 s2v_lstm_computations=12, s2v_memory_size=50,\n                 out_depth=1, out_hidden_dim=200, out_dropout_p=0):\n        super(ENNS2V, self).__init__(node_features, edge_features, message_size, message_passes, out_features)\n\n        self.enn = FeedForwardNetwork(\n            edge_features, [enn_hidden_dim] * enn_depth, node_features * message_size, dropout_p=enn_dropout_p\n        )\n        self.gru = nn.GRUCell(input_size=message_size, hidden_size=node_features, bias=False)\n        self.s2v = Set2Vec(node_features, s2v_lstm_computations, s2v_memory_size)\n        self.out_nn = FeedForwardNetwork(\n            s2v_memory_size * 2, [out_hidden_dim] * out_depth, out_features, dropout_p=out_dropout_p, bias=False\n        )\n\n    def message_terms(self, nodes, node_neighbours, edges):\n        enn_output = self.enn(edges)\n        matrices = enn_output.view(-1, self.message_size, self.node_features)\n        msg_terms = torch.matmul(matrices, node_neighbours.unsqueeze(-1)).squeeze(-1)\n        return msg_terms\n\n    def update(self, nodes, messages):\n        return self.gru(messages, nodes)\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        graph_embeddings = self.s2v(hidden_nodes, input_nodes, node_mask)\n        return self.out_nn(graph_embeddings)\n\n\nclass GGNN(SummationMPNN):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features,\n                 msg_depth=4, msg_hidden_dim=200, msg_dropout_p=0.0,\n                 gather_width=100,\n                 gather_att_depth=3, gather_att_hidden_dim=100, gather_att_dropout_p=0.0,\n                 gather_emb_depth=3, gather_emb_hidden_dim=100, gather_emb_dropout_p=0.0,\n                 out_depth=2, out_hidden_dim=100, out_dropout_p=0.0, out_layer_shrinkage=1.0):\n        super(GGNN, self).__init__(node_features, edge_features, message_size, message_passes, out_features)\n\n        self.msg_nns = nn.ModuleList()\n        for _ in range(edge_features):\n            self.msg_nns.append(\n                FeedForwardNetwork(node_features, [msg_hidden_dim] * msg_depth, message_size, dropout_p=msg_dropout_p,\n                                   bias=False)\n            )\n        self.gru = nn.GRUCell(input_size=message_size, hidden_size=node_features, bias=False)\n        self.gather = GraphGather(\n            node_features, gather_width,\n            gather_att_depth, gather_att_hidden_dim, gather_att_dropout_p,\n            gather_emb_depth, gather_emb_hidden_dim, gather_emb_dropout_p\n        )\n        out_layer_sizes = [  # example: depth 5, dim 50, shrinkage 0.5 => out_layer_sizes [50, 42, 35, 30, 25]\n            round(out_hidden_dim * (out_layer_shrinkage ** (i / (out_depth - 1 + 1e-9)))) for i in range(out_depth)\n        ]\n        self.out_nn = FeedForwardNetwork(gather_width, out_layer_sizes, out_features, dropout_p=out_dropout_p)\n\n    def message_terms(self, nodes, node_neighbours, edges):\n        # terms_masked_per_edge contains (edge_batch_size, message_size)-shape tensors, that has 0s in all rows except\n        # the ones corresponding to the edge type indicated by the list index\n        # intuitive way of writing this involves a torch.stack along batch dimension and is immensely slow\n        edges_v = edges.view(-1, self.edge_features, 1)\n        node_neighbours_v = edges_v * node_neighbours.view(-1, 1, self.node_features)\n        terms_masked_per_edge = [\n            edges_v[:, i, :] * self.msg_nns[i](node_neighbours_v[:, i, :]) for i in range(self.edge_features)\n        ]\n        return sum(terms_masked_per_edge)\n\n    def update(self, nodes, messages):\n        return self.gru(messages, nodes)\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        graph_embeddings = self.gather(hidden_nodes, input_nodes, node_mask)\n        return self.out_nn(graph_embeddings)\n\n\n###############  gnn_test_case.py\n\nimport unittest\n\n# padding invariance is important because it indicates whether node vectors full of 0s (corresponding to\n# non-existant nodes) affects the output\n#\n# node order invariance is important because it shold in principle not matter\n#\n# add a newly implemented MPNN by extending MPNNTestCase and overrding it's member net, see bottom of file\n\n# some graphs generated that 1) are small enough so that the tensors are readable and\n# 2) have different size adjacency matrices, to assure molgraph_collate_fn:s padding is being used\n\n\nCOMPOUNDS = ['OC(=O)[C@@H]1CCN1', 'BrC1=NC=CC=C1', 'ClCCOC=C']\nBATCH_SIZE = len(COMPOUNDS)\nDUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES, DUMMY_TARGET = \\\n    molgraph_collate_fn(list(map(lambda smile: (smile_to_graph(smile), [1]), COMPOUNDS)))\nNODE_FEATURES = 5\nDUMMY_NODES = DUMMY_NODES[:, :, :NODE_FEATURES]  # dropping all node features except a few\nEDGE_FEATURES = DUMMY_EDGES.shape[3]\nOUT_FEATURES = DUMMY_TARGET.shape[1]\n\n\nclass DummyMPNN(SummationMPNN):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features):\n        super(DummyMPNN, self).__init__(node_features, edge_features, message_size, message_passes, out_features)\n\n        # breaks padding invariance, unless node_mask is used properly\n        # self.readout_layer = nn.Linear(NODE_FEATURES, 1, bias=True)\n        self.readout_layer = nn.Linear(NODE_FEATURES, 1, bias=False)\n\n    def message_terms(self, nodes, node_neighbours, edges):\n        message_terms = nodes + node_neighbours\n        return message_terms\n\n    def update(self, nodes, messages):\n        return messages\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        output = self.readout_layer(hidden_nodes).sum(dim=1)\n        return output\n\n\nclass GNNTestCase(unittest.TestCase):\n    NODE_FEATURES = NODE_FEATURES\n    EDGE_FEATURES = EDGE_FEATURES\n    OUT_FEATURES = OUT_FEATURES\n\n    # keep number of weights down to make this run fast\n    MESSAGE_SIZE = 5\n    MESSAGE_PASSES = 2\n\n    net = DummyMPNN(NODE_FEATURES, EDGE_FEATURES, MESSAGE_SIZE, MESSAGE_PASSES, OUT_FEATURES)\n\n    @classmethod\n    def setUpClass(self):\n        optimizer = optim.Adam(self.net.parameters(), lr=0.0005)\n        criterion = nn.MSELoss()\n        self.net.train()\n        for i in range(10):\n            self.net.zero_grad()\n            output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            loss = criterion(output, DUMMY_TARGET)\n            loss.backward()\n            optimizer.step()\n\n    def test_padding_invariance(self):\n        padded_dim_size = DUMMY_ADJ.shape[1] + 5\n        padded_adj = torch.zeros(BATCH_SIZE, padded_dim_size, padded_dim_size)\n        padded_adj[:, :DUMMY_ADJ.shape[1], :DUMMY_ADJ.shape[2]] = DUMMY_ADJ\n        padded_nodes = torch.zeros(BATCH_SIZE, padded_dim_size, NODE_FEATURES)\n        padded_nodes[:, :DUMMY_NODES.shape[1], :] = DUMMY_NODES\n        padded_edges = torch.zeros(BATCH_SIZE, padded_dim_size, padded_dim_size, EDGE_FEATURES)\n        padded_edges[:, :DUMMY_EDGES.shape[1], :DUMMY_EDGES.shape[2], :] = DUMMY_EDGES\n\n        with torch.no_grad():\n            self.net.eval()\n            normal_output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            extra_padding_output = self.net(padded_adj, padded_nodes, padded_edges)\n            # consider outputs equal if difference is smaller than 0.001%\n            # this is not always exact for whatever numerical reason\n            self.assertTrue(np.allclose(normal_output, extra_padding_output, rtol=1e-5))\n\n    def test_sample_order_invariance(self):\n        permutation = [1, 2, 0]\n        shuffled_adj = DUMMY_ADJ[permutation, :, :]\n        shuffled_nodes = DUMMY_NODES[permutation, :, :]\n        shuffled_edges = DUMMY_EDGES[permutation, :, :, :]\n\n        with torch.no_grad():\n            self.net.eval()\n            output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            shuffling_after_prop_output = output[permutation]\n            shuffling_before_prop_output = self.net(shuffled_adj, shuffled_nodes, shuffled_edges)\n            # consider outputs equal if difference is smaller than 0.001%\n            # this is not always exact for whatever numerical reason\n            self.assertTrue(np.allclose(shuffling_after_prop_output, shuffling_before_prop_output, rtol=1e-5))\n\n    def test_node_order_invariance(self):\n        shuffled_adj = torch.zeros_like(DUMMY_ADJ)\n        shuffled_nodes = torch.zeros_like(DUMMY_NODES)\n        shuffled_edges = torch.zeros_like(DUMMY_EDGES)\n        for i in range(BATCH_SIZE):\n            n_real_nodes = (DUMMY_ADJ[i, :, :].sum(dim=1) != 0).sum().item()\n            perm = np.random.permutation(n_real_nodes).reshape(1, -1)\n            perm_t = perm.transpose()\n            shuffled_adj[i, :n_real_nodes, :n_real_nodes] = DUMMY_ADJ[i, perm, perm_t]\n            shuffled_nodes[i, :n_real_nodes] = DUMMY_NODES[i, perm, :]\n            shuffled_edges[i, :n_real_nodes, :n_real_nodes, :] = DUMMY_EDGES[i, perm, perm_t, :]\n\n        with torch.no_grad():\n            self.net.eval()\n            normal_output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            shuffling_output = self.net(shuffled_adj, shuffled_nodes, shuffled_edges)\n            # consider outputs equal if difference is smaller than 0.001%\n            # this is not always exact for whatever numerical reason\n            self.assertTrue(np.allclose(normal_output, shuffling_output, rtol=1e-5))\n\n\n############### test_examples.py\n\nimport unittest\nfrom torch import optim, nn\n\n# this script checks that the MPNN implementations can be initialized and trained for a few iterations\n# without crashing, and that they fulfill the principles of invariance to node order, padding size\n# and shuffled input order\n#\n# padding invariance is important because it indicates whether node vectors full of 0s (corresponding to\n# non-existant nodes) affects the output\n#\n# node order invariance is important because it shold in principle not matter\n#\n# add a newly implemented MPNN by extending MPNNTestCase and overrding it's member net, see bottom of file\n\n# some graphs generated that 1) are small enough so that the tensors are readable and\n# 2) have different size adjacency matrices, to assure molgraph_collate_fn:s padding is being used\n\n\nCOMPOUNDS = ['OC(=O)[C@@H]1CCN1', 'BrC1=NC=CC=C1', 'ClCCOC=C']\nBATCH_SIZE = len(COMPOUNDS)\nDUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES, DUMMY_TARGET = \\\n    molgraph_collate_fn(list(map(lambda smile: (smile_to_graph(smile), [1]), COMPOUNDS)))\nNODE_FEATURES = 5\nDUMMY_NODES = DUMMY_NODES[:, :, :NODE_FEATURES]  # dropping all node features except a few\nEDGE_FEATURES = DUMMY_EDGES.shape[3]\nOUT_FEATURES = DUMMY_TARGET.shape[1]\n\n\nclass DummyMPNN(SummationMPNN):\n\n    def __init__(self, node_features, edge_features, message_size, message_passes, out_features):\n        super(DummyMPNN, self).__init__(node_features, edge_features, message_size, message_passes, out_features)\n\n        # breaks padding invariance, unless node_mask is used properly\n        # self.readout_layer = nn.Linear(NODE_FEATURES, 1, bias=True)\n        self.readout_layer = nn.Linear(NODE_FEATURES, 1, bias=False)\n\n    def message_terms(self, nodes, node_neighbours, edges):\n        message_terms = nodes + node_neighbours\n        return message_terms\n\n    def update(self, nodes, messages):\n        return messages\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        output = self.readout_layer(hidden_nodes).sum(dim=1)\n        return output\n\n\nclass GNNTestCase(unittest.TestCase):\n    NODE_FEATURES = NODE_FEATURES\n    EDGE_FEATURES = EDGE_FEATURES\n    OUT_FEATURES = OUT_FEATURES\n\n    # keep number of weights down to make this run fast\n    MESSAGE_SIZE = 5\n    MESSAGE_PASSES = 2\n\n    net = DummyMPNN(NODE_FEATURES, EDGE_FEATURES, MESSAGE_SIZE, MESSAGE_PASSES, OUT_FEATURES)\n\n    @classmethod\n    def setUpClass(self):\n        optimizer = optim.Adam(self.net.parameters(), lr=0.0005)\n        criterion = nn.MSELoss()\n        self.net.train()\n        for i in range(10):\n            self.net.zero_grad()\n            output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            loss = criterion(output, DUMMY_TARGET)\n            loss.backward()\n            optimizer.step()\n\n    def test_padding_invariance(self):\n        padded_dim_size = DUMMY_ADJ.shape[1] + 5\n        padded_adj = torch.zeros(BATCH_SIZE, padded_dim_size, padded_dim_size)\n        padded_adj[:, :DUMMY_ADJ.shape[1], :DUMMY_ADJ.shape[2]] = DUMMY_ADJ\n        padded_nodes = torch.zeros(BATCH_SIZE, padded_dim_size, NODE_FEATURES)\n        padded_nodes[:, :DUMMY_NODES.shape[1], :] = DUMMY_NODES\n        padded_edges = torch.zeros(BATCH_SIZE, padded_dim_size, padded_dim_size, EDGE_FEATURES)\n        padded_edges[:, :DUMMY_EDGES.shape[1], :DUMMY_EDGES.shape[2], :] = DUMMY_EDGES\n\n        with torch.no_grad():\n            self.net.eval()\n            normal_output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            extra_padding_output = self.net(padded_adj, padded_nodes, padded_edges)\n            # consider outputs equal if difference is smaller than 0.001%\n            # this is not always exact for whatever numerical reason\n            self.assertTrue(np.allclose(normal_output, extra_padding_output, rtol=1e-5))\n\n    def test_sample_order_invariance(self):\n        permutation = [1, 2, 0]\n        shuffled_adj = DUMMY_ADJ[permutation, :, :]\n        shuffled_nodes = DUMMY_NODES[permutation, :, :]\n        shuffled_edges = DUMMY_EDGES[permutation, :, :, :]\n\n        with torch.no_grad():\n            self.net.eval()\n            output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            shuffling_after_prop_output = output[permutation]\n            shuffling_before_prop_output = self.net(shuffled_adj, shuffled_nodes, shuffled_edges)\n            # consider outputs equal if difference is smaller than 0.001%\n            # this is not always exact for whatever numerical reason\n            self.assertTrue(np.allclose(shuffling_after_prop_output, shuffling_before_prop_output, rtol=1e-5))\n\n    def test_node_order_invariance(self):\n        shuffled_adj = torch.zeros_like(DUMMY_ADJ)\n        shuffled_nodes = torch.zeros_like(DUMMY_NODES)\n        shuffled_edges = torch.zeros_like(DUMMY_EDGES)\n        for i in range(BATCH_SIZE):\n            n_real_nodes = (DUMMY_ADJ[i, :, :].sum(dim=1) != 0).sum().item()\n            perm = np.random.permutation(n_real_nodes).reshape(1, -1)\n            perm_t = perm.transpose()\n            shuffled_adj[i, :n_real_nodes, :n_real_nodes] = DUMMY_ADJ[i, perm, perm_t]\n            shuffled_nodes[i, :n_real_nodes] = DUMMY_NODES[i, perm, :]\n            shuffled_edges[i, :n_real_nodes, :n_real_nodes, :] = DUMMY_EDGES[i, perm, perm_t, :]\n\n        with torch.no_grad():\n            self.net.eval()\n            normal_output = self.net(DUMMY_ADJ, DUMMY_NODES, DUMMY_EDGES)\n            shuffling_output = self.net(shuffled_adj, shuffled_nodes, shuffled_edges)\n            # consider outputs equal if difference is smaller than 0.001%\n            # this is not always exact for whatever numerical reason\n            self.assertTrue(np.allclose(normal_output, shuffling_output, rtol=1e-5))\n\n\n#################example.py\n\n\nclass ExampleAttentionMPNN(AggregationMPNN):\n\n    def __init__(self, node_features, edge_features, out_features, message_passes=3):\n        super(ExampleAttentionMPNN, self).__init__(node_features, edge_features, node_features, message_passes,\n                                                   out_features)\n\n        self.message_att_weight = nn.Linear(node_features, 1)\n        self.message_emb_weight = nn.Linear(node_features, node_features)\n        self.out_weight = nn.Linear(node_features, out_features)\n\n    def aggregate_message(self, nodes, node_neighbours, edges, mask):\n        neighbourhood = torch.cat([nodes.unsqueeze(1), node_neighbours], dim=1)\n\n        neighbourhood_mask = torch.cat([torch.ones((mask.shape[0], 1)), mask], dim=1)\n        energy_mask = (neighbourhood_mask == 0).float() * 1e6\n\n        energies = self.message_att_weight(neighbourhood) - energy_mask.unsqueeze(-1)\n        attention = torch.softmax(energies, dim=1)\n        embedding = self.message_emb_weight(neighbourhood)\n        messages = torch.sum(attention * embedding, dim=1)\n        return messages\n\n    def update(self, nodes, messages):\n        hidden_nodes = torch.selu(messages)\n        return hidden_nodes\n\n    def readout(self, hidden_nodes, input_nodes, node_mask):\n        graph_embedding = torch.sum(hidden_nodes, dim=1)\n        output = self.out_weight(graph_embedding)\n        return output\n\n\n# if __name__ == '__main__':\n#     print('loading data')\n#     train_dataset = MolGraphDataset('toydata/piece-of-esol.csv.gz')\n#     train_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True, collate_fn=molgraph_collate_fn)\n#\n#     print('instantiating ExampleAttentionMPNN')\n#     # 75 and 4 corresponds to MolGraphDataset, 1 corresponds to ESOL\n#     net = ExampleAttentionMPNN(node_features=75, edge_features=4, out_features=1)\n#     optimizer = optim.Adam(net.parameters(), lr=2e-5)\n#     criterion = nn.MSELoss()\n#\n#     print('starting training')\n#     for epoch in range(10):\n#         for i_batch, batch in enumerate(train_dataloader):\n#             adjacency, nodes, edges, target = batch\n#             optimizer.zero_grad()\n#             output = net(adjacency, nodes, edges)\n#             loss = criterion(output, target)\n#             loss.backward()\n#             torch.nn.utils.clip_grad_value_(net.parameters(), 5.0)\n#             optimizer.step()\n#\n#         print('epoch: {}, training MSE: {}'.format(epoch + 1, loss))\n\n\n################  test_example.py\n\n\nNODE_FEATURES = GNNTestCase.NODE_FEATURES\nEDGE_FEATURES = GNNTestCase.EDGE_FEATURES\nMESSAGE_SIZE = GNNTestCase.MESSAGE_SIZE\nMESSAGE_PASSES = GNNTestCase.MESSAGE_PASSES\nOUT_FEATURES = GNNTestCase.OUT_FEATURES\n\n\nclass ExampleMPNNTestCase(GNNTestCase):\n    net = ExampleAttentionMPNN(\n        NODE_FEATURES, EDGE_FEATURES, OUT_FEATURES\n    )\n\n\n###############  test_implementation.py\n\n\nNODE_FEATURES = GNNTestCase.NODE_FEATURES\nEDGE_FEATURES = GNNTestCase.EDGE_FEATURES\nMESSAGE_SIZE = GNNTestCase.MESSAGE_SIZE\nMESSAGE_PASSES = GNNTestCase.MESSAGE_PASSES\nOUT_FEATURES = GNNTestCase.OUT_FEATURES\n\n\nclass ENNS2VTestCase(GNNTestCase):\n    net = ENNS2V(\n        NODE_FEATURES, EDGE_FEATURES, MESSAGE_SIZE, MESSAGE_PASSES, OUT_FEATURES,\n        enn_hidden_dim=4, out_hidden_dim=6, s2v_memory_size=5\n    )\n\n\nclass GGNNTestCase(GNNTestCase):\n    net = GGNN(\n        NODE_FEATURES, EDGE_FEATURES, MESSAGE_SIZE, MESSAGE_PASSES, OUT_FEATURES,\n        msg_hidden_dim=4, gather_width=7, gather_att_hidden_dim=9, gather_emb_hidden_dim=7, out_hidden_dim=3\n    )\n\n\nclass AttentionENNS2VTestCase(GNNTestCase):\n    net = AttentionENNS2V(\n        NODE_FEATURES, EDGE_FEATURES, MESSAGE_SIZE, 10, OUT_FEATURES,\n        enn_hidden_dim=4, out_hidden_dim=6, s2v_memory_size=5\n    )\n\n\nclass AttentionGGNNTestCase(GNNTestCase):\n    net = AttentionGGNN(\n        NODE_FEATURES, EDGE_FEATURES, MESSAGE_SIZE, MESSAGE_PASSES, OUT_FEATURES,\n        att_hidden_dim=9,\n        msg_hidden_dim=4, gather_width=7, gather_att_hidden_dim=9, gather_emb_hidden_dim=7, out_hidden_dim=3\n    )\n\n\nclass EMNImplementationTestCase(GNNTestCase):\n    EDGE_EMBEDDING_SIZE = 7\n    net = EMNImplementation(\n        edge_features=EDGE_FEATURES, edge_embedding_size=EDGE_EMBEDDING_SIZE, message_passes=MESSAGE_PASSES,\n        out_features=OUT_FEATURES, node_features=NODE_FEATURES,\n        msg_hidden_dim=4, gather_width=7, gather_att_hidden_dim=9, gather_emb_hidden_dim=7, out_hidden_dim=3\n    )\n\n\n#################  losses.py\n\n\nclass MaskedMultiTaskCrossEntropy(nn.Module):\n\n    def forward(self, input, target):\n        scores = torch.sigmoid(input)\n        target_active = (target == 1).float()  # from -1/1 to 0/1\n        loss_terms = -(target_active * torch.log(scores) + (1 - target_active) * torch.log(1 - scores))\n        missing_values_mask = (target != 0).float()\n        return (loss_terms * missing_values_mask).sum() / missing_values_mask.sum()\n\n\nLOSS_FUNCTIONS = {\n    'MaskedMultiTaskCrossEntropy': MaskedMultiTaskCrossEntropy(),\n    'MSE': nn.MSELoss()\n}\n\n#####################  predict.py\n\nimport argparse\n\nparser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\nparser.add_argument('--cuda', action='store_true', default=False, help='Enables CUDA training')\n\nparser.add_argument('--modelpath', type=str, help='Path to saved model', required=True)\nparser.add_argument('--datapath', type=str, default='toydata/piece-of-tox21-test.csv.gz', help='Testing dataset path')\nparser.add_argument('--score', type=str, choices=['roc-auc', 'pr-auc', 'MSE', 'RMSE'], required=True)\n\n# if __name__ == '__main__':\n#     global args\n#     args = parser.parse_args()\n#\n#     with torch.no_grad():\n#         net = torch.load(args.modelpath)\n#         if args.cuda:\n#             net = net.cuda()\n#         else:\n#             net = net.cpu()\n#         net.eval()\n#\n#         dataset = MolGraphDataset(args.datapath, prediction=True)\n#         dataloader = DataLoader(dataset, batch_size=50, collate_fn=molgraph_collate_fn)\n#\n#         batch_outputs = []\n#         for i_batch, batch in enumerate(dataloader):\n#             if args.cuda:\n#                 batch = [tensor.cuda() for tensor in batch]\n#             adjacency, nodes, edges, target = batch\n#             batch_output = net(adjacency, nodes, edges)\n#             if args.score == 'roc-auc' or args.score == 'pr-auc':\n#                 batch_output = torch.sigmoid(batch_output)\n#             batch_outputs.append(batch_output)\n#\n#         output = torch.cat(batch_outputs).cpu().numpy()\n#\n#         print('\\t'.join([str(col) for col in dataset.header_cols]))\n#         for i in range(len(output)):\n#             comment = dataset.comments[i]\n#             row_str = '\\t'.join([str(x) for x in output[i]])\n#             print('{}, {}'.format(comment, row_str))\n\n\n#################  predict.py\n\n\nMODEL_CONSTRUCTOR_DICTS = {\n    'ENNS2V': {\n        'constructor': ENNS2V,\n        'hyperparameters': {\n            'message-passes': {'type': int, 'default': 5},\n            'message-size': {'type': int, 'default': 50},\n            'enn-depth': {'type': int, 'default': 3},\n            'enn-hidden-dim': {'type': int, 'default': 100},\n            'enn-dropout-p': {'type': float, 'default': 0.0},\n            's2v-lstm-computations': {'type': int, 'default': 7},\n            's2v-memory-size': {'type': int, 'default': 50},\n            'out-depth': {'type': int, 'default': 2},\n            'out-hidden-dim': {'type': int, 'default': 300},\n            'out-dropout-p': {'type': float, 'default': 0.0}\n        }\n    },\n    'GGNN': {\n        'constructor': GGNN,\n        'hyperparameters': {  # the below, batch size 50, learn rate 1.176e-5 and 1200 epochs is good for ESOL\n            'message-passes': {'type': int, 'default': 1},\n            'message-size': {'type': int, 'default': 25},\n            'msg-depth': {'type': int, 'default': 2},\n            'msg-hidden-dim': {'type': int, 'default': 50},\n            'msg-dropout-p': {'type': float, 'default': 0.0},\n            'gather-width': {'type': int, 'default': 45},\n            'gather-att-depth': {'type': int, 'default': 2},\n            'gather-att-hidden-dim': {'type': int, 'default': 26},\n            'gather-att-dropout-p': {'type': float, 'default': 0.0},\n            'gather-emb-depth': {'type': int, 'default': 2},\n            'gather-emb-hidden-dim': {'type': int, 'default': 26},\n            'gather-emb-dropout-p': {'type': float, 'default': 0.0},\n            'out-depth': {'type': int, 'default': 2},\n            'out-hidden-dim': {'type': int, 'default': 450},\n            'out-dropout-p': {'type': float, 'default': 0.00463},\n            'out-layer-shrinkage': {'type': float, 'default': 0.5028}\n        }\n    },\n    'AttentionGGNN': {  # the below, batch size 50, learn rate 1.560e-5 and 600 epochs is good for BBBP\n        'constructor': AttentionGGNN,\n        'hyperparameters': {\n            'message-passes': {'type': int, 'default': 8},\n            'message-size': {'type': int, 'default': 25},\n            'msg-depth': {'type': int, 'default': 2},\n            'msg-hidden-dim': {'type': int, 'default': 50},\n            'msg-dropout-p': {'type': float, 'default': 0.0},\n            'att-depth': {'type': int, 'default': 2},\n            'att-hidden-dim': {'type': int, 'default': 50},\n            'att-dropout-p': {'type': float, 'default': 0.0},\n            'gather-width': {'type': int, 'default': 45},\n            'gather-att-depth': {'type': int, 'default': 2},\n            'gather-att-hidden-dim': {'type': int, 'default': 45},\n            'gather-att-dropout-p': {'type': float, 'default': 0.0},\n            'gather-emb-depth': {'type': int, 'default': 2},\n            'gather-emb-hidden-dim': {'type': int, 'default': 26},\n            'gather-emb-dropout-p': {'type': float, 'default': 0.0},\n            'out-depth': {'type': int, 'default': 2},\n            'out-hidden-dim': {'type': int, 'default': 560},\n            'out-dropout-p': {'type': float, 'default': 0.1},\n            'out-layer-shrinkage': {'type': float, 'default': 0.6}\n        }\n    },\n    'EMN': {  # the below, batch size 50, learn rate 1e-4 and 1000 epochs is good for SIDER\n        'constructor': EMNImplementation,\n        'hyperparameters': {\n            'message-passes': {'type': int, 'default': 8},\n            'edge-embedding-size': {'type': int, 'default': 50},\n            'edge-emb-depth': {'type': int, 'default': 2},\n            'edge-emb-hidden-dim': {'type': int, 'default': 105},\n            'edge-emb-dropout-p': {'type': float, 'default': 0.0},\n            'att-depth': {'type': int, 'default': 2},\n            'att-hidden-dim': {'type': int, 'default': 85},\n            'att-dropout-p': {'type': float, 'default': 0.0},\n            'msg-depth': {'type': int, 'default': 2},\n            'msg-hidden-dim': {'type': int, 'default': 150},\n            'msg-dropout-p': {'type': float, 'default': 0.0},\n            'gather-width': {'type': int, 'default': 45},\n            'gather-att-depth': {'type': int, 'default': 2},\n            'gather-att-hidden-dim': {'type': int, 'default': 45},\n            'gather-att-dropout-p': {'type': float, 'default': 0.0},\n            'gather-emb-depth': {'type': int, 'default': 2},\n            'gather-emb-hidden-dim': {'type': int, 'default': 45},\n            'gather-emb-dropout-p': {'type': float, 'default': 0.0},\n            'out-depth': {'type': int, 'default': 2},\n            'out-hidden-dim': {'type': int, 'default': 450},\n            'out-dropout-p': {'type': float, 'default': 0.1},\n            'out-layer-shrinkage': {'type': float, 'default': 0.6}\n        }\n    }\n}\n\n# common_args_parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter, add_help=False)\n# \n# common_args_parser.add_argument('--cuda', action='store_true', default=False, help='Enables CUDA training')\n# \n# common_args_parser.add_argument('--train-set', type=str, default='toydata/piece-of-tox21-train.csv.gz',\n#                                 help='Training dataset path')\n# common_args_parser.add_argument('--valid-set', type=str, default='toydata/piece-of-tox21-valid.csv.gz',\n#                                 help='Validation dataset path')\n# common_args_parser.add_argument('--test-set', type=str, default='toydata/piece-of-tox21-test.csv.gz',\n#                                 help='Testing dataset path')\n# common_args_parser.add_argument('--loss', type=str, default='MaskedMultiTaskCrossEntropy',\n#                                 choices=[k for k, v in LOSS_FUNCTIONS.items()])\n# common_args_parser.add_argument('--score', type=str, default='roc-auc', help='roc-auc or MSE')\n# \n# common_args_parser.add_argument('--epochs', type=int, default=500, help='Number of training epochs')\n# common_args_parser.add_argument('--batch-size', type=int, default=50, help='Number of graphs in a mini-batch')\n# common_args_parser.add_argument('--learn-rate', type=float, default=1e-5)\n# \n# common_args_parser.add_argument('--savemodel', action='store_true', default=False,\n#                                 help='Saves model with highest validation score')\n# common_args_parser.add_argument('--logging', type=str, default='less', choices=[k for k, v in LOG_FUNCTIONS.items()])\n# \n# main_parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n# subparsers = main_parser.add_subparsers(help=', '.join([k for k, v in MODEL_CONSTRUCTOR_DICTS.items()]), dest='model')\n# subparsers.required = True\n# \n# model_parsers = {}\n# for model_name, constructor_dict in MODEL_CONSTRUCTOR_DICTS.items():\n#     subparser = subparsers.add_parser(model_name, parents=[common_args_parser])\n#     for hp_name, hp_kwargs in constructor_dict['hyperparameters'].items():\n#         subparser.add_argument('--' + hp_name, **hp_kwargs, help=model_name + ' hyperparameter')\n#     model_parsers[model_name] = subparser\n# \n# \n# def main():\n#     global args\n#     args = main_parser.parse_args()\n#     args_dict = vars(args)\n#     # dictionary of hyperparameters that are specific to the chosen model\n#     model_hp_kwargs = {\n#         name.replace('-', '_'): args_dict[name.replace('-', '_')]  # argparse converts to \"_\" implicitly\n#         for name, v in MODEL_CONSTRUCTOR_DICTS[args.model]['hyperparameters'].items()\n#     }\n# \n#     train_dataset = MolGraphDataset(args.train_set)\n#     train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True,\n#                                   collate_fn=molgraph_collate_fn)\n#     validation_dataset = MolGraphDataset(args.valid_set)\n#     validation_dataloader = DataLoader(validation_dataset, batch_size=args.batch_size, collate_fn=molgraph_collate_fn)\n#     test_dataset = MolGraphDataset(args.test_set)\n#     test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, collate_fn=molgraph_collate_fn)\n# \n#     ((sample_adjacency, sample_nodes, sample_edges), sample_target) = train_dataset[0]\n#     net = MODEL_CONSTRUCTOR_DICTS[args.model]['constructor'](\n#         node_features=len(sample_nodes[0]), edge_features=len(sample_edges[0, 0]), out_features=len(sample_target),\n#         **model_hp_kwargs\n#     )\n#     if args.cuda:\n#         net = net.cuda()\n# \n#     optimizer = optim.Adam(net.parameters(), lr=args.learn_rate)\n#     criterion = LOSS_FUNCTIONS[args.loss]\n# \n#     for epoch in range(args.epochs):\n#         net.train()\n#         for i_batch, batch in enumerate(train_dataloader):\n# \n#             if args.cuda:\n#                 batch = [tensor.cuda() for tensor in batch]\n#             adjacency, nodes, edges, target = batch\n# \n#             optimizer.zero_grad()\n#             output = net(adjacency, nodes, edges)\n#             loss = criterion(output, target)\n#             loss.backward()\n#             torch.nn.utils.clip_grad_value_(net.parameters(), 5.0)\n#             optimizer.step()\n# \n#         with torch.no_grad():\n#             net.eval()\n#             LOG_FUNCTIONS[args.logging](\n#                 net, train_dataloader, validation_dataloader, test_dataloader, criterion, epoch, args\n#             )\n\n\n#################  train_logging.py\n\nfrom sklearn.metrics import roc_auc_score, average_precision_score\n\nimport datetime\n\nOUTPUT_DIR = 'output/'\nTENSORBOARDX_OUTPUT_DIR = 'tbxoutput/'\nSAVEDMODELS_DIR = 'savedmodels/'\n# time of importing this file, including microseconds because slurm may start queued jobs very close in time\nDATETIME_STR = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\n\nclass Globals:  # container for all objects getting passed between log calls\n    evaluate_called = False\n\n\ng = Globals()\n\nTRAIN_SUBSET_SIZE = 500\nSUBSET_LOADER_BATCH_SIZE = 50\n\n\ndef subset_loader(dataloader, subset_size, seed=0):\n    np.random.seed(seed)\n    random_indices = np.random.choice(len(dataloader.dataset), subset_size)\n    np.random.seed()  # \"reset\" seed\n    subset = data.Subset(dataloader.dataset, random_indices)\n    return data.DataLoader(subset, batch_size=SUBSET_LOADER_BATCH_SIZE, collate_fn=dataloader.collate_fn)\n\n\ndef compute_roc_auc(output, target):\n    def roc_auc_of_column(scores_column, targets_column):\n        relevant_indices = targets_column.nonzero()\n        relevant_targets = targets_column[relevant_indices]\n        relevant_scores = scores_column[relevant_indices]\n        relevant_targets_np = relevant_targets.cpu().numpy()\n        relevant_targets_np = relevant_targets_np == 1  # -1s/1s => Falses/Trues\n        try:\n            score = roc_auc_score(relevant_targets_np, relevant_scores.cpu().detach().numpy())\n        except:\n            score = np.nan\n        return score\n\n    scores = torch.sigmoid(output)\n    roc_aucs = [\n        roc_auc_of_column(scores[:, i], target[:, i])\n        for i in range(target.shape[1])\n    ]\n    return roc_aucs\n\n\ndef compute_pr_auc(output, target):\n    def pr_auc_of_column(scores_column, targets_column):\n        relevant_indices = targets_column.nonzero()\n        relevant_targets = targets_column[relevant_indices]\n        relevant_scores = scores_column[relevant_indices]\n        relevant_targets_np = relevant_targets.cpu().numpy()\n        relevant_targets_np = relevant_targets_np == 1  # -1s/1s => Falses/Trues\n        return average_precision_score(relevant_targets_np, relevant_scores.cpu().detach().numpy())\n\n    scores = torch.sigmoid(output)\n    pr_aucs = [\n        pr_auc_of_column(scores[:, i], target[:, i])\n        for i in range(target.shape[1])\n    ]\n    return pr_aucs\n\n\ndef compute_mse(output, target):\n    nn_mse = torch.nn.MSELoss()\n    mses = [\n        nn_mse(output[:, i], target[:, i]).cpu().detach().numpy()\n        for i in range(target.shape[1])\n    ]\n    return mses\n\n\ndef compute_rmse(output, target):\n    mses = compute_mse(output, target)\n    return np.sqrt(mses)\n\n\nSCORE_FUNCTIONS = {\n    'roc-auc': compute_roc_auc, 'pr-auc': compute_pr_auc, 'MSE': compute_mse, 'RMSE': compute_rmse\n}\n\n\ndef feed_net(net, dataloader, criterion, cuda):\n    batch_outputs = []\n    batch_losses = []\n    batch_targets = []\n    for i_batch, batch in enumerate(dataloader):\n        if cuda:\n            batch = [tensor.cuda(non_blocking=True) for tensor in batch]\n        adjacency, nodes, edges, target = batch\n        output = net(adjacency, nodes, edges)\n        loss = criterion(output, target)\n        batch_outputs.append(output)\n        batch_losses.append(loss.item())\n        batch_targets.append(target)\n    outputs = torch.cat(batch_outputs)\n    loss = np.mean(batch_losses)\n    targets = torch.cat(batch_targets)\n    return outputs, loss, targets\n\n\ndef evaluate_net(net, train_dataloader, validation_dataloader, test_dataloader, criterion, args):\n    global g\n    if not g.evaluate_called:\n        g.evaluate_called = True\n        if args.score == 'roc-auc' or args.score == 'pr-auc':\n            g.best_mean_train_score, g.best_mean_validation_score, g.best_mean_test_score = 0, 0, 0\n        elif args.score == 'MSE' or args.score == 'RMSE':\n            # just something large, this is arbitrary\n            g.best_mean_train_score, g.best_mean_validation_score, g.best_mean_test_score = 10, 10, 10\n        # g.train_subset_loader = subset_loader(train_dataloader, TRAIN_SUBSET_SIZE, seed=0)\n        g.train_subset_loader = train_dataloader\n\n    train_output, train_loss, train_target = feed_net(net, g.train_subset_loader, criterion, args.cuda)\n    validation_output, validation_loss, validation_target = feed_net(net, validation_dataloader, criterion, args.cuda)\n    test_output, test_loss, test_target = feed_net(net, test_dataloader, criterion, args.cuda)\n\n    train_scores = SCORE_FUNCTIONS[args.score](train_output, train_target)\n    train_mean_score = np.nanmean(train_scores)\n    validation_scores = SCORE_FUNCTIONS[args.score](validation_output, validation_target)\n    validation_mean_score = np.nanmean(validation_scores)\n    test_scores = SCORE_FUNCTIONS[args.score](test_output, test_target)\n    test_mean_score = np.nanmean(test_scores)\n\n    if args.score == 'roc-auc' or args.score == 'pr-auc':\n        new_best_model_found = validation_mean_score > g.best_mean_validation_score\n    elif args.score == 'MSE' or args.score == 'RMSE':\n        new_best_model_found = validation_mean_score < g.best_mean_validation_score\n\n    if new_best_model_found:\n        g.best_mean_train_score = train_mean_score\n        g.best_mean_validation_score = validation_mean_score\n        g.best_mean_test_score = test_mean_score\n\n        if args.savemodel:\n            path = SAVEDMODELS_DIR + type(net).__name__ + DATETIME_STR\n            torch.save(net, path)\n\n    target_names = train_dataloader.dataset.target_names\n    return {  # if made deeper, tensorboardx writing breaks I think\n        'loss': {'train': train_loss, 'test': test_loss},\n        'mean {}'.format(args.score):\n            {'train': train_mean_score, 'validation': validation_mean_score, 'test': test_mean_score},\n        'train {}s'.format(args.score): {target_names[i]: train_scores[i] for i in range(len(target_names))},\n        'test {}s'.format(args.score): {target_names[i]: test_scores[i] for i in range(len(target_names))},\n        'best mean {}'.format(args.score):\n            {'train': g.best_mean_train_score, 'validation': g.best_mean_validation_score,\n             'test': g.best_mean_test_score}\n    }\n\n\ndef get_run_info(net, args):\n    return {\n        'net': type(net).__name__,\n        'args': ', '.join([str(k) + ': ' + str(v) for k, v in vars(args).items()]),\n        'modules': {name: str(module) for name, module in net._modules.items()}\n    }\n\n\ndef less_log(net, train_dataloader, validation_dataloader, test_dataloader, criterion, epoch, args):\n    scalars = evaluate_net(net, train_dataloader, validation_dataloader, test_dataloader, criterion, args)\n    mean_score_key = 'mean {}'.format(args.score)\n    print('epoch {}, training mean {}: {}, validation mean {}: {}, testing mean {}: {}'.format(\n        epoch + 1,\n        args.score, scalars[mean_score_key]['train'],\n        args.score, scalars[mean_score_key]['validation'],\n        args.score, scalars[mean_score_key]['test'])\n    )\n\n\ndef more_log(net, train_dataloader, validation_dataloader, test_dataloader, criterion, epoch, args):\n    mean_score_key = 'mean {}'.format(args.score)\n    best_mean_score_key = 'best {}'.format(mean_score_key)\n    global g\n    if not g.evaluate_called:\n        run_info = get_run_info(net, args)\n        print('net: ' + run_info['net'])\n        print('args: {' + run_info['args'] + '}')\n        print('****** MODULES: ******')\n        for name, description in run_info['modules'].items():\n            print(name + ': ' + description)\n        print('**********************')\n        print('score metric: {}'.format(args.score))\n        print('columns:')\n        print(\n            'epochs, ' + \\\n            'mean training score, mean validation score, mean testing score, ' + \\\n            'best-model-so-far mean training score, best-model-so-far mean validation score, best-model-so-far mean testing score'\n        )\n\n    scalars = evaluate_net(net, train_dataloader, validation_dataloader, test_dataloader, criterion, args)\n    print(\n        '%d, %f, %f, %f, %f, %f, %f' % (\n            epoch + 1,\n            scalars[mean_score_key]['train'], scalars[mean_score_key]['validation'], scalars[mean_score_key]['test'],\n            scalars[best_mean_score_key]['train'], scalars[best_mean_score_key]['validation'],\n            scalars[best_mean_score_key]['test']\n        )\n    )\n\n\n# to open tensorboard training summaries, live or static:\n# 1) do some training to generate them in tbxoutput/\n# 2) install tensorflow (in a separate environment is fine)\n# 3) run tensorboard --port 6011 --logdir tbxoutput/ and open localhost:6011 in a browser\ndef tensorboardx_log(net, train_dataloader, validation_dataloader, test_dataloader, criterion, epoch, args):\n    global g\n    if not g.evaluate_called:\n        from tensorboardX import SummaryWriter\n\n        run_info = get_run_info(net, args)\n\n        class_str = run_info['net']\n        output_subdir = TENSORBOARDX_OUTPUT_DIR + class_str + ' ' + DATETIME_STR\n        g.writer = SummaryWriter(output_subdir)\n\n        g.writer.add_text('args', run_info['args'])\n        for k, v in run_info['modules'].items():\n            g.writer.add_text(k, v)\n    else:\n        # writer = SummaryWriter(output_subdir) # tensorboardx bug causes this to crash on epoch 40 or so\n        g.writer.file_writer.reopen()  # workaround\n\n    scalars = evaluate_net(net, train_dataloader, validation_dataloader, test_dataloader, criterion, args)\n\n    for k, v in scalars.items():\n        g.writer.add_scalars(k, v, epoch)\n\n    # writer.close() # tensorboardx bug causes this to crash on epoch 40 or so\n    g.writer.file_writer.close()  # workaround\n\n    print('epoch %d, training loss: %f, validation loss: %f' %\n          (epoch + 1, scalars['loss']['train'], scalars['loss']['validation']))\n\n\nLOG_FUNCTIONS = {\n    'less': less_log, 'more': more_log, 'tensorboardx': tensorboardx_log\n}\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imports\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nimport datetime","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = MolGraphDataset('/kaggle/working/2019-nCov/Data/protease_train.csv.gz')\ntrain_dataloader = DataLoader(train_dataset, batch_size=50, shuffle=True, collate_fn=molgraph_collate_fn)\nvalidation_dataset = MolGraphDataset('/kaggle/working/2019-nCov/Data/protease_valid.csv.gz')\nvalidation_dataloader = DataLoader(validation_dataset, batch_size=50, collate_fn=molgraph_collate_fn)\ntest_dataset = MolGraphDataset('/kaggle/working/2019-nCov/Data/protease_test.csv.gz')\ntest_dataloader = DataLoader(test_dataset, batch_size=50, collate_fn=molgraph_collate_fn)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"((sample_adjacency, sample_nodes, sample_edges), sample_target) = train_dataset[0]\n\nnet = EMNImplementation(node_features=len(sample_nodes[0]), \n                                                edge_features=len(sample_edges[0, 0]), \n                                                out_features=len(sample_target), \n                                                message_passes=8, edge_embedding_size=50, \n                                                edge_emb_depth=2, edge_emb_hidden_dim=150, \n                                                edge_emb_dropout_p=0.0, att_depth=2, att_hidden_dim=85, \n                                                att_dropout_p=0.0, msg_depth=2, msg_hidden_dim=150, \n                                                msg_dropout_p=0.0, gather_width=45, gather_att_depth=2, \n                                                gather_att_hidden_dim=45, gather_att_dropout_p=0.0, \n                                                gather_emb_depth=2, gather_emb_hidden_dim=45, \n                                                gather_emb_dropout_p=0.0, out_depth=2, out_hidden_dim=450, \n                                                out_dropout_p=0.1, out_layer_shrinkage=0.6)\n                                                \nif True:\n    net = net.cuda()\n\noptimizer = optim.Adam(net.parameters(), lr=1e-4)\ncriterion = nn.MSELoss()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import os\n# os.mkdir(\"/kaggle/working/hello\")\n# os.listdir(\"/kaggle/working\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAVEDMODELS_DIR = \"/kaggle/working/savedmodels/\"\nos.mkdir(SAVEDMODELS_DIR)\ndef evaluate_net(net, train_dataloader, validation_dataloader, test_dataloader, criterion):\n    global evaluate_called\n    global DATETIME_STR\n    global best_mean_train_score\n    global best_mean_validation_score\n    global best_mean_test_score\n    global train_subset_loader\n    \n    if not evaluate_called:\n        evaluate_called = True\n        best_mean_train_score, best_mean_validation_score, best_mean_test_score = 10, 10, 10\n        train_subset_loader = train_dataloader\n\n    train_output, train_loss, train_target = feed_net(net, train_subset_loader, criterion, True)\n    validation_output, validation_loss, validation_target = feed_net(net, validation_dataloader, criterion, True)\n    test_output, test_loss, test_target = feed_net(net, test_dataloader, criterion, True)\n\n    train_scores = compute_mse(train_output, train_target)\n    train_mean_score = np.nanmean(train_scores)\n    validation_scores = compute_mse(validation_output, validation_target)\n    validation_mean_score = np.nanmean(validation_scores)\n    test_scores = compute_mse(test_output, test_target)\n    test_mean_score = np.nanmean(test_scores)\n\n    new_best_model_found = validation_mean_score < best_mean_validation_score\n\n    if new_best_model_found:\n        best_mean_train_score = train_mean_score\n        best_mean_validation_score = validation_mean_score\n        best_mean_test_score = test_mean_score\n\n        path = SAVEDMODELS_DIR + type(net).__name__ + DATETIME_STR\n        torch.save(net, path)\n\n    target_names = train_dataloader.dataset.target_names\n    return {  # if made deeper, tensorboardx writing breaks I think\n        'loss': {'train': train_loss, 'test': test_loss},\n        'mean {}'.format(\"MSE\"):\n            {'train': train_mean_score, 'validation': validation_mean_score, 'test': test_mean_score},\n        'train {}s'.format(\"MSE\"): {target_names[i]: train_scores[i] for i in range(len(target_names))},\n        'test {}s'.format(\"MSE\"): {target_names[i]: test_scores[i] for i in range(len(target_names))},\n        'best mean {}'.format(\"MSE\"):\n            {'train': best_mean_train_score, 'validation': best_mean_validation_score, 'test': best_mean_test_score}\n    }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def less_log(net, train_dataloader, validation_dataloader, test_dataloader, criterion, epoch):\n    scalars = evaluate_net(net, train_dataloader, validation_dataloader, test_dataloader, criterion)\n    mean_score_key = 'mean {}'.format(\"MSE\")\n    print('epoch {}, training mean {}: {}, validation mean {}: {}, testing mean {}: {}'.format(\n        epoch + 1,\n        \"MSE\", scalars[mean_score_key]['train'],\n        \"MSE\", scalars[mean_score_key]['validation'],\n        \"MSE\", scalars[mean_score_key]['test'])\n    )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate_called = False\nbest_mean_train_score, best_mean_validation_score, best_mean_test_score = 10, 10, 10\ntrain_subset_loader = None\nDATETIME_STR = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S.%f')\n\nfor epoch in range(10):   # epoch in range(10)\n    net.train()\n    for i_batch, batch in enumerate(train_dataloader):\n\n        if True:\n            batch = [tensor.cuda() for tensor in batch]\n        adjacency, nodes, edges, target = batch\n\n        optimizer.zero_grad()\n        output = net(adjacency, nodes, edges)\n        loss = criterion(output, target)\n        loss.backward()\n        torch.nn.utils.clip_grad_value_(net.parameters(), 5.0)\n        optimizer.step()\n\n    with torch.no_grad():\n        net.eval()\n        less_log(net, train_dataloader, validation_dataloader, test_dataloader, criterion, epoch)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(test_set):\n    with torch.no_grad():\n        #Change this path to predict using different trained models\n        net = torch.load(\"/kaggle/working/2019-nCov/EMNN/savedmodels/EMNImplementation2020-03-01 17:49:07.529952\")\n        if True:\n            net = net.cuda()\n        else:\n            net = net.cpu()\n        net.eval()\n\n        dataset = MolGraphDataset(test_set, prediction=True)\n        dataloader = DataLoader(dataset, batch_size=50, collate_fn=molgraph_collate_fn)\n\n        batch_outputs = []\n        for i_batch, batch in enumerate(dataloader):\n            if True:\n                batch = [tensor.cuda() for tensor in batch]\n            adjacency, nodes, edges, target = batch\n            batch_output = net(adjacency, nodes, edges)\n            batch_outputs.append(batch_output)\n\n        output = torch.cat(batch_outputs).cpu().numpy()\n        \n        df = pd.read_csv(test_set)\n        \n        df.insert(1, 'pred_log_std_scaled', output, True)\n        \n        return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predict(\"/kaggle/working/2019-nCov/Data/protease_test.csv.gz\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The N3 ligand from the crystal structure was extracted and its smiles string was searched on pubchem to find similar structures, which were saves and their activities can be predicted using the newly trained model"},{"metadata":{"trusted":true},"cell_type":"code","source":"sim_compound = pd.read_csv(\"/kaggle/working/2019-nCov/Data/n3_similar_compounds.csv\")\ncids = list(sim_compound[['cid']].values.astype(\"int32\").squeeze())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#re-using some code from above, the \nmols = []\nfor CID in cids:\n    #os.system('curl https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/%s/sdf -o Data/cmp.sdf' %CID)\n    os.system('wget https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/cid/%s/sdf -O Data/cmp.sdf' %CID)\n    if os.stat(f'/kaggle/working/2019-nCov/Data/cmp.sdf').st_size != 0:\n        mols.append(Chem.SDMolSupplier(\"/kaggle/working/2019-nCov/Data/cmp.sdf\")[0])\n    else:\n        mols.append(None)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sim_df = pd.DataFrame(mols)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sim_df.insert(0, 'smiles', [Chem.MolToSmiles(x) for x in sim_df[[0]].values[:,0]], True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sim_df.insert(0, 'empty', [None]*len(sim_df), True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sim_df[[\"empty\",\"smiles\"]].to_csv(\"/kaggle/working/2019-nCov/Data/n3_similarity_test.csv.gz\", \n                                  index=False, compression='gzip', sep='\\t')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predict(\"/kaggle/working/2019-nCov/Data/n3_similarity_test.csv.gz\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predictions = predict(\"/kaggle/working/2019-nCov/Data/n3_similarity_test.csv.gz\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"predictions.sort_values(\"pred_log_std_scaled\", ascending=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Because we have no way of verifying the accuracy of the predicted values for these compounds, we're going to instead just take the 10 highest predicted compounds and dock them using Autodock, By doing this we actually don't even need to re-scale the scaled standard activity value, we can just take the compounds knowing that why were predicted to be the best"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Keeping the best 10 compounds\nbest_predicted = predictions[[\"empty\\tsmiles\"]].values[:10,0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Removing a strangly appearing tab character from the front of each string\nbest_predicted = [best_predicted[i][1:] for i in range(len(best_predicted))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(best_predicted, open(\"/kaggle/working/2019-nCov/Data/best_predicted_smiles.pkl\", \"wb\"))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 5. Molecular Generation I - Constrained Graph Variational Autoencoder"},{"metadata":{},"cell_type":"markdown","source":"They describe a variational autoencoder trained directly on molecular graphs. More details on VAE's can be found here: (kigma paper). Essentially, a network learns a latent representation of the distribution of the training data that is normal with respect to each of the dimensions of the latent space. A new vector can be sampled from this latent distibution and then be constructed into a molecule,"},{"metadata":{},"cell_type":"markdown","source":"Outline of this section: First we train the model using the dataset obtained in section 3. Using the built-in generation phase of the VAE, we generate new compounds and consider some of them as candidiates. The design of the method allows for optimization in the latent space, by doing gradient accent on a target value. This was not employed and random sampling generation was used."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install docopt typing planarity\n!git clone https://github.com/microsoft/constrained-graph-variational-autoencoder.git","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"################   utils.py\n\nimport planarity\nimport sascorer\n# !/usr/bin/env/python\nimport tensorflow as tf\nfrom rdkit import Chem\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Crippen\nfrom rdkit.Chem import Draw\nfrom rdkit.Chem import QED\nfrom rdkit.Chem import rdmolops\n\nSMALL_NUMBER = 1e-7\nLARGE_NUMBER = 1e10\n\ngeometry_numbers = [3, 4, 5, 6]  # triangle, square, pentagen, hexagon\n\n# bond mapping\nbond_dict = {'SINGLE': 0, 'DOUBLE': 1, 'TRIPLE': 2, \"AROMATIC\": 3}\nnumber_to_bond = {0: Chem.rdchem.BondType.SINGLE, 1: Chem.rdchem.BondType.DOUBLE,\n                  2: Chem.rdchem.BondType.TRIPLE, 3: Chem.rdchem.BondType.AROMATIC}\n\n\ndef dataset_info(dataset):  # qm9, zinc, cep\n    if dataset == 'qm9':\n        return {'atom_types': [\"H\", \"C\", \"N\", \"O\", \"F\"],\n                'maximum_valence': {0: 1, 1: 4, 2: 3, 3: 2, 4: 1},\n                'number_to_atom': {0: \"H\", 1: \"C\", 2: \"N\", 3: \"O\", 4: \"F\"},\n                'bucket_sizes': np.array(list(range(4, 28, 2)) + [29])\n                }\n    elif dataset == 'zinc':\n        return {'atom_types': ['Br1(0)', 'C4(0)', 'Cl1(0)', 'F1(0)', 'H1(0)', 'I1(0)',\n                               'N2(-1)', 'N3(0)', 'N4(1)', 'O1(-1)', 'O2(0)', 'S2(0)', 'S4(0)', 'S6(0)'],\n                'maximum_valence': {0: 1, 1: 4, 2: 1, 3: 1, 4: 1, 5: 1, 6: 2, 7: 3, 8: 4, 9: 1, 10: 2, 11: 2, 12: 4,\n                                    13: 6, 14: 3},\n                'number_to_atom': {0: 'Br', 1: 'C', 2: 'Cl', 3: 'F', 4: 'H', 5: 'I', 6: 'N', 7: 'N', 8: 'N', 9: 'O',\n                                   10: 'O', 11: 'S', 12: 'S', 13: 'S'},\n                'bucket_sizes': np.array(\n                    [28, 31, 33, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 53, 55, 58, 84])\n                }\n\n    elif dataset == \"cep\":\n        return {'atom_types': [\"C\", \"S\", \"N\", \"O\", \"Se\", \"Si\"],\n                'maximum_valence': {0: 4, 1: 2, 2: 3, 3: 2, 4: 2, 5: 4},\n                'number_to_atom': {0: \"C\", 1: \"S\", 2: \"N\", 3: \"O\", 4: \"Se\", 5: \"Si\"},\n                'bucket_sizes': np.array([25, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 43, 46])\n                }\n    else:\n        print(\"the datasets in use are qm9|zinc|cep\")\n        exit(1)\n\n\n# add one edge to adj matrix\ndef add_edge_mat(amat, src, dest, e, considering_edge_type=True):\n    if considering_edge_type:\n        amat[e, dest, src] = 1\n        amat[e, src, dest] = 1\n    else:\n        amat[src, dest] = 1\n        amat[dest, src] = 1\n\n\ndef graph_to_adj_mat(graph, max_n_vertices, num_edge_types, tie_fwd_bkwd=True, considering_edge_type=True):\n    if considering_edge_type:\n        amat = np.zeros((num_edge_types, max_n_vertices, max_n_vertices))\n        for src, e, dest in graph:\n            add_edge_mat(amat, src, dest, e)\n    else:\n        amat = np.zeros((max_n_vertices, max_n_vertices))\n        for src, e, dest in graph:\n            add_edge_mat(amat, src, dest, e, considering_edge_type=False)\n    return amat\n\n\ndef check_edge_prob(dataset):\n    with open('intermediate_results_%s' % dataset, 'rb') as f:\n        adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance = pickle.load(\n            f)\n    for ep, epl in zip(edge_prob, edge_prob_label):\n        print(\"prediction\")\n        print(ep)\n        print(\"label\")\n        print(epl)\n\n\n# check whether a graph is planar or not\ndef is_planar(location, adj_list, is_dense=False):\n    if is_dense:\n        new_adj_list = defaultdict(list)\n        for x in range(len(adj_list)):\n            for y in range(len(adj_list)):\n                if adj_list[x][y] == 1:\n                    new_adj_list[x].append((y, 1))\n        adj_list = new_adj_list\n    edges = []\n    seen = set()\n    for src, l in adj_list.items():\n        for dst, e in l:\n            if (dst, src) not in seen:\n                edges.append((src, dst))\n                seen.add((src, dst))\n    edges += [location, (location[1], location[0])]\n    return planarity.is_planar(edges)\n\n\ndef check_edge_type_prob(filter=None):\n    with open('intermediate_results_%s' % dataset, 'rb') as f:\n        adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance = pickle.load(\n            f)\n    for ep, epl in zip(edge_type_prob, edge_type_label):\n        print(\"prediction\")\n        print(ep)\n        print(\"label\")\n        print(epl)\n\n\ndef check_mean(dataset, filter=None):\n    with open('intermediate_results_%s' % dataset, 'rb') as f:\n        adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance = pickle.load(\n            f)\n    print(mean.tolist()[:40])\n\n\ndef check_variance(dataset, filter=None):\n    with open('intermediate_results_%s' % dataset, 'rb') as f:\n        adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance = pickle.load(\n            f)\n    print(np.exp(logvariance).tolist()[:40])\n\n\ndef check_node_prob(filter=None):\n    print(dataset)\n    with open('intermediate_results_%s' % dataset, 'rb') as f:\n        adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance = pickle.load(\n            f)\n    print(node_symbol_prob[0])\n    print(node_symbol[0])\n    print(node_symbol_prob.shape)\n\n\ndef check_qed(filter=None):\n    with open('intermediate_results_%s' % dataset, 'rb') as f:\n        adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance = pickle.load(\n            f)\n    print(qed_prediction)\n    print(qed_labels[0])\n    print(np.mean(np.abs(qed_prediction - qed_labels[0])))\n\n\ndef onehot(idx, len):\n    z = [0 for _ in range(len)]\n    z[idx] = 1\n    return z\n\n\ndef generate_empty_adj_matrix(maximum_vertice_num):\n    return np.zeros((1, 3, maximum_vertice_num, maximum_vertice_num))\n\n\n# standard normal with shape [a1, a2, a3]\ndef generate_std_normal(a1, a2, a3):\n    return np.random.normal(0, 1, [a1, a2, a3])\n\n\ndef check_validity(dataset):\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    count = 0\n    for smiles in all_smiles:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is not None:\n            count += 1\n    return len(all_smiles), count\n\n\n# Get length for each graph based on node masks\ndef get_graph_length(all_node_mask):\n    all_lengths = []\n    for graph in all_node_mask:\n        if 0 in graph:\n            length = np.argmin(graph)\n        else:\n            length = len(graph)\n        all_lengths.append(length)\n    return all_lengths\n\n\ndef make_dir(path):\n    if not os.path.exists(path):\n        os.mkdir(path)\n        print('made directory %s' % path)\n\n\n# sample node symbols based on node predictions\ndef sample_node_symbol(all_node_symbol_prob, all_lengths, dataset):\n    all_node_symbol = []\n    for graph_idx, graph_prob in enumerate(all_node_symbol_prob):\n        node_symbol = []\n        for node_idx in range(all_lengths[graph_idx]):\n            symbol = np.random.choice(np.arange(len(dataset_info(dataset)['atom_types'])), p=graph_prob[node_idx])\n            node_symbol.append(symbol)\n        all_node_symbol.append(node_symbol)\n    return all_node_symbol\n\n\ndef dump(file_name, content):\n    with open(file_name, 'wb') as out_file:\n        pickle.dump(content, out_file, pickle.HIGHEST_PROTOCOL)\n\n\ndef load(file_name):\n    with open(file_name, 'rb') as f:\n        return pickle.load(f)\n\n    # generate a new feature on whether adding the edges will generate more than two overlapped edges for rings\n\n\ndef get_overlapped_edge_feature(edge_mask, color, new_mol):\n    overlapped_edge_feature = []\n    for node_in_focus, neighbor in edge_mask:\n        if color[neighbor] == 1:\n            # attempt to add the edge\n            new_mol.AddBond(int(node_in_focus), int(neighbor), number_to_bond[0])\n            # Check whether there are two cycles having more than two overlap edges\n            try:\n                ssr = Chem.GetSymmSSSR(new_mol)\n            except:\n                ssr = []\n            overlap_flag = False\n            for idx1 in range(len(ssr)):\n                for idx2 in range(idx1 + 1, len(ssr)):\n                    if len(set(ssr[idx1]) & set(ssr[idx2])) > 2:\n                        overlap_flag = True\n            # remove that edge\n            new_mol.RemoveBond(int(node_in_focus), int(neighbor))\n            if overlap_flag:\n                overlapped_edge_feature.append((node_in_focus, neighbor))\n    return overlapped_edge_feature\n\n\n# adj_list [3, v, v] or defaultdict. bfs distance on a graph\ndef bfs_distance(start, adj_list, is_dense=False):\n    distances = {}\n    visited = set()\n    queue = deque([(start, 0)])\n    visited.add(start)\n    while len(queue) != 0:\n        current, d = queue.popleft()\n        for neighbor, edge_type in adj_list[current]:\n            if neighbor not in visited:\n                distances[neighbor] = d + 1\n                visited.add(neighbor)\n                queue.append((neighbor, d + 1))\n    return [(start, node, d) for node, d in distances.items()]\n\n\ndef get_initial_valence(node_symbol, dataset):\n    return [dataset_info(dataset)['maximum_valence'][s] for s in node_symbol]\n\n\ndef add_atoms(new_mol, node_symbol, dataset):\n    for number in node_symbol:\n        if dataset == 'qm9' or dataset == 'cep':\n            idx = new_mol.AddAtom(Chem.Atom(dataset_info(dataset)['number_to_atom'][number]))\n        elif dataset == 'zinc':\n            new_atom = Chem.Atom(dataset_info(dataset)['number_to_atom'][number])\n            charge_num = int(dataset_info(dataset)['atom_types'][number].split('(')[1].strip(')'))\n            new_atom.SetFormalCharge(charge_num)\n            new_mol.AddAtom(new_atom)\n\n\ndef visualize_mol(path, new_mol):\n    AllChem.Compute2DCoords(new_mol)\n    print(path)\n    Draw.MolToFile(new_mol, path)\n\n\ndef get_idx_of_largest_frag(frags):\n    return np.argmax([len(frag) for frag in frags])\n\n\ndef remove_extra_nodes(new_mol):\n    frags = Chem.rdmolops.GetMolFrags(new_mol)\n    while len(frags) > 1:\n        # Get the idx of the frag with largest length\n        largest_idx = get_idx_of_largest_frag(frags)\n        for idx in range(len(frags)):\n            if idx != largest_idx:\n                # Remove one atom that is not in the largest frag\n                new_mol.RemoveAtom(frags[idx][0])\n                break\n        frags = Chem.rdmolops.GetMolFrags(new_mol)\n\n\ndef novelty_metric(dataset):\n    with open('all_smiles_%s.pkl' % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        generated_all_smiles = set(pickle.load(f))\n    total_new_molecules = 0\n    for generated_smiles in generated_all_smiles:\n        if generated_smiles not in all_smiles:\n            total_new_molecules += 1\n\n    return float(total_new_molecules) / len(generated_all_smiles)\n\n\ndef count_edge_type(dataset, generated=True):\n    if generated:\n        filename = 'generated_smiles_%s' % dataset\n    else:\n        filename = 'all_smiles_%s.pkl' % dataset\n    with open(filename, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n\n    counter = defaultdict(int)\n    edge_type_per_molecule = []\n    for smiles in all_smiles:\n        nodes, edges = to_graph(smiles, dataset)\n        edge_type_this_molecule = [0] * len(bond_dict)\n        for edge in edges:\n            edge_type = edge[1]\n            edge_type_this_molecule[edge_type] += 1\n            counter[edge_type] += 1\n        edge_type_per_molecule.append(edge_type_this_molecule)\n    total_sum = 0\n    return len(all_smiles), counter, edge_type_per_molecule\n\n\ndef need_kekulize(mol):\n    for bond in mol.GetBonds():\n        if bond_dict[str(bond.GetBondType())] >= 3:\n            return True\n    return False\n\n\ndef check_planar(dataset):\n    with open(\"generated_smiles_%s\" % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    total_non_planar = 0\n    for smiles in all_smiles:\n        try:\n            nodes, edges = to_graph(smiles, dataset)\n        except:\n            continue\n        edges = [(src, dst) for src, e, dst in edges]\n        if edges == []:\n            continue\n\n        if not planarity.is_planar(edges):\n            total_non_planar += 1\n    return len(all_smiles), total_non_planar\n\n\ndef count_atoms(dataset):\n    with open(\"generated_smiles_%s\" % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    counter = defaultdict(int)\n    atom_count_per_molecule = []  # record the counts for each molecule\n    for smiles in all_smiles:\n        try:\n            nodes, edges = to_graph(smiles, dataset)\n        except:\n            continue\n        atom_count_this_molecule = [0] * len(dataset_info(dataset)['atom_types'])\n        for node in nodes:\n            atom_type = np.argmax(node)\n            atom_count_this_molecule[atom_type] += 1\n            counter[atom_type] += 1\n        atom_count_per_molecule.append(atom_count_this_molecule)\n    total_sum = 0\n\n    return len(all_smiles), counter, atom_count_per_molecule\n\n\ndef to_graph(smiles, dataset):\n    mol = Chem.MolFromSmiles(smiles)\n    if mol is None:\n        return [], []\n    # Kekulize it\n    if need_kekulize(mol):\n        rdmolops.Kekulize(mol)\n        if mol is None:\n            return None, None\n    # remove stereo information, such as inward and outward edges\n    Chem.RemoveStereochemistry(mol)\n\n    edges = []\n    nodes = []\n    for bond in mol.GetBonds():\n        edges.append((bond.GetBeginAtomIdx(), bond_dict[str(bond.GetBondType())], bond.GetEndAtomIdx()))\n        assert bond_dict[str(bond.GetBondType())] != 3\n    for atom in mol.GetAtoms():\n        if dataset == 'qm9' or dataset == \"cep\":\n            nodes.append(onehot(dataset_info(dataset)['atom_types'].index(atom.GetSymbol()),\n                                len(dataset_info(dataset)['atom_types'])))\n        elif dataset == 'zinc':  # transform using \"<atom_symbol><valence>(<charge>)\"  notation\n            symbol = atom.GetSymbol()\n            valence = atom.GetTotalValence()\n            charge = atom.GetFormalCharge()\n            atom_str = \"%s%i(%i)\" % (symbol, valence, charge)\n\n            if atom_str not in dataset_info(dataset)['atom_types']:\n                print('unrecognized atom type %s' % atom_str)\n                return [], []\n\n            nodes.append(\n                onehot(dataset_info(dataset)['atom_types'].index(atom_str), len(dataset_info(dataset)['atom_types'])))\n\n    return nodes, edges\n\n\ndef check_uniqueness(dataset):\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        all_smiles = pickle.load(f)\n    original_num = len(all_smiles)\n    all_smiles = set(all_smiles)\n    new_num = len(all_smiles)\n    return new_num / original_num\n\n\ndef shape_count(dataset, remove_print=False, all_smiles=None):\n    if all_smiles == None:\n        with open('generated_smiles_%s' % dataset, 'rb') as f:\n            all_smiles = set(pickle.load(f))\n\n    geometry_counts = [0] * len(geometry_numbers)\n    geometry_counts_per_molecule = []  # record the geometry counts for each molecule\n    for smiles in all_smiles:\n        nodes, edges = to_graph(smiles, dataset)\n        if len(edges) <= 0:\n            continue\n        new_mol = Chem.MolFromSmiles(smiles)\n\n        ssr = Chem.GetSymmSSSR(new_mol)\n        counts_for_molecule = [0] * len(geometry_numbers)\n        for idx in range(len(ssr)):\n            ring_len = len(list(ssr[idx]))\n            if ring_len in geometry_numbers:\n                geometry_counts[geometry_numbers.index(ring_len)] += 1\n                counts_for_molecule[geometry_numbers.index(ring_len)] += 1\n        geometry_counts_per_molecule.append(counts_for_molecule)\n\n    return len(all_smiles), geometry_counts, geometry_counts_per_molecule\n\n\ndef check_adjacent_sparse(adj_list, node, neighbor_in_doubt):\n    for neighbor, edge_type in adj_list[node]:\n        if neighbor == neighbor_in_doubt:\n            return True, edge_type\n    return False, None\n\n\ndef glorot_init(shape):\n    initialization_range = np.sqrt(6.0 / (shape[-2] + shape[-1]))\n    return np.random.uniform(low=-initialization_range, high=initialization_range, size=shape).astype(np.float32)\n\n\nclass ThreadedIterator:\n    \"\"\"An iterator object that computes its elements in a parallel thread to be ready to be consumed.\n    The iterator should *not* return None\"\"\"\n\n    def __init__(self, original_iterator, max_queue_size: int = 2):\n        self.__queue = queue.Queue(maxsize=max_queue_size)\n        self.__thread = threading.Thread(target=lambda: self.worker(original_iterator))\n        self.__thread.start()\n\n    def worker(self, original_iterator):\n        for element in original_iterator:\n            assert element is not None, 'By convention, iterator elements much not be None'\n            self.__queue.put(element, block=True)\n        self.__queue.put(None, block=True)\n\n    def __iter__(self):\n        next_element = self.__queue.get(block=True)\n        while next_element is not None:\n            yield next_element\n            next_element = self.__queue.get(block=True)\n        self.__thread.join()\n\n\n# Implements multilayer perceptron\nclass MLP(object):\n    def __init__(self, in_size, out_size, hid_sizes, dropout_keep_prob):\n        self.in_size = in_size\n        self.out_size = out_size\n        self.hid_sizes = hid_sizes\n        self.dropout_keep_prob = dropout_keep_prob\n        self.params = self.make_network_params()\n\n    def make_network_params(self):\n        dims = [self.in_size] + self.hid_sizes + [self.out_size]\n        weight_sizes = list(zip(dims[:-1], dims[1:]))\n        weights = [tf.Variable(self.init_weights(s), name='MLP_W_layer%i' % i)\n                   for (i, s) in enumerate(weight_sizes)]\n        biases = [tf.Variable(np.zeros(s[-1]).astype(np.float32), name='MLP_b_layer%i' % i)\n                  for (i, s) in enumerate(weight_sizes)]\n\n        network_params = {\n            \"weights\": weights,\n            \"biases\": biases,\n        }\n\n        return network_params\n\n    def init_weights(self, shape):\n        return np.sqrt(6.0 / (shape[-2] + shape[-1])) * (2 * np.random.rand(*shape).astype(np.float32) - 1)\n\n    def __call__(self, inputs):\n        acts = inputs\n        for W, b in zip(self.params[\"weights\"], self.params[\"biases\"]):\n            hid = tf.matmul(acts, tf.nn.dropout(W, self.dropout_keep_prob)) + b\n            acts = tf.nn.relu(hid)\n        last_hidden = hid\n        return last_hidden\n\n\nclass Graph():\n\n    def __init__(self, V, g):\n        self.V = V\n        self.graph = g\n\n    def addEdge(self, v, w):\n        # Add w to v ist.\n        self.graph[v].append(w)\n        # Add v to w list.\n        self.graph[w].append(v)\n\n        # A recursive function that uses visited[] \n\n    # and parent to detect cycle in subgraph \n    # reachable from vertex v.\n    def isCyclicUtil(self, v, visited, parent):\n\n        # Mark current node as visited\n        visited[v] = True\n\n        # Recur for all the vertices adjacent \n        # for this vertex\n        for i in self.graph[v]:\n            # If an adjacent is not visited, \n            # then recur for that adjacent\n            if visited[i] == False:\n                if self.isCyclicUtil(i, visited, v) == True:\n                    return True\n\n            # If an adjacent is visited and not \n            # parent of current vertex, then there \n            # is a cycle.\n            elif i != parent:\n                return True\n\n        return False\n\n    # Returns true if the graph is a tree, \n    # else false.\n    def isTree(self):\n        # Mark all the vertices as not visited \n        # and not part of recursion stack\n        visited = [False] * self.V\n\n        # The call to isCyclicUtil serves multiple \n        # purposes. It returns true if graph reachable \n        # from vertex 0 is cyclcic. It also marks \n        # all vertices reachable from 0.\n        if self.isCyclicUtil(0, visited, -1) == True:\n            return False\n\n        # If we find a vertex which is not reachable\n        # from 0 (not marked by isCyclicUtil(), \n        # then we return false\n        for i in range(self.V):\n            if visited[i] == False:\n                return False\n\n        return True\n\n\n# whether whether the graphs has no cycle or not \ndef check_cyclic(dataset, generated=True):\n    if generated:\n        with open(\"generated_smiles_%s\" % dataset, 'rb') as f:\n            all_smiles = set(pickle.load(f))\n    else:\n        with open(\"all_smiles_%s.pkl\" % dataset, 'rb') as f:\n            all_smiles = set(pickle.load(f))\n\n    tree_count = 0\n    for smiles in all_smiles:\n        nodes, edges = to_graph(smiles, dataset)\n        edges = [(src, dst) for src, e, dst in edges]\n        if edges == []:\n            continue\n        new_adj_list = defaultdict(list)\n\n        for src, dst in edges:\n            new_adj_list[src].append(dst)\n            new_adj_list[dst].append(src)\n        graph = Graph(len(nodes), new_adj_list)\n        if graph.isTree():\n            tree_count += 1\n    return len(all_smiles), tree_count\n\n\ndef check_sascorer(dataset):\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    sa_sum = 0\n    total = 0\n    sa_score_per_molecule = []\n    for smiles in all_smiles:\n        new_mol = Chem.MolFromSmiles(smiles)\n        try:\n            val = sascorer.calculateScore(new_mol)\n        except:\n            continue\n        sa_sum += val\n        sa_score_per_molecule.append(val)\n        total += 1\n    return sa_sum / total, sa_score_per_molecule\n\n\ndef check_logp(dataset):\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    logp_sum = 0\n    total = 0\n    logp_score_per_molecule = []\n    for smiles in all_smiles:\n        new_mol = Chem.MolFromSmiles(smiles)\n        try:\n            val = Crippen.MolLogP(new_mol)\n        except:\n            continue\n        logp_sum += val\n        logp_score_per_molecule.append(val)\n        total += 1\n    return logp_sum / total, logp_score_per_molecule\n\n\ndef check_qed(dataset):\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    qed_sum = 0\n    total = 0\n    qed_score_per_molecule = []\n    for smiles in all_smiles:\n        new_mol = Chem.MolFromSmiles(smiles)\n        try:\n            val = QED.qed(new_mol)\n        except:\n            continue\n        qed_sum += val\n        qed_score_per_molecule.append(val)\n        total += 1\n    return qed_sum / total, qed_score_per_molecule\n\n\ndef sssr_metric(dataset):\n    with open('generated_smiles_%s' % dataset, 'rb') as f:\n        all_smiles = set(pickle.load(f))\n    overlapped_molecule = 0\n    for smiles in all_smiles:\n        new_mol = Chem.MolFromSmiles(smiles)\n        ssr = Chem.GetSymmSSSR(new_mol)\n        overlap_flag = False\n        for idx1 in range(len(ssr)):\n            for idx2 in range(idx1 + 1, len(ssr)):\n                if len(set(ssr[idx1]) & set(ssr[idx2])) > 2:\n                    overlap_flag = True\n        if overlap_flag:\n            overlapped_molecule += 1\n    return overlapped_molecule / len(all_smiles)\n\n\n# select the best based on shapes and probs\ndef select_best(all_mol):\n    # sort by shape\n    all_mol = sorted(all_mol)\n    best_shape = all_mol[-1][0]\n    all_mol = [(p, m) for s, p, m in all_mol if s == best_shape]\n    # sort by probs\n    all_mol = sorted(all_mol)\n    return all_mol[-1][1]\n\n\n# a series util function converting sparse matrix representation to dense \n\ndef incre_adj_mat_to_dense(incre_adj_mat, num_edge_types, maximum_vertice_num):\n    new_incre_adj_mat = []\n    for sparse_incre_adj_mat in incre_adj_mat:\n        dense_incre_adj_mat = np.zeros((num_edge_types, maximum_vertice_num, maximum_vertice_num))\n        for current, adj_list in sparse_incre_adj_mat.items():\n            for neighbor, edge_type in adj_list:\n                dense_incre_adj_mat[edge_type][current][neighbor] = 1\n        new_incre_adj_mat.append(dense_incre_adj_mat)\n    return new_incre_adj_mat  # [number_iteration,num_edge_types,maximum_vertice_num, maximum_vertice_num]\n\n\ndef distance_to_others_dense(distance_to_others, maximum_vertice_num):\n    new_all_distance = []\n    for sparse_distances in distance_to_others:\n        dense_distances = np.zeros((maximum_vertice_num), dtype=int)\n        for x, y, d in sparse_distances:\n            dense_distances[y] = d\n        new_all_distance.append(dense_distances)\n    return new_all_distance  # [number_iteration, maximum_vertice_num]\n\n\ndef overlapped_edge_features_to_dense(overlapped_edge_features, maximum_vertice_num):\n    new_overlapped_edge_features = []\n    for sparse_overlapped_edge_features in overlapped_edge_features:\n        dense_overlapped_edge_features = np.zeros((maximum_vertice_num), dtype=int)\n        for node_in_focus, neighbor in sparse_overlapped_edge_features:\n            dense_overlapped_edge_features[neighbor] = 1\n        new_overlapped_edge_features.append(dense_overlapped_edge_features)\n    return new_overlapped_edge_features  # [number_iteration, maximum_vertice_num]\n\n\ndef node_sequence_to_dense(node_sequence, maximum_vertice_num):\n    new_node_sequence = []\n    for node in node_sequence:\n        s = [0] * maximum_vertice_num\n        s[node] = 1\n        new_node_sequence.append(s)\n    return new_node_sequence  # [number_iteration, maximum_vertice_num]\n\n\ndef edge_type_masks_to_dense(edge_type_masks, maximum_vertice_num, num_edge_types):\n    new_edge_type_masks = []\n    for mask_sparse in edge_type_masks:\n        mask_dense = np.zeros([num_edge_types, maximum_vertice_num])\n        for node_in_focus, neighbor, bond in mask_sparse:\n            mask_dense[bond][neighbor] = 1\n        new_edge_type_masks.append(mask_dense)\n    return new_edge_type_masks  # [number_iteration, 3, maximum_vertice_num]\n\n\ndef edge_type_labels_to_dense(edge_type_labels, maximum_vertice_num, num_edge_types):\n    new_edge_type_labels = []\n    for labels_sparse in edge_type_labels:\n        labels_dense = np.zeros([num_edge_types, maximum_vertice_num])\n        for node_in_focus, neighbor, bond in labels_sparse:\n            labels_dense[bond][neighbor] = 1 / float(len(labels_sparse))  # fix the probability bug here.\n        new_edge_type_labels.append(labels_dense)\n    return new_edge_type_labels  # [number_iteration, 3, maximum_vertice_num]\n\n\ndef edge_masks_to_dense(edge_masks, maximum_vertice_num):\n    new_edge_masks = []\n    for mask_sparse in edge_masks:\n        mask_dense = [0] * maximum_vertice_num\n        for node_in_focus, neighbor in mask_sparse:\n            mask_dense[neighbor] = 1\n        new_edge_masks.append(mask_dense)\n    return new_edge_masks  # [number_iteration, maximum_vertice_num]\n\n\ndef edge_labels_to_dense(edge_labels, maximum_vertice_num):\n    new_edge_labels = []\n    for label_sparse in edge_labels:\n        label_dense = [0] * maximum_vertice_num\n        for node_in_focus, neighbor in label_sparse:\n            label_dense[neighbor] = 1 / float(len(label_sparse))\n        new_edge_labels.append(label_dense)\n    return new_edge_labels  # [number_iteration, maximum_vertice_num]\n\n\n# !/usr/bin/env/python\n\"\"\"\nUsage:\n    get_qm9.py\n\nOptions:\n    -h --help                Show this screen.\n\"\"\"\n\nimport os\nimport sys\n\nfrom GGNN_core import ChemModel\n\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))\n\ndataset = 'qm9'\n\n\ndef get_validation_file_names(unzip_path):\n    print('loading train/validation split')\n    with open('valid_idx_qm9.json', 'r') as f:\n        valid_idx = json.load(f)['valid_idxs']\n    valid_files = [os.path.join(unzip_path, 'dsgdb9nsd_%s.xyz' % i) for i in valid_idx]\n    return valid_files\n\n\ndef read_xyz(file_path):\n    with open(file_path, 'r') as f:\n        lines = f.readlines()\n        smiles = lines[-2].split('\\t')[0]\n        mu = QED.qed(Chem.MolFromSmiles(smiles))\n    return {'smiles': smiles, 'QED': mu}\n\n\ndef train_valid_split(unzip_path):\n    print('reading data...')\n    raw_data = {'train': [], 'valid': []}  # save the train, valid dataset.\n    all_files = glob.glob(os.path.join(unzip_path, '*.xyz'))\n    valid_files = get_validation_file_names(unzip_path)\n\n    file_count = 0\n    for file_idx, file_path in enumerate(all_files):\n        if file_path not in valid_files:\n            raw_data['train'].append(read_xyz(file_path))\n        else:\n            raw_data['valid'].append(read_xyz(file_path))\n        file_count += 1\n        if file_count % 2000 == 0:\n            print('finished reading: %d' % file_count, end='\\r')\n    return raw_data\n\n\ndef preprocess(raw_data, dataset):\n    print('parsing smiles as graphs...')\n    processed_data = {'train': [], 'valid': []}\n\n    file_count = 0\n    for section in ['train', 'valid']:\n        all_smiles = []  # record all smiles in training dataset\n        for i, (smiles, QED) in enumerate([(mol['smiles'], mol['QED'])\n                                           for mol in raw_data[section]]):\n            nodes, edges = to_graph(smiles, dataset)\n            if len(edges) <= 0:\n                continue\n            processed_data[section].append({\n                'targets': [[(QED)]],\n                'graph': edges,\n                'node_features': nodes,\n                'smiles': smiles\n            })\n            all_smiles.append(smiles)\n            if file_count % 2000 == 0:\n                print('finished processing: %d' % file_count, end='\\r')\n            file_count += 1\n        print('%s: 100 %%      ' % (section))\n        # save the dataset\n        with open('molecules_%s_%s.json' % (section, dataset), 'w') as f:\n            json.dump(processed_data[section], f)\n        # save all molecules in the training dataset\n        if section == 'train':\n            utils.dump('smiles_%s.pkl' % dataset, all_smiles)\n\n\n# if __name__ == \"__main__\":\n#     # download   \n#     download_path = 'dsgdb9nsd.xyz.tar.bz2'\n#     if not os.path.exists(download_path):\n#         print('downloading data to %s ...' % download_path)\n#         source = 'https://ndownloader.figshare.com/files/3195389'\n#         os.system('wget -O %s %s' % (download_path, source))\n#         print('finished downloading')\n# \n#     # unzip\n#     unzip_path = 'qm9_raw'\n#     if not os.path.exists(unzip_path):\n#         print('extracting data to %s ...' % unzip_path)\n#         os.mkdir(unzip_path)\n#         os.system('tar xvjf %s -C %s' % (download_path, unzip_path))\n#         print('finished extracting')\n# \n#     raw_data = train_valid_split(unzip_path)\n#     preprocess(raw_data, dataset)\n\n######  get_zinc.py\n\n# !/usr/bin/env/python\n\"\"\"\nUsage:\n    get_data.py --dataset zinc|qm9|cep\n\nOptions:\n    -h --help                Show this screen.\n    --dataset NAME           Dataset name: zinc, qm9, cep\n\"\"\"\n\nimport sys, os\n\nsys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))\nimport glob\nimport csv\n\ndataset = \"zinc\"\n\n\ndef train_valid_split(download_path):\n    # load validation dataset\n    with open(\"valid_idx_zinc.json\", 'r') as f:\n        valid_idx = json.load(f)\n\n    print('reading data...')\n    raw_data = {'train': [], 'valid': []}  # save the train, valid dataset.\n    with open(download_path, 'r') as f:\n        all_data = list(csv.DictReader(f))\n\n    file_count = 0\n    for i, data_item in enumerate(all_data):\n        smiles = data_item['smiles'].strip()\n        QED = float(data_item['qed'])\n        if i not in valid_idx:\n            raw_data['train'].append({'smiles': smiles, 'QED': QED})\n        else:\n            raw_data['valid'].append({'smiles': smiles, 'QED': QED})\n        file_count += 1\n        if file_count % 2000 == 0:\n            print('finished reading: %d' % file_count, end='\\r')\n    return raw_data\n\n\n# \n# if __name__ == \"__main__\":\n#     download_path = '250k_rndm_zinc_drugs_clean_3.csv'\n#     if not os.path.exists(download_path):\n#         print('downloading data to %s ...' % download_path)\n#         source = 'https://raw.githubusercontent.com/aspuru-guzik-group/chemical_vae/master/models/zinc_properties/250k_rndm_zinc_drugs_clean_3.csv'\n#         os.system('wget -O %s %s' % (download_path, source))\n#         print('finished downloading')\n# \n#     raw_data = train_valid_split(download_path)\n#     preprocess(raw_data, dataset)\n\n\n#######################   CGVAE.py\n\n# !/usr/bin/env/python\n\"\"\"\nUsage:\n    CGVAE.py [options]\n\nOptions:\n    -h --help                Show this screen\n    --dataset NAME           Dataset name: zinc, qm9, cep\n    --config-file FILE       Hyperparameter configuration file path (in JSON format)\n    --config CONFIG          Hyperparameter configuration dictionary (in JSON format)\n    --log_dir NAME           log dir name\n    --data_dir NAME          data dir name\n    --restore FILE           File to restore weights from.\n    --freeze-graph-model     Freeze weights of graph model components\n\"\"\"\n\n'''\nComments provide the expected tensor shapes where helpful.\n\nKey to symbols in comments:\n---------------------------\n[...]:  a tensor\n; ; :   a list\nb:      batch size\ne:      number of edege types (3)\nes:     maximum number of BFS transitions in this batch\nv:      number of vertices per graph in this batch\nh:      GNN hidden size\n'''\n\n\nclass DenseGGNNChemModel(ChemModel):\n    def __init__(self, args):\n        super().__init__(args)\n\n    @classmethod\n    def default_params(cls):\n        params = dict(super().default_params())\n        params.update({\n            'task_sample_ratios': {},\n            'use_edge_bias': True,  # whether use edge bias in gnn\n\n            'clamp_gradient_norm': 1.0,\n            'out_layer_dropout_keep_prob': 1.0,\n\n            'tie_fwd_bkwd': True,\n            'task_ids': [0],  # id of property prediction\n\n            'random_seed': 0,  # fixed for reproducibility \n\n            'batch_size': 8 if dataset == 'zinc' or dataset == 'cep' else 64,\n            \"qed_trade_off_lambda\": 10,\n            'prior_learning_rate': 0.05,\n            'stop_criterion': 0.01,\n            'num_epochs': 3 if dataset == 'zinc' or dataset == 'cep' else 10,\n            'epoch_to_generate': 3 if dataset == 'zinc' or dataset == 'cep' else 10,\n            'number_of_generation': 30000,\n            'optimization_step': 0,\n            'maximum_distance': 50,\n            \"use_argmax_generation\": False,  # use random sampling or argmax during generation\n            'residual_connection_on': True,  # whether residual connection is on\n            'residual_connections': {  # For iteration i, specify list of layers whose output is added as an input\n                2: [0],\n                4: [0, 2],\n                6: [0, 2, 4],\n                8: [0, 2, 4, 6],\n                10: [0, 2, 4, 6, 8],\n                12: [0, 2, 4, 6, 8, 10],\n                14: [0, 2, 4, 6, 8, 10, 12],\n            },\n            'num_timesteps': 12,  # gnn propagation step\n            'hidden_size': 100,\n            \"kl_trade_off_lambda\": 0.3,  # kl tradeoff\n            'learning_rate': 0.001,\n            'graph_state_dropout_keep_prob': 1,\n            \"compensate_num\": 1,  # how many atoms to be added during generation\n\n            'train_file': 'data/molecules_train_%s.json' % dataset,\n            'valid_file': 'data/molecules_valid_%s.json' % dataset,\n\n            'try_different_starting': True,\n            \"num_different_starting\": 6,\n\n            'generation': False,  # only for generation\n            'use_graph': True,  # use gnn\n            \"label_one_hot\": False,  # one hot label or not\n            \"multi_bfs_path\": False,  # whether sample several BFS paths for each molecule\n            \"bfs_path_count\": 30,\n            \"path_random_order\": False,  # False: canonical order, True: random order\n            \"sample_transition\": False,  # whether use transition sampling\n            'edge_weight_dropout_keep_prob': 1,\n            'check_overlap_edge': False,\n            \"truncate_distance\": 10,\n        })\n\n        return params\n\n    def prepare_specific_graph_model(self) -> None:\n        h_dim = self.params['hidden_size']\n        expanded_h_dim = self.params['hidden_size'] + self.params['hidden_size'] + 1  # 1 for focus bit\n        self.placeholders['graph_state_keep_prob'] = tf.placeholder(tf.float32, None, name='graph_state_keep_prob')\n        self.placeholders['edge_weight_dropout_keep_prob'] = tf.placeholder(tf.float32, None,\n                                                                            name='edge_weight_dropout_keep_prob')\n        self.placeholders['initial_node_representation'] = tf.placeholder(tf.float32,\n                                                                          [None, None, self.params['hidden_size']],\n                                                                          name='node_features')  # padded node symbols\n        # mask out invalid node\n        self.placeholders['node_mask'] = tf.placeholder(tf.float32, [None, None], name='node_mask')  # [b x v]\n        self.placeholders['num_vertices'] = tf.placeholder(tf.int32, ())\n        # adj for encoder\n        self.placeholders['adjacency_matrix'] = tf.placeholder(tf.float32,\n                                                               [None, self.num_edge_types, None, None],\n                                                               name=\"adjacency_matrix\")  # [b, e, v, v]\n        # labels for node symbol prediction\n        self.placeholders['node_symbols'] = tf.placeholder(tf.float32, [None, None, self.params[\n            'num_symbols']])  # [b, v, edge_type]\n        # node symbols used to enhance latent representations\n        self.placeholders['latent_node_symbols'] = tf.placeholder(tf.float32,\n                                                                  [None, None, self.params['hidden_size']],\n                                                                  name='latent_node_symbol')  # [b, v, h]\n        # mask out cross entropies in decoder\n        self.placeholders['iteration_mask'] = tf.placeholder(tf.float32, [None, None])  # [b, es]\n        # adj matrices used in decoder\n        self.placeholders['incre_adj_mat'] = tf.placeholder(tf.float32, [None, None, self.num_edge_types, None, None],\n                                                            name='incre_adj_mat')  # [b, es, e, v, v]\n        # distance \n        self.placeholders['distance_to_others'] = tf.placeholder(tf.int32, [None, None, None],\n                                                                 name='distance_to_others')  # [b, es,v]\n        # maximum iteration number of this batch\n        self.placeholders['max_iteration_num'] = tf.placeholder(tf.int32, [], name='max_iteration_num')  # number\n        # node number in focus at each iteration step\n        self.placeholders['node_sequence'] = tf.placeholder(tf.float32, [None, None, None],\n                                                            name='node_sequence')  # [b, es, v]\n        # mask out invalid edge types at each iteration step \n        self.placeholders['edge_type_masks'] = tf.placeholder(tf.float32, [None, None, self.num_edge_types, None],\n                                                              name='edge_type_masks')  # [b, es, e, v]\n        # ground truth edge type labels at each iteration step \n        self.placeholders['edge_type_labels'] = tf.placeholder(tf.float32, [None, None, self.num_edge_types, None],\n                                                               name='edge_type_labels')  # [b, es, e, v]\n        # mask out invalid edge at each iteration step \n        self.placeholders['edge_masks'] = tf.placeholder(tf.float32, [None, None, None],\n                                                         name='edge_masks')  # [b, es, v]\n        # ground truth edge labels at each iteration step \n        self.placeholders['edge_labels'] = tf.placeholder(tf.float32, [None, None, None],\n                                                          name='edge_labels')  # [b, es, v]        \n        # ground truth labels for whether it stops at each iteration step\n        self.placeholders['local_stop'] = tf.placeholder(tf.float32, [None, None], name='local_stop')  # [b, es]\n        # z_prior sampled from standard normal distribution\n        self.placeholders['z_prior'] = tf.placeholder(tf.float32, [None, None, self.params['hidden_size']],\n                                                      name='z_prior')  # the prior of z sampled from normal distribution\n        # put in front of kl latent loss\n        self.placeholders['kl_trade_off_lambda'] = tf.placeholder(tf.float32, [], name='kl_trade_off_lambda')  # number\n        # overlapped edge features\n        self.placeholders['overlapped_edge_features'] = tf.placeholder(tf.int32, [None, None, None],\n                                                                       name='overlapped_edge_features')  # [b, es, v]\n\n        # weights for encoder and decoder GNN. \n        if self.params[\"residual_connection_on\"]:\n            # weights for encoder and decoder GNN. Different weights for each iteration\n            for scope in ['_encoder', '_decoder']:\n                if scope == '_encoder':\n                    new_h_dim = h_dim\n                else:\n                    new_h_dim = expanded_h_dim\n                for iter_idx in range(self.params['num_timesteps']):\n                    with tf.variable_scope(\"gru_scope\" + scope + str(iter_idx), reuse=False):\n                        self.weights['edge_weights' + scope + str(iter_idx)] = tf.Variable(\n                            glorot_init([self.num_edge_types, new_h_dim, new_h_dim]))\n                        if self.params['use_edge_bias']:\n                            self.weights['edge_biases' + scope + str(iter_idx)] = tf.Variable(\n                                np.zeros([self.num_edge_types, 1, new_h_dim]).astype(np.float32))\n\n                        cell = tf.contrib.rnn.GRUCell(new_h_dim)\n                        cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n                                                             state_keep_prob=self.placeholders['graph_state_keep_prob'])\n                        self.weights['node_gru' + scope + str(iter_idx)] = cell\n        else:\n            for scope in ['_encoder', '_decoder']:\n                if scope == '_encoder':\n                    new_h_dim = h_dim\n                else:\n                    new_h_dim = expanded_h_dim\n                self.weights['edge_weights' + scope] = tf.Variable(\n                    glorot_init([self.num_edge_types, new_h_dim, new_h_dim]))\n                if self.params['use_edge_bias']:\n                    self.weights['edge_biases' + scope] = tf.Variable(\n                        np.zeros([self.num_edge_types, 1, new_h_dim]).astype(np.float32))\n                with tf.variable_scope(\"gru_scope\" + scope):\n                    cell = tf.contrib.rnn.GRUCell(new_h_dim)\n                    cell = tf.nn.rnn_cell.DropoutWrapper(cell,\n                                                         state_keep_prob=self.placeholders['graph_state_keep_prob'])\n                    self.weights['node_gru' + scope] = cell\n\n        # weights for calculating mean and variance\n        self.weights['mean_weights'] = tf.Variable(glorot_init([h_dim, h_dim]))\n        self.weights['mean_biases'] = tf.Variable(np.zeros([1, h_dim]).astype(np.float32))\n        self.weights['variance_weights'] = tf.Variable(glorot_init([h_dim, h_dim]))\n        self.weights['variance_biases'] = tf.Variable(np.zeros([1, h_dim]).astype(np.float32))\n\n        # The weights for generating nodel symbol logits    \n        self.weights['node_symbol_weights'] = tf.Variable(glorot_init([h_dim, self.params['num_symbols']]))\n        self.weights['node_symbol_biases'] = tf.Variable(np.zeros([1, self.params['num_symbols']]).astype(np.float32))\n\n        feature_dimension = 6 * expanded_h_dim\n        # record the total number of features\n        self.params[\"feature_dimension\"] = 6\n        # weights for generating edge type logits\n        for i in range(self.num_edge_types):\n            self.weights['edge_type_%d' % i] = tf.Variable(glorot_init([feature_dimension, feature_dimension]))\n            self.weights['edge_type_biases_%d' % i] = tf.Variable(np.zeros([1, feature_dimension]).astype(np.float32))\n            self.weights['edge_type_output_%d' % i] = tf.Variable(glorot_init([feature_dimension, 1]))\n        # weights for generating edge logits\n        self.weights['edge_iteration'] = tf.Variable(glorot_init([feature_dimension, feature_dimension]))\n        self.weights['edge_iteration_biases'] = tf.Variable(np.zeros([1, feature_dimension]).astype(np.float32))\n        self.weights['edge_iteration_output'] = tf.Variable(glorot_init([feature_dimension, 1]))\n        # Weights for the stop node\n        self.weights[\"stop_node\"] = tf.Variable(glorot_init([1, expanded_h_dim]))\n        # Weight for distance embedding\n        self.weights['distance_embedding'] = tf.Variable(glorot_init([self.params['maximum_distance'], expanded_h_dim]))\n        # Weight for overlapped edge feature\n        self.weights[\"overlapped_edge_weight\"] = tf.Variable(glorot_init([2, expanded_h_dim]))\n        # weights for linear projection on qed prediction input\n        self.weights['qed_weights'] = tf.Variable(glorot_init([h_dim, h_dim]))\n        self.weights['qed_biases'] = tf.Variable(np.zeros([1, h_dim]).astype(np.float32))\n        # use node embeddings\n        self.weights[\"node_embedding\"] = tf.Variable(glorot_init([self.params[\"num_symbols\"], h_dim]))\n\n        # graph state mask\n        self.ops['graph_state_mask'] = tf.expand_dims(self.placeholders['node_mask'], 2)\n\n    # transform one hot vector to dense embedding vectors\n    def get_node_embedding_state(self, one_hot_state):\n        node_nums = tf.argmax(one_hot_state, axis=2)\n        return tf.nn.embedding_lookup(self.weights[\"node_embedding\"], node_nums) * self.ops['graph_state_mask']\n\n    def compute_final_node_representations_with_residual(self, h, adj, scope_name):  # scope_name: _encoder or _decoder\n        # h: initial representation, adj: adjacency matrix, different GNN parameters for encoder and decoder\n        v = self.placeholders['num_vertices']\n        # _decoder uses a larger latent space because concat of symbol and latent representation\n        if scope_name == \"_decoder\":\n            h_dim = self.params['hidden_size'] + self.params['hidden_size'] + 1\n        else:\n            h_dim = self.params['hidden_size']\n        h = tf.reshape(h, [-1, h_dim])  # [b*v, h]\n        # record all hidden states at each iteration\n        all_hidden_states = [h]\n        for iter_idx in range(self.params['num_timesteps']):\n            with tf.variable_scope(\"gru_scope\" + scope_name + str(iter_idx), reuse=None) as g_scope:\n                for edge_type in range(self.num_edge_types):\n                    # the message passed from this vertice to other vertices\n                    m = tf.matmul(h, self.weights['edge_weights' + scope_name + str(iter_idx)][edge_type])  # [b*v, h]\n                    if self.params['use_edge_bias']:\n                        m += self.weights['edge_biases' + scope_name + str(iter_idx)][edge_type]  # [b, v, h]\n                    m = tf.reshape(m, [-1, v, h_dim])  # [b, v, h]\n                    # collect the messages from other vertices to each vertice\n                    if edge_type == 0:\n                        acts = tf.matmul(adj[edge_type], m)\n                    else:\n                        acts += tf.matmul(adj[edge_type], m)\n                # all messages collected for each node\n                acts = tf.reshape(acts, [-1, h_dim])  # [b*v, h]\n                # add residual connection here\n                layer_residual_connections = self.params['residual_connections'].get(iter_idx)\n                if layer_residual_connections is None:\n                    layer_residual_states = []\n                else:\n                    layer_residual_states = [all_hidden_states[residual_layer_idx]\n                                             for residual_layer_idx in layer_residual_connections]\n                # concat current hidden states with residual states\n                acts = tf.concat([acts] + layer_residual_states, axis=1)  # [b, (1+num residual connection)* h]\n\n                # feed msg inputs and hidden states to GRU\n                h = self.weights['node_gru' + scope_name + str(iter_idx)](acts, h)[1]  # [b*v, h]\n                # record the new hidden states\n                all_hidden_states.append(h)\n        last_h = tf.reshape(all_hidden_states[-1], [-1, v, h_dim])\n        return last_h\n\n    def compute_final_node_representations_without_residual(self, h, adj, edge_weights, edge_biases, node_gru,\n                                                            gru_scope_name):\n        # h: initial representation, adj: adjacency matrix, different GNN parameters for encoder and decoder\n        v = self.placeholders['num_vertices']\n        if gru_scope_name == \"gru_scope_decoder\":\n            h_dim = self.params['hidden_size'] + self.params['hidden_size']\n        else:\n            h_dim = self.params['hidden_size']\n        h = tf.reshape(h, [-1, h_dim])\n\n        with tf.variable_scope(gru_scope_name) as scope:\n            for i in range(self.params['num_timesteps']):\n                if i > 0:\n                    tf.get_variable_scope().reuse_variables()\n                for edge_type in range(self.num_edge_types):\n                    m = tf.matmul(h, tf.nn.dropout(edge_weights[edge_type],\n                                                   keep_prob=self.placeholders[\n                                                       'edge_weight_dropout_keep_prob']))  # [b*v, h]\n                    if self.params['use_edge_bias']:\n                        m += edge_biases[edge_type]  # [b, v, h]\n                    m = tf.reshape(m, [-1, v, h_dim])  # [b, v, h]\n                    if edge_type == 0:\n                        acts = tf.matmul(adj[edge_type], m)\n                    else:\n                        acts += tf.matmul(adj[edge_type], m)\n                acts = tf.reshape(acts, [-1, h_dim])  # [b*v, h]\n                h = node_gru(acts, h)[1]  # [b*v, h]\n            last_h = tf.reshape(h, [-1, v, h_dim])\n        return last_h\n\n    def compute_mean_and_logvariance(self):\n        h_dim = self.params['hidden_size']\n        reshped_last_h = tf.reshape(self.ops['final_node_representations'], [-1, h_dim])\n        mean = tf.matmul(reshped_last_h, self.weights['mean_weights']) + self.weights['mean_biases']\n        logvariance = tf.matmul(reshped_last_h, self.weights['variance_weights']) + self.weights['variance_biases']\n        return mean, logvariance\n\n    def sample_with_mean_and_logvariance(self):\n        v = self.placeholders['num_vertices']\n        h_dim = self.params['hidden_size']\n        # Sample from normal distribution\n        z_prior = tf.reshape(self.placeholders['z_prior'], [-1, h_dim])\n        # Train: sample from u, Sigma. Generation: sample from 0,1\n        z_sampled = tf.cond(self.placeholders['is_generative'], lambda: z_prior,  # standard normal \n                            lambda: tf.add(self.ops['mean'], tf.multiply(tf.sqrt(tf.exp(self.ops['logvariance'])),\n                                                                         z_prior)))  # non-standard normal\n        # filter\n        z_sampled = tf.reshape(z_sampled, [-1, v, h_dim]) * self.ops['graph_state_mask']\n        return z_sampled\n\n    def fully_connected(self, input, hidden_weight, hidden_bias, output_weight):\n        output = tf.nn.relu(tf.matmul(input, hidden_weight) + hidden_bias)\n        output = tf.matmul(output, output_weight)\n        return output\n\n    def generate_cross_entropy(self, idx, cross_entropy_losses, edge_predictions, edge_type_predictions):\n        v = self.placeholders['num_vertices']\n        h_dim = self.params['hidden_size']\n        num_symbols = self.params['num_symbols']\n        batch_size = tf.shape(self.placeholders['initial_node_representation'])[0]\n        # Use latent representation as decoder GNN'input \n        filtered_z_sampled = self.ops[\"initial_repre_for_decoder\"]  # [b, v, h+h]\n        # data needed in this iteration\n        incre_adj_mat = self.placeholders['incre_adj_mat'][:, idx, :, :, :]  # [b, e, v, v]\n        distance_to_others = self.placeholders['distance_to_others'][:, idx, :]  # [b,v]\n        overlapped_edge_features = self.placeholders['overlapped_edge_features'][:, idx, :]  # [b,v]\n        node_sequence = self.placeholders['node_sequence'][:, idx, :]  # [b, v]\n        node_sequence = tf.expand_dims(node_sequence, axis=2)  # [b,v,1]\n        edge_type_masks = self.placeholders['edge_type_masks'][:, idx, :, :]  # [b, e, v]\n        # make invalid locations to be very small before using softmax function\n        edge_type_masks = edge_type_masks * LARGE_NUMBER - LARGE_NUMBER\n        edge_type_labels = self.placeholders['edge_type_labels'][:, idx, :, :]  # [b, e, v]\n        edge_masks = self.placeholders['edge_masks'][:, idx, :]  # [b, v]\n        # make invalid locations to be very small before using softmax function\n        edge_masks = edge_masks * LARGE_NUMBER - LARGE_NUMBER\n        edge_labels = self.placeholders['edge_labels'][:, idx, :]  # [b, v]  \n        local_stop = self.placeholders['local_stop'][:, idx]  # [b]        \n        # concat the hidden states with the node in focus\n        filtered_z_sampled = tf.concat([filtered_z_sampled, node_sequence], axis=2)  # [b, v, h + h + 1]\n        # Decoder GNN\n        if self.params[\"use_graph\"]:\n            if self.params[\"residual_connection_on\"]:\n                new_filtered_z_sampled = self.compute_final_node_representations_with_residual(filtered_z_sampled,\n                                                                                               tf.transpose(\n                                                                                                   incre_adj_mat,\n                                                                                                   [1, 0, 2, 3]),\n                                                                                               \"_decoder\")  # [b, v, h + h]\n            else:\n                new_filtered_z_sampled = self.compute_final_node_representations_without_residual(filtered_z_sampled,\n                                                                                                  tf.transpose(\n                                                                                                      incre_adj_mat,\n                                                                                                      [1, 0, 2, 3]),\n                                                                                                  self.weights[\n                                                                                                      'edge_weights_decoder'],\n                                                                                                  self.weights[\n                                                                                                      'edge_biases_decoder'],\n                                                                                                  self.weights[\n                                                                                                      'node_gru_decoder'],\n                                                                                                  \"gru_scope_decoder\")  # [b, v, h + h]\n        else:\n            new_filtered_z_sampled = filtered_z_sampled\n        # Filter nonexist nodes\n        new_filtered_z_sampled = new_filtered_z_sampled * self.ops['graph_state_mask']\n        # Take out the node in focus\n        node_in_focus = tf.reduce_sum(node_sequence * new_filtered_z_sampled, axis=1)  # [b, h + h]\n        # edge pair representation\n        edge_repr = tf.concat( \\\n            [tf.tile(tf.expand_dims(node_in_focus, 1), [1, v, 1]), new_filtered_z_sampled],\n            axis=2)  # [b, v, 2*(h+h)]            \n        # combine edge repre with local and global repr\n        local_graph_repr_before_expansion = tf.reduce_sum(new_filtered_z_sampled, axis=1) / \\\n                                            tf.reduce_sum(self.placeholders['node_mask'], axis=1,\n                                                          keep_dims=True)  # [b, h + h]\n        local_graph_repr = tf.expand_dims(local_graph_repr_before_expansion, 1)\n        local_graph_repr = tf.tile(local_graph_repr, [1, v, 1])  # [b, v, h+h]        \n        global_graph_repr_before_expansion = tf.reduce_sum(filtered_z_sampled, axis=1) / \\\n                                             tf.reduce_sum(self.placeholders['node_mask'], axis=1, keep_dims=True)\n        global_graph_repr = tf.expand_dims(global_graph_repr_before_expansion, 1)\n        global_graph_repr = tf.tile(global_graph_repr, [1, v, 1])  # [b, v, h+h]\n        # distance representation\n        distance_repr = tf.nn.embedding_lookup(self.weights['distance_embedding'], distance_to_others)  # [b, v, h+h]\n        # overlapped edge feature representation\n        overlapped_edge_repr = tf.nn.embedding_lookup(self.weights['overlapped_edge_weight'],\n                                                      overlapped_edge_features)  # [b, v, h+h]\n        # concat and reshape.\n        combined_edge_repr = tf.concat([edge_repr, local_graph_repr,\n                                        global_graph_repr, distance_repr, overlapped_edge_repr], axis=2)\n\n        combined_edge_repr = tf.reshape(combined_edge_repr,\n                                        [-1, self.params[\"feature_dimension\"] * (h_dim + h_dim + 1)])\n        # Calculate edge logits\n        edge_logits = self.fully_connected(combined_edge_repr, self.weights['edge_iteration'],\n                                           self.weights['edge_iteration_biases'], self.weights['edge_iteration_output'])\n        edge_logits = tf.reshape(edge_logits, [-1, v])  # [b, v]\n        # filter invalid terms\n        edge_logits = edge_logits + edge_masks\n        # Calculate whether it will stop at this step\n        # prepare the data\n        expanded_stop_node = tf.tile(self.weights['stop_node'], [batch_size, 1])  # [b, h + h]\n        distance_to_stop_node = tf.nn.embedding_lookup(self.weights['distance_embedding'],\n                                                       tf.tile([0], [batch_size]))  # [b, h + h]\n        overlap_edge_stop_node = tf.nn.embedding_lookup(self.weights['overlapped_edge_weight'],\n                                                        tf.tile([0], [batch_size]))  # [b, h + h]\n\n        combined_stop_node_repr = tf.concat([node_in_focus, expanded_stop_node, local_graph_repr_before_expansion,\n                                             global_graph_repr_before_expansion, distance_to_stop_node,\n                                             overlap_edge_stop_node], axis=1)  # [b, 6 * (h + h)]\n        # logits for stop node                                    \n        stop_logits = self.fully_connected(combined_stop_node_repr,\n                                           self.weights['edge_iteration'], self.weights['edge_iteration_biases'],\n                                           self.weights['edge_iteration_output'])  # [b, 1]\n        edge_logits = tf.concat([edge_logits, stop_logits], axis=1)  # [b, v + 1]\n\n        # Calculate edge type logits\n        edge_type_logits = []\n        for i in range(self.num_edge_types):\n            edge_type_logit = self.fully_connected(combined_edge_repr,\n                                                   self.weights['edge_type_%d' % i],\n                                                   self.weights['edge_type_biases_%d' % i],\n                                                   self.weights[\n                                                       'edge_type_output_%d' % i])  # [b * v, 1]                        \n            edge_type_logits.append(tf.reshape(edge_type_logit, [-1, 1, v]))  # [b, 1, v]\n\n        edge_type_logits = tf.concat(edge_type_logits, axis=1)  # [b, e, v]\n        # filter invalid items\n        edge_type_logits = edge_type_logits + edge_type_masks  # [b, e, v]\n        # softmax over edge type axis\n        edge_type_probs = tf.nn.softmax(edge_type_logits, 1)  # [b, e, v]\n\n        # edge labels\n        edge_labels = tf.concat([edge_labels, tf.expand_dims(local_stop, 1)], axis=1)  # [b, v + 1]                \n        # softmax for edge\n        edge_loss = - tf.reduce_sum(tf.log(tf.nn.softmax(edge_logits) + SMALL_NUMBER) * edge_labels, axis=1)\n        # softmax for edge type \n        edge_type_loss = - edge_type_labels * tf.log(edge_type_probs + SMALL_NUMBER)  # [b, e, v]\n        edge_type_loss = tf.reduce_sum(edge_type_loss, axis=[1, 2])  # [b]\n        # total loss\n        iteration_loss = edge_loss + edge_type_loss\n        cross_entropy_losses = cross_entropy_losses.write(idx, iteration_loss)\n        edge_predictions = edge_predictions.write(idx, tf.nn.softmax(edge_logits))\n        edge_type_predictions = edge_type_predictions.write(idx, edge_type_probs)\n        return (idx + 1, cross_entropy_losses, edge_predictions, edge_type_predictions)\n\n    def construct_logit_matrices(self):\n        v = self.placeholders['num_vertices']\n        batch_size = tf.shape(self.placeholders['initial_node_representation'])[0]\n        h_dim = self.params['hidden_size']\n\n        # Initial state: embedding\n        latent_node_state = self.get_node_embedding_state(self.placeholders[\"latent_node_symbols\"])\n        # concat z_sampled with node symbols\n        filtered_z_sampled = tf.concat([self.ops['z_sampled'],\n                                        latent_node_state], axis=2)  # [b, v, h + h]\n        self.ops[\"initial_repre_for_decoder\"] = filtered_z_sampled\n        # The tensor array used to collect the cross entropy losses at each step\n        cross_entropy_losses = tf.TensorArray(dtype=tf.float32, size=self.placeholders['max_iteration_num'])\n        edge_predictions = tf.TensorArray(dtype=tf.float32, size=self.placeholders['max_iteration_num'])\n        edge_type_predictions = tf.TensorArray(dtype=tf.float32, size=self.placeholders['max_iteration_num'])\n        idx_final, cross_entropy_losses_final, edge_predictions_final, edge_type_predictions_final = \\\n            tf.while_loop(\n                lambda idx, cross_entropy_losses, edge_predictions, edge_type_predictions: idx < self.placeholders[\n                    'max_iteration_num'],\n                self.generate_cross_entropy,\n                (tf.constant(0), cross_entropy_losses, edge_predictions, edge_type_predictions,))\n\n        # record the predictions for generation\n        self.ops['edge_predictions'] = edge_predictions_final.read(0)\n        self.ops['edge_type_predictions'] = edge_type_predictions_final.read(0)\n\n        # final cross entropy losses\n        cross_entropy_losses_final = cross_entropy_losses_final.stack()\n        self.ops['cross_entropy_losses'] = tf.transpose(cross_entropy_losses_final, [1, 0])  # [b, es]\n\n        # Logits for node symbols\n        self.ops['node_symbol_logits'] = tf.reshape(\n            tf.matmul(tf.reshape(self.ops['z_sampled'], [-1, h_dim]), self.weights['node_symbol_weights']) +\n            self.weights['node_symbol_biases'], [-1, v, self.params['num_symbols']])\n\n    def construct_loss(self):\n        v = self.placeholders['num_vertices']\n        h_dim = self.params['hidden_size']\n        kl_trade_off_lambda = self.placeholders['kl_trade_off_lambda']\n        # Edge loss\n        self.ops[\"edge_loss\"] = tf.reduce_sum(self.ops['cross_entropy_losses'] * self.placeholders['iteration_mask'],\n                                              axis=1)\n        # KL loss \n        kl_loss = 1 + self.ops['logvariance'] - tf.square(self.ops['mean']) - tf.exp(self.ops['logvariance'])\n        kl_loss = tf.reshape(kl_loss, [-1, v, h_dim]) * self.ops['graph_state_mask']\n        self.ops['kl_loss'] = -0.5 * tf.reduce_sum(kl_loss, [1, 2])\n        # Node symbol loss\n        self.ops['node_symbol_prob'] = tf.nn.softmax(self.ops['node_symbol_logits'])\n        self.ops['node_symbol_loss'] = -tf.reduce_sum(tf.log(self.ops['node_symbol_prob'] + SMALL_NUMBER) *\n                                                      self.placeholders['node_symbols'], axis=[1, 2])\n        # Add in the loss for calculating QED\n        for (internal_id, task_id) in enumerate(self.params['task_ids']):\n            with tf.variable_scope(\"out_layer_task%i\" % task_id):\n                with tf.variable_scope(\"regression_gate\"):\n                    self.weights['regression_gate_task%i' % task_id] = MLP(self.params['hidden_size'], 1, [],\n                                                                           self.placeholders[\n                                                                               'out_layer_dropout_keep_prob'])\n                with tf.variable_scope(\"regression\"):\n                    self.weights['regression_transform_task%i' % task_id] = MLP(self.params['hidden_size'], 1, [],\n                                                                                self.placeholders[\n                                                                                    'out_layer_dropout_keep_prob'])\n                normalized_z_sampled = tf.nn.l2_normalize(self.ops['z_sampled'], 2)\n                self.ops['qed_computed_values'] = computed_values = self.gated_regression(normalized_z_sampled,\n                                                                                          self.weights[\n                                                                                              'regression_gate_task%i' % task_id],\n                                                                                          self.weights[\n                                                                                              'regression_transform_task%i' % task_id],\n                                                                                          self.params[\"hidden_size\"],\n                                                                                          self.weights['qed_weights'],\n                                                                                          self.weights['qed_biases'],\n                                                                                          self.placeholders[\n                                                                                              'num_vertices'],\n                                                                                          self.placeholders[\n                                                                                              'node_mask'])\n                diff = computed_values - self.placeholders['target_values'][internal_id, :]  # [b]\n                task_target_mask = self.placeholders['target_mask'][internal_id, :]\n                task_target_num = tf.reduce_sum(task_target_mask) + SMALL_NUMBER\n                diff = diff * task_target_mask  # Mask out unused values [b]\n                self.ops['accuracy_task%i' % task_id] = tf.reduce_sum(tf.abs(diff)) / task_target_num\n                task_loss = tf.reduce_sum(0.5 * tf.square(diff)) / task_target_num  # number\n                # Normalise loss to account for fewer task-specific examples in batch:\n                task_loss = task_loss * (1.0 / (self.params['task_sample_ratios'].get(task_id) or 1.0))\n                self.ops['qed_loss'].append(task_loss)\n                if task_id == 0:  # Assume it is the QED score\n                    z_sampled_shape = tf.shape(self.ops['z_sampled'])\n                    flattened_z_sampled = tf.reshape(self.ops['z_sampled'], [z_sampled_shape[0], -1])\n                    self.ops['l2_loss'] = 0.01 * tf.reduce_sum(flattened_z_sampled * flattened_z_sampled, axis=1) / 2\n                    # Calculate the derivative with respect to QED + l2 loss\n                    self.ops['derivative_z_sampled'] = tf.gradients(self.ops['qed_computed_values'] -\n                                                                    self.ops['l2_loss'], self.ops['z_sampled'])\n        self.ops['total_qed_loss'] = tf.reduce_sum(self.ops['qed_loss'])  # number\n        self.ops['mean_edge_loss'] = tf.reduce_mean(self.ops[\"edge_loss\"])  # record the mean edge loss\n        self.ops['mean_node_symbol_loss'] = tf.reduce_mean(self.ops[\"node_symbol_loss\"])\n        self.ops['mean_kl_loss'] = tf.reduce_mean(kl_trade_off_lambda * self.ops['kl_loss'])\n        self.ops['mean_total_qed_loss'] = self.params[\"qed_trade_off_lambda\"] * self.ops['total_qed_loss']\n        return tf.reduce_mean(self.ops[\"edge_loss\"] + self.ops['node_symbol_loss'] + \\\n                              kl_trade_off_lambda * self.ops['kl_loss']) \\\n               + self.params[\"qed_trade_off_lambda\"] * self.ops['total_qed_loss']\n\n    def gated_regression(self, last_h, regression_gate, regression_transform, hidden_size, projection_weight,\n                         projection_bias, v, mask):\n        # last_h: [b x v x h]\n        last_h = tf.reshape(last_h, [-1, hidden_size])  # [b*v, h]    \n        # linear projection on last_h\n        last_h = tf.nn.relu(tf.matmul(last_h, projection_weight) + projection_bias)  # [b*v, h]  \n        # same as last_h\n        gate_input = last_h\n        # linear projection and combine                                       \n        gated_outputs = tf.nn.sigmoid(regression_gate(gate_input)) * tf.nn.tanh(\n            regression_transform(last_h))  # [b*v, 1]\n        gated_outputs = tf.reshape(gated_outputs, [-1, v])  # [b, v]\n        masked_gated_outputs = gated_outputs * mask  # [b x v]\n        output = tf.reduce_sum(masked_gated_outputs, axis=1)  # [b]\n        output = tf.sigmoid(output)\n        return output\n\n    def calculate_incremental_results(self, raw_data, bucket_sizes, file_name):\n        incremental_results = []\n        # copy the raw_data if more than 1 BFS path is added\n        new_raw_data = []\n        for idx, d in enumerate(raw_data):\n            # Use canonical order or random order here. canonical order starts from index 0. random order starts from random nodes\n            if not self.params[\"path_random_order\"]:\n                # Use several different starting index if using multi BFS path\n                if self.params[\"multi_bfs_path\"]:\n                    list_of_starting_idx = list(range(self.params[\"bfs_path_count\"]))\n                else:\n                    list_of_starting_idx = [0]  # the index 0\n            else:\n                # get the node length for this molecule\n                node_length = len(d[\"node_features\"])\n                if self.params[\"multi_bfs_path\"]:\n                    list_of_starting_idx = np.random.choice(node_length, self.params[\"bfs_path_count\"],\n                                                            replace=True)  # randomly choose several\n                else:\n                    list_of_starting_idx = [random.choice(list(range(node_length)))]  # randomly choose one\n            for list_idx, starting_idx in enumerate(list_of_starting_idx):\n                # choose a bucket\n                chosen_bucket_idx = np.argmax(bucket_sizes > max([v for e in d['graph']\n                                                                  for v in [e[0], e[2]]]))\n                chosen_bucket_size = bucket_sizes[chosen_bucket_idx]\n\n                # Calculate incremental results without master node\n                nodes_no_master, edges_no_master = to_graph(d['smiles'], self.params[\"dataset\"])\n                incremental_adj_mat, distance_to_others, node_sequence, edge_type_masks, edge_type_labels, local_stop, edge_masks, edge_labels, overlapped_edge_features = \\\n                    construct_incremental_graph(dataset, edges_no_master, chosen_bucket_size,\n                                                len(nodes_no_master), nodes_no_master, self.params,\n                                                initial_idx=starting_idx)\n                if self.params[\"sample_transition\"] and list_idx > 0:\n                    incremental_results[-1] = [x + y for x, y in\n                                               zip(incremental_results[-1], [incremental_adj_mat, distance_to_others,\n                                                                             node_sequence, edge_type_masks,\n                                                                             edge_type_labels, local_stop, edge_masks,\n                                                                             edge_labels, overlapped_edge_features])]\n                else:\n                    incremental_results.append([incremental_adj_mat, distance_to_others, node_sequence, edge_type_masks,\n                                                edge_type_labels, local_stop, edge_masks, edge_labels,\n                                                overlapped_edge_features])\n                    # copy the raw_data here \n                    new_raw_data.append(d)\n                if idx % 50 == 0:\n                    print('finish calculating %d incremental matrices' % idx, end=\"\\r\")\n        return incremental_results, new_raw_data\n\n    # ----- Data preprocessing and chunking into minibatches:\n    def process_raw_graphs(self, raw_data, is_training_data, file_name, bucket_sizes=None):\n        if bucket_sizes is None:\n            bucket_sizes = dataset_info(self.params[\"dataset\"])[\"bucket_sizes\"]\n        incremental_results, raw_data = self.calculate_incremental_results(raw_data, bucket_sizes, file_name)\n        bucketed = defaultdict(list)\n        x_dim = len(raw_data[0][\"node_features\"][0])\n\n        for d, (incremental_adj_mat, distance_to_others, node_sequence, edge_type_masks, edge_type_labels, local_stop,\n                edge_masks, edge_labels, overlapped_edge_features) \\\n                in zip(raw_data, incremental_results):\n            # choose a bucket\n            chosen_bucket_idx = np.argmax(bucket_sizes > max([v for e in d['graph']\n                                                              for v in [e[0], e[2]]]))\n            chosen_bucket_size = bucket_sizes[chosen_bucket_idx]\n            # total number of nodes in this data point\n            n_active_nodes = len(d[\"node_features\"])\n            bucketed[chosen_bucket_idx].append({\n                'adj_mat': graph_to_adj_mat(d['graph'], chosen_bucket_size, self.num_edge_types,\n                                            self.params['tie_fwd_bkwd']),\n                'incre_adj_mat': incremental_adj_mat,\n                'distance_to_others': distance_to_others,\n                'overlapped_edge_features': overlapped_edge_features,\n                'node_sequence': node_sequence,\n                'edge_type_masks': edge_type_masks,\n                'edge_type_labels': edge_type_labels,\n                'edge_masks': edge_masks,\n                'edge_labels': edge_labels,\n                'local_stop': local_stop,\n                'number_iteration': len(local_stop),\n                'init': d[\"node_features\"] + [[0 for _ in range(x_dim)] for __ in\n                                              range(chosen_bucket_size - n_active_nodes)],\n                'labels': [d[\"targets\"][task_id][0] for task_id in self.params['task_ids']],\n                'mask': [1. for _ in range(n_active_nodes)] + [0. for _ in range(chosen_bucket_size - n_active_nodes)]\n            })\n\n        if is_training_data:\n            for (bucket_idx, bucket) in bucketed.items():\n                np.random.shuffle(bucket)\n                for task_id in self.params['task_ids']:\n                    task_sample_ratio = self.params['task_sample_ratios'].get(str(task_id))\n                    if task_sample_ratio is not None:\n                        ex_to_sample = int(len(bucket) * task_sample_ratio)\n                        for ex_id in range(ex_to_sample, len(bucket)):\n                            bucket[ex_id]['labels'][task_id] = None\n\n        bucket_at_step = [[bucket_idx for _ in range(len(bucket_data) // self.params['batch_size'])]\n                          for bucket_idx, bucket_data in bucketed.items()]\n        bucket_at_step = [x for y in bucket_at_step for x in y]\n\n        return (bucketed, bucket_sizes, bucket_at_step)\n\n    def pad_annotations(self, annotations):\n        return np.pad(annotations,\n                      pad_width=[[0, 0], [0, 0], [0, self.params['hidden_size'] - self.params[\"num_symbols\"]]],\n                      mode='constant')\n\n    def make_batch(self, elements, maximum_vertice_num):\n        # get maximum number of iterations in this batch. used to control while_loop\n        max_iteration_num = -1\n        for d in elements:\n            max_iteration_num = max(d['number_iteration'], max_iteration_num)\n        batch_data = {'adj_mat': [], 'init': [], 'labels': [], 'edge_type_masks': [], 'edge_type_labels': [],\n                      'edge_masks': [],\n                      'edge_labels': [], 'node_mask': [], 'task_masks': [], 'node_sequence': [],\n                      'iteration_mask': [], 'local_stop': [], 'incre_adj_mat': [], 'distance_to_others': [],\n                      'max_iteration_num': max_iteration_num, 'overlapped_edge_features': []}\n        for d in elements:\n            # sparse to dense for saving memory           \n            incre_adj_mat = incre_adj_mat_to_dense(d['incre_adj_mat'], self.num_edge_types, maximum_vertice_num)\n            distance_to_others = distance_to_others_dense(d['distance_to_others'], maximum_vertice_num)\n            overlapped_edge_features = overlapped_edge_features_to_dense(d['overlapped_edge_features'],\n                                                                         maximum_vertice_num)\n            node_sequence = node_sequence_to_dense(d['node_sequence'], maximum_vertice_num)\n            edge_type_masks = edge_type_masks_to_dense(d['edge_type_masks'], maximum_vertice_num, self.num_edge_types)\n            edge_type_labels = edge_type_labels_to_dense(d['edge_type_labels'], maximum_vertice_num,\n                                                         self.num_edge_types)\n            edge_masks = edge_masks_to_dense(d['edge_masks'], maximum_vertice_num)\n            edge_labels = edge_labels_to_dense(d['edge_labels'], maximum_vertice_num)\n\n            batch_data['adj_mat'].append(d['adj_mat'])\n            batch_data['init'].append(d['init'])\n            batch_data['node_mask'].append(d['mask'])\n\n            batch_data['incre_adj_mat'].append(incre_adj_mat +\n                                               [np.zeros(\n                                                   (self.num_edge_types, maximum_vertice_num, maximum_vertice_num))\n                                                   for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['distance_to_others'].append(distance_to_others +\n                                                    [np.zeros((maximum_vertice_num))\n                                                     for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['overlapped_edge_features'].append(overlapped_edge_features +\n                                                          [np.zeros((maximum_vertice_num))\n                                                           for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['node_sequence'].append(node_sequence +\n                                               [np.zeros((maximum_vertice_num))\n                                                for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['edge_type_masks'].append(edge_type_masks +\n                                                 [np.zeros((self.num_edge_types, maximum_vertice_num))\n                                                  for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['edge_masks'].append(edge_masks +\n                                            [np.zeros((maximum_vertice_num))\n                                             for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['edge_type_labels'].append(edge_type_labels +\n                                                  [np.zeros((self.num_edge_types, maximum_vertice_num))\n                                                   for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['edge_labels'].append(edge_labels +\n                                             [np.zeros((maximum_vertice_num))\n                                              for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['iteration_mask'].append([1 for _ in range(d['number_iteration'])] +\n                                                [0 for _ in range(max_iteration_num - d['number_iteration'])])\n            batch_data['local_stop'].append([int(s) for s in d[\"local_stop\"]] +\n                                            [0 for _ in range(max_iteration_num - d['number_iteration'])])\n\n            target_task_values = []\n            target_task_mask = []\n            for target_val in d['labels']:\n                if target_val is None:  # This is one of the examples we didn't sample...\n                    target_task_values.append(0.)\n                    target_task_mask.append(0.)\n                else:\n                    target_task_values.append(target_val)\n                    target_task_mask.append(1.)\n            batch_data['labels'].append(target_task_values)\n            batch_data['task_masks'].append(target_task_mask)\n\n        return batch_data\n\n    def get_dynamic_feed_dict(self, elements, latent_node_symbol, incre_adj_mat, num_vertices,\n                              distance_to_others, overlapped_edge_dense, node_sequence, edge_type_masks, edge_masks,\n                              random_normal_states):\n        if incre_adj_mat is None:\n            incre_adj_mat = np.zeros((1, 1, self.num_edge_types, 1, 1))\n            distance_to_others = np.zeros((1, 1, 1))\n            overlapped_edge_dense = np.zeros((1, 1, 1))\n            node_sequence = np.zeros((1, 1, 1))\n            edge_type_masks = np.zeros((1, 1, self.num_edge_types, 1))\n            edge_masks = np.zeros((1, 1, 1))\n            latent_node_symbol = np.zeros((1, 1, self.params[\"num_symbols\"]))\n        return {\n            self.placeholders['z_prior']: random_normal_states,  # [1, v, h]\n            self.placeholders['incre_adj_mat']: incre_adj_mat,  # [1, 1, e, v, v]\n            self.placeholders['num_vertices']: num_vertices,  # v\n\n            self.placeholders['initial_node_representation']: \\\n                self.pad_annotations([elements['init']]),\n            self.placeholders['node_symbols']: [elements['init']],\n            self.placeholders['latent_node_symbols']: self.pad_annotations(latent_node_symbol),\n            self.placeholders['adjacency_matrix']: [elements['adj_mat']],\n            self.placeholders['node_mask']: [elements['mask']],\n\n            self.placeholders['graph_state_keep_prob']: 1,\n            self.placeholders['edge_weight_dropout_keep_prob']: 1,\n            self.placeholders['iteration_mask']: [[1]],\n            self.placeholders['is_generative']: True,\n            self.placeholders['out_layer_dropout_keep_prob']: 1.0,\n            self.placeholders['distance_to_others']: distance_to_others,  # [1, 1,v]\n            self.placeholders['overlapped_edge_features']: overlapped_edge_dense,\n            self.placeholders['max_iteration_num']: 1,\n            self.placeholders['node_sequence']: node_sequence,  # [1, 1, v]\n            self.placeholders['edge_type_masks']: edge_type_masks,  # [1, 1, e, v]\n            self.placeholders['edge_masks']: edge_masks,  # [1, 1, v]\n        }\n\n    def get_node_symbol(self, batch_feed_dict):\n        fetch_list = [self.ops['node_symbol_prob']]\n        result = self.sess.run(fetch_list, feed_dict=batch_feed_dict)\n        return result[0]\n\n    def node_symbol_one_hot(self, sampled_node_symbol, real_n_vertices, max_n_vertices):\n        one_hot_representations = []\n        for idx in range(max_n_vertices):\n            representation = [0] * self.params[\"num_symbols\"]\n            if idx < real_n_vertices:\n                atom_type = sampled_node_symbol[idx]\n                representation[atom_type] = 1\n            one_hot_representations.append(representation)\n        return one_hot_representations\n\n    def search_and_generate_molecule(self, initial_idx, valences,\n                                     sampled_node_symbol, real_n_vertices, random_normal_states,\n                                     elements, max_n_vertices):\n        # New molecule\n        new_mol = Chem.MolFromSmiles('')\n        new_mol = Chem.rdchem.RWMol(new_mol)\n        # Add atoms\n        add_atoms(new_mol, sampled_node_symbol, self.params[\"dataset\"])\n        # Breadth first search over the molecule\n        queue = deque([initial_idx])\n        # color 0: have not found 1: in the queue 2: searched already\n        color = [0] * max_n_vertices\n        color[initial_idx] = 1\n        # Empty adj list at the beginning\n        incre_adj_list = defaultdict(list)\n        # record the log probabilities at each step\n        total_log_prob = 0\n        while len(queue) > 0:\n            node_in_focus = queue.popleft()\n            # iterate until the stop node is selected \n            while True:\n                # Prepare data for one iteration based on the graph state\n                edge_type_mask_sparse, edge_mask_sparse = generate_mask(valences, incre_adj_list, color,\n                                                                        real_n_vertices, node_in_focus,\n                                                                        self.params[\"check_overlap_edge\"], new_mol)\n                edge_type_mask = edge_type_masks_to_dense([edge_type_mask_sparse], max_n_vertices,\n                                                          self.num_edge_types)  # [1, e, v]\n                edge_mask = edge_masks_to_dense([edge_mask_sparse], max_n_vertices)  # [1, v]\n                node_sequence = node_sequence_to_dense([node_in_focus], max_n_vertices)  # [1, v]\n                distance_to_others_sparse = bfs_distance(node_in_focus, incre_adj_list)\n                distance_to_others = distance_to_others_dense([distance_to_others_sparse], max_n_vertices)  # [1, v]\n                overlapped_edge_sparse = get_overlapped_edge_feature(edge_mask_sparse, color, new_mol)\n\n                overlapped_edge_dense = overlapped_edge_features_to_dense([overlapped_edge_sparse],\n                                                                          max_n_vertices)  # [1, v]\n                incre_adj_mat = incre_adj_mat_to_dense([incre_adj_list],\n                                                       self.num_edge_types, max_n_vertices)  # [1, e, v, v]\n                sampled_node_symbol_one_hot = self.node_symbol_one_hot(sampled_node_symbol, real_n_vertices,\n                                                                       max_n_vertices)\n\n                # get feed_dict\n                feed_dict = self.get_dynamic_feed_dict(elements, [sampled_node_symbol_one_hot],\n                                                       [incre_adj_mat], max_n_vertices, [distance_to_others],\n                                                       [overlapped_edge_dense],\n                                                       [node_sequence], [edge_type_mask], [edge_mask],\n                                                       random_normal_states)\n\n                # fetch nn predictions\n                fetch_list = [self.ops['edge_predictions'], self.ops['edge_type_predictions']]\n                edge_probs, edge_type_probs = self.sess.run(fetch_list, feed_dict=feed_dict)\n                # select an edge\n                if not self.params[\"use_argmax_generation\"]:\n                    neighbor = np.random.choice(np.arange(max_n_vertices + 1), p=edge_probs[0])\n                else:\n                    neighbor = np.argmax(edge_probs[0])\n                # update log prob\n                total_log_prob += np.log(edge_probs[0][neighbor] + SMALL_NUMBER)\n                # stop it if stop node is picked\n                if neighbor == max_n_vertices:\n                    break\n                    # or choose an edge type\n                if not self.params[\"use_argmax_generation\"]:\n                    bond = np.random.choice(np.arange(self.num_edge_types), p=edge_type_probs[0, :, neighbor])\n                else:\n                    bond = np.argmax(edge_type_probs[0, :, neighbor])\n                # update log prob\n                total_log_prob += np.log(edge_type_probs[0, :, neighbor][bond] + SMALL_NUMBER)\n                # update valences\n                valences[node_in_focus] -= (bond + 1)\n                valences[neighbor] -= (bond + 1)\n                # add the bond\n                new_mol.AddBond(int(node_in_focus), int(neighbor), number_to_bond[bond])\n                # add the edge to increment adj list\n                incre_adj_list[node_in_focus].append((neighbor, bond))\n                incre_adj_list[neighbor].append((node_in_focus, bond))\n                # Explore neighbor nodes\n                if color[neighbor] == 0:\n                    queue.append(neighbor)\n                    color[neighbor] = 1\n            color[node_in_focus] = 2  # explored\n        # Remove unconnected node     \n        remove_extra_nodes(new_mol)\n        new_mol = Chem.MolFromSmiles(Chem.MolToSmiles(new_mol))\n        return new_mol, total_log_prob\n\n    def gradient_ascent(self, random_normal_states, derivative_z_sampled):\n        return random_normal_states + self.params['prior_learning_rate'] * derivative_z_sampled\n\n    # optimization in latent space. generate one molecule for each optimization step\n    def optimization_over_prior(self, random_normal_states, num_vertices, generated_all_similes, elements, count):\n        # record how many optimization steps are taken\n        step = 0\n        # generate a new molecule\n        self.generate_graph_with_state(random_normal_states, num_vertices, generated_all_similes, elements, step, count)\n        fetch_list = [self.ops['derivative_z_sampled'], self.ops['qed_computed_values'], self.ops['l2_loss']]\n        for _ in range(self.params['optimization_step']):\n            # get current qed and derivative\n            batch_feed_dict = self.get_dynamic_feed_dict(elements, None, None, num_vertices, None,\n                                                         None, None, None, None,\n                                                         random_normal_states)\n            derivative_z_sampled, qed_computed_values, l2_loss = self.sess.run(fetch_list, feed_dict=batch_feed_dict)\n            # update the states\n            random_normal_states = self.gradient_ascent(random_normal_states,\n                                                        derivative_z_sampled[0])\n            # generate a new molecule\n            step += 1\n            self.generate_graph_with_state(random_normal_states, num_vertices,\n                                           generated_all_similes, elements, step, count)\n        return random_normal_states\n\n    def generate_graph_with_state(self, random_normal_states, num_vertices,\n                                  generated_all_similes, elements, step, count):\n        # Get back node symbol predictions\n        # Prepare dict\n        node_symbol_batch_feed_dict = self.get_dynamic_feed_dict(elements, None, None,\n                                                                 num_vertices, None, None, None, None, None,\n                                                                 random_normal_states)\n        # Get predicted node probs\n        predicted_node_symbol_prob = self.get_node_symbol(node_symbol_batch_feed_dict)\n        # Node numbers for each graph\n        real_length = get_graph_length([elements['mask']])[0]  # [valid_node_number] \n        # Sample node symbols\n        sampled_node_symbol = sample_node_symbol(predicted_node_symbol_prob, [real_length], self.params[\"dataset\"])[\n            0]  # [v]        \n        # Maximum valences for each node\n        valences = get_initial_valence(sampled_node_symbol, self.params[\"dataset\"])  # [v]\n        # randomly pick the starting point or use zero \n        if not self.params[\"path_random_order\"]:\n            # Try different starting points\n            if self.params[\"try_different_starting\"]:\n                # starting_point=list(range(self.params[\"num_different_starting\"]))\n                starting_point = random.sample(range(real_length),\n                                               min(self.params[\"num_different_starting\"], real_length))\n            else:\n                starting_point = [0]\n        else:\n            if self.params[\"try_different_starting\"]:\n                starting_point = random.sample(range(real_length),\n                                               min(self.params[\"num_different_starting\"], real_length))\n            else:\n                starting_point = [random.choice(list(range(real_length)))]  # randomly choose one\n        # record all molecules from different starting points\n        all_mol = []\n        for idx in starting_point:\n            # generate a new molecule\n            new_mol, total_log_prob = self.search_and_generate_molecule(idx, np.copy(valences),\n                                                                        sampled_node_symbol, real_length,\n                                                                        random_normal_states, elements, num_vertices)\n            # record the molecule with largest number of shapes\n            if dataset == 'qm9' and new_mol is not None:\n                all_mol.append((np.sum(shape_count(self.params[\"dataset\"], True,\n                                                   [Chem.MolToSmiles(new_mol)])[1]), total_log_prob, new_mol))\n            # record the molecule with largest number of pentagon and hexagonal for zinc and cep\n            elif dataset == 'zinc' and new_mol is not None:\n                counts = shape_count(self.params[\"dataset\"], True, [Chem.MolToSmiles(new_mol)])\n                all_mol.append((0.5 * counts[1][2] + counts[1][3], total_log_prob, new_mol))\n            elif dataset == 'cep' and new_mol is not None:\n                all_mol.append((np.sum(shape_count(self.params[\"dataset\"], True,\n                                                   [Chem.MolToSmiles(new_mol)])[1][2:]), total_log_prob, new_mol))\n        # select one out\n        best_mol = select_best(all_mol)\n        # nothing generated\n        if best_mol is None:\n            return\n        # visualize it \n        make_dir('visualization_%s' % dataset)\n        visualize_mol('visualization_%s/%d_%d.png' % (dataset, count, step), best_mol)\n        # record the best molecule\n        generated_all_similes.append(Chem.MolToSmiles(best_mol))\n        dump('generated_smiles_%s' % (dataset), generated_all_similes)\n        print(\"Real QED value\")\n        print(QED.qed(best_mol))\n        if len(generated_all_similes) >= self.params['number_of_generation']:\n            print(\"generation done\")\n            exit(0)\n\n    def compensate_node_length(self, elements, bucket_size):\n        maximum_length = bucket_size + self.params[\"compensate_num\"]\n        real_length = get_graph_length([elements['mask']])[0] + self.params[\"compensate_num\"]\n        elements['mask'] = [1] * real_length + [0] * (maximum_length - real_length)\n        elements['init'] = np.zeros((maximum_length, self.params[\"num_symbols\"]))\n        elements['adj_mat'] = np.zeros((self.num_edge_types, maximum_length, maximum_length))\n        return maximum_length\n\n    def generate_new_graphs(self, data):\n        # bucketed: data organized by bucket\n        (bucketed, bucket_sizes, bucket_at_step) = data\n        bucket_counters = defaultdict(int)\n        # all generated similes\n        generated_all_similes = []\n        # counter\n        count = 0\n        # shuffle the lengths\n        np.random.shuffle(bucket_at_step)\n        for step in range(len(bucket_at_step)):\n            bucket = bucket_at_step[step]  # bucket number\n            # data index\n            start_idx = bucket_counters[bucket] * self.params['batch_size']\n            end_idx = (bucket_counters[bucket] + 1) * self.params['batch_size']\n            # batch data\n            elements_batch = bucketed[bucket][start_idx:end_idx]\n            for elements in elements_batch:\n                # compensate for the length during generation \n                # (this is a result that BFS may not make use of all candidate nodes during generation)\n                maximum_length = self.compensate_node_length(elements, bucket_sizes[bucket])\n                # initial state\n                random_normal_states = generate_std_normal(1, maximum_length, \\\n                                                           self.params['hidden_size'])  # [1, v, h]                \n                random_normal_states = self.optimization_over_prior(random_normal_states,\n                                                                    maximum_length, generated_all_similes, elements,\n                                                                    count)\n                count += 1\n            bucket_counters[bucket] += 1\n\n    def make_minibatch_iterator(self, data, is_training: bool):\n        (bucketed, bucket_sizes, bucket_at_step) = data\n        if is_training:\n            np.random.shuffle(bucket_at_step)\n            for _, bucketed_data in bucketed.items():\n                np.random.shuffle(bucketed_data)\n        bucket_counters = defaultdict(int)\n        dropout_keep_prob = self.params['graph_state_dropout_keep_prob'] if is_training else 1.\n        edge_dropout_keep_prob = self.params['edge_weight_dropout_keep_prob'] if is_training else 1.\n        for step in range(len(bucket_at_step)):\n            bucket = bucket_at_step[step]\n            start_idx = bucket_counters[bucket] * self.params['batch_size']\n            end_idx = (bucket_counters[bucket] + 1) * self.params['batch_size']\n            elements = bucketed[bucket][start_idx:end_idx]\n            batch_data = self.make_batch(elements, bucket_sizes[bucket])\n\n            num_graphs = len(batch_data['init'])\n            initial_representations = batch_data['init']\n            initial_representations = self.pad_annotations(initial_representations)\n            batch_feed_dict = {\n                self.placeholders['initial_node_representation']: initial_representations,\n                self.placeholders['node_symbols']: batch_data['init'],\n                self.placeholders['latent_node_symbols']: initial_representations,\n                self.placeholders['target_values']: np.transpose(batch_data['labels'], axes=[1, 0]),\n                self.placeholders['target_mask']: np.transpose(batch_data['task_masks'], axes=[1, 0]),\n                self.placeholders['num_graphs']: num_graphs,\n                self.placeholders['num_vertices']: bucket_sizes[bucket],\n                self.placeholders['adjacency_matrix']: batch_data['adj_mat'],\n                self.placeholders['node_mask']: batch_data['node_mask'],\n                self.placeholders['graph_state_keep_prob']: dropout_keep_prob,\n                self.placeholders['edge_weight_dropout_keep_prob']: edge_dropout_keep_prob,\n                self.placeholders['iteration_mask']: batch_data['iteration_mask'],\n                self.placeholders['incre_adj_mat']: batch_data['incre_adj_mat'],\n                self.placeholders['distance_to_others']: batch_data['distance_to_others'],\n                self.placeholders['node_sequence']: batch_data['node_sequence'],\n                self.placeholders['edge_type_masks']: batch_data['edge_type_masks'],\n                self.placeholders['edge_type_labels']: batch_data['edge_type_labels'],\n                self.placeholders['edge_masks']: batch_data['edge_masks'],\n                self.placeholders['edge_labels']: batch_data['edge_labels'],\n                self.placeholders['local_stop']: batch_data['local_stop'],\n                self.placeholders['max_iteration_num']: batch_data['max_iteration_num'],\n                self.placeholders['kl_trade_off_lambda']: self.params['kl_trade_off_lambda'],\n                self.placeholders['overlapped_edge_features']: batch_data['overlapped_edge_features']\n            }\n            bucket_counters[bucket] += 1\n            yield batch_feed_dict\n\n\n# if __name__ == \"__main__\":\n#     args = docopt(__doc__)\n#     dataset = args.get('--dataset')\n#     try:\n#         model = DenseGGNNChemModel(args)\n#         evaluation = False\n#         if evaluation:\n#             model.example_evaluation()\n#         else:\n#             model.train()\n#     except:\n#         typ, value, tb = sys.exc_info()\n#         traceback.print_exc()\n#         pdb.post_mortem(tb)\n\n#####################  data_augmentation.py\n\nfrom utils import *\nfrom copy import deepcopy\n\n\n# Generate the mask based on the valences and adjacent matrix so far\n# For a (node_in_focus, neighbor, edge_type) to be valid, neighbor's color < 2 and \n# there is no edge so far between node_in_focus and neighbor and it satisfy the valence constraint\n# and node_in_focus != neighbor \ndef generate_mask(valences, adj_mat, color, real_n_vertices, node_in_focus, check_overlap_edge, new_mol):\n    edge_type_mask = []\n    edge_mask = []\n    for neighbor in range(real_n_vertices):\n        if neighbor != node_in_focus and color[neighbor] < 2 and \\\n                not check_adjacent_sparse(adj_mat, node_in_focus, neighbor)[0]:\n            min_valence = min(valences[node_in_focus], valences[neighbor], 3)\n            # Check whether two cycles have more than two overlap edges here\n            # the neighbor color = 1 and there are left valences and \n            # adding that edge will not cause overlap edges.\n            if check_overlap_edge and min_valence > 0 and color[neighbor] == 1:\n                # attempt to add the edge\n                new_mol.AddBond(int(node_in_focus), int(neighbor), number_to_bond[0])\n                # Check whether there are two cycles having more than two overlap edges\n                ssr = Chem.GetSymmSSSR(new_mol)\n                overlap_flag = False\n                for idx1 in range(len(ssr)):\n                    for idx2 in range(idx1 + 1, len(ssr)):\n                        if len(set(ssr[idx1]) & set(ssr[idx2])) > 2:\n                            overlap_flag = True\n                # remove that edge\n                new_mol.RemoveBond(int(node_in_focus), int(neighbor))\n                if overlap_flag:\n                    continue\n            for v in range(min_valence):\n                assert v < 3\n                edge_type_mask.append((node_in_focus, neighbor, v))\n            # there might be an edge between node in focus and neighbor\n            if min_valence > 0:\n                edge_mask.append((node_in_focus, neighbor))\n    return edge_type_mask, edge_mask\n\n\n# when a new edge is about to be added, we generate labels based on ground truth\n# if an edge is in ground truth and has not been added to incremental adj yet, we label it as positive\ndef generate_label(ground_truth_graph, incremental_adj, node_in_focus, real_neighbor, real_n_vertices, params):\n    edge_type_label = []\n    edge_label = []\n    for neighbor in range(real_n_vertices):\n        adjacent, edge_type = check_adjacent_sparse(ground_truth_graph, node_in_focus, neighbor)\n        incre_adjacent, incre_edge_type = check_adjacent_sparse(incremental_adj, node_in_focus, neighbor)\n        if not params[\"label_one_hot\"] and adjacent and not incre_adjacent:\n            assert edge_type < 3\n            edge_type_label.append((node_in_focus, neighbor, edge_type))\n            edge_label.append((node_in_focus, neighbor))\n        elif params[\"label_one_hot\"] and adjacent and not incre_adjacent and neighbor == real_neighbor:\n            edge_type_label.append((node_in_focus, neighbor, edge_type))\n            edge_label.append((node_in_focus, neighbor))\n    return edge_type_label, edge_label\n\n\n# add a incremental adj with one new edge\ndef genereate_incremental_adj(last_adj, node_in_focus, neighbor, edge_type):\n    # copy last incremental adj matrix\n    new_adj = deepcopy(last_adj)\n    # Add a new edge into it\n    new_adj[node_in_focus].append((neighbor, edge_type))\n    new_adj[neighbor].append((node_in_focus, edge_type))\n    return new_adj\n\n\ndef update_one_step(overlapped_edge_features, distance_to_others, node_sequence, node_in_focus, neighbor, edge_type,\n                    edge_type_masks, valences, incremental_adj_mat,\n                    color, real_n_vertices, graph, edge_type_labels, local_stop, edge_masks, edge_labels,\n                    local_stop_label, params,\n                    check_overlap_edge, new_mol, up_to_date_adj_mat, keep_prob):\n    # check whether to keep this transition or not\n    if params[\"sample_transition\"] and random.random() > keep_prob:\n        return\n    # record the current node in focus\n    node_sequence.append(node_in_focus)\n    # generate mask based on current situation\n    edge_type_mask, edge_mask = generate_mask(valences, up_to_date_adj_mat,\n                                              color, real_n_vertices, node_in_focus, check_overlap_edge, new_mol)\n    edge_type_masks.append(edge_type_mask)\n    edge_masks.append(edge_mask)\n    if not local_stop_label:\n        # generate the label based on ground truth graph\n        edge_type_label, edge_label = generate_label(graph, up_to_date_adj_mat, node_in_focus, neighbor,\n                                                     real_n_vertices, params)\n        edge_type_labels.append(edge_type_label)\n        edge_labels.append(edge_label)\n    else:\n        edge_type_labels.append([])\n        edge_labels.append([])\n    # update local stop \n    local_stop.append(local_stop_label)\n    # Calculate distance using bfs from the current node to all other node\n    distances = bfs_distance(node_in_focus, up_to_date_adj_mat)\n    distances = [(start, node, params[\"truncate_distance\"]) if d > params[\"truncate_distance\"] else (start, node, d) for\n                 start, node, d in distances]\n    distance_to_others.append(distances)\n    # Calculate the overlapped edge mask\n    overlapped_edge_features.append(get_overlapped_edge_feature(edge_mask, color, new_mol))\n    # update the incremental adj mat at this step\n    incremental_adj_mat.append(deepcopy(up_to_date_adj_mat))\n\n\ndef construct_incremental_graph(dataset, edges, max_n_vertices, real_n_vertices, node_symbol, params, initial_idx=0):\n    # avoid calculating this if it is just for generating new molecules for speeding up\n    if params[\"generation\"]:\n        return [], [], [], [], [], [], [], [], []\n    # avoid the initial index is larger than real_n_vertices:\n    if initial_idx >= real_n_vertices:\n        initial_idx = 0\n    # Maximum valences for each node\n    valences = get_initial_valence([np.argmax(symbol) for symbol in node_symbol], dataset)\n    # Add backward edges\n    edges_bw = [(dst, edge_type, src) for src, edge_type, dst in edges]\n    edges = edges + edges_bw\n    # Construct a graph object using the edges\n    graph = defaultdict(list)\n    for src, edge_type, dst in edges:\n        graph[src].append((dst, edge_type))\n    # Breadth first search over the molecule \n    # color 0: have not found 1: in the queue 2: searched already\n    color = [0] * max_n_vertices\n    color[initial_idx] = 1\n    queue = deque([initial_idx])\n    # create a adj matrix without any edges\n    up_to_date_adj_mat = defaultdict(list)\n    # record incremental adj mat\n    incremental_adj_mat = []\n    # record the distance to other nodes at the moment\n    distance_to_others = []\n    # soft constraint on overlapped edges\n    overlapped_edge_features = []\n    # the exploration order of the nodes\n    node_sequence = []\n    # edge type masks for nn predictions at each step\n    edge_type_masks = []\n    # edge type labels for nn predictions at each step\n    edge_type_labels = []\n    # edge masks for nn predictions at each step\n    edge_masks = []\n    # edge labels for nn predictions at each step\n    edge_labels = []\n    # local stop labels\n    local_stop = []\n    # record the incremental molecule\n    new_mol = Chem.MolFromSmiles('')\n    new_mol = Chem.rdchem.RWMol(new_mol)\n    # Add atoms\n    add_atoms(new_mol, sample_node_symbol([node_symbol], [len(node_symbol)], dataset)[0], dataset)\n    # calculate keep probability\n    sample_transition_count = real_n_vertices + len(edges) / 2\n    keep_prob = float(sample_transition_count) / (\n            (real_n_vertices + len(edges) / 2) * params[\"bfs_path_count\"])  # to form a binomial distribution\n    while len(queue) > 0:\n        node_in_focus = queue.popleft()\n        current_adj_list = graph[node_in_focus]\n        # sort (canonical order) it or shuffle (random order) it \n        if not params[\"path_random_order\"]:\n            current_adj_list = sorted(current_adj_list)\n        else:\n            random.shuffle(current_adj_list)\n        for neighbor, edge_type in current_adj_list:\n            # Add this edge if the color of neighbor node is not 2\n            if color[neighbor] < 2:\n                update_one_step(overlapped_edge_features, distance_to_others, node_sequence, node_in_focus, neighbor,\n                                edge_type,\n                                edge_type_masks, valences, incremental_adj_mat, color, real_n_vertices, graph,\n                                edge_type_labels, local_stop, edge_masks, edge_labels, False, params,\n                                params[\"check_overlap_edge\"], new_mol,\n                                up_to_date_adj_mat, keep_prob)\n                # Add the edge and obtain a new adj mat\n                up_to_date_adj_mat = genereate_incremental_adj(\n                    up_to_date_adj_mat, node_in_focus, neighbor, edge_type)\n                # suppose the edge is selected and update valences after adding the \n                valences[node_in_focus] -= (edge_type + 1)\n                valences[neighbor] -= (edge_type + 1)\n                # update the incremental mol\n                new_mol.AddBond(int(node_in_focus), int(neighbor), number_to_bond[edge_type])\n            # Explore neighbor nodes\n            if color[neighbor] == 0:\n                queue.append(neighbor)\n                color[neighbor] = 1\n        # local stop here. We move on to another node for exploration or stop completely\n        update_one_step(overlapped_edge_features, distance_to_others, node_sequence, node_in_focus, None, None,\n                        edge_type_masks,\n                        valences, incremental_adj_mat, color, real_n_vertices, graph,\n                        edge_type_labels, local_stop, edge_masks, edge_labels, True, params,\n                        params[\"check_overlap_edge\"], new_mol, up_to_date_adj_mat, keep_prob)\n        color[node_in_focus] = 2\n\n    return incremental_adj_mat, distance_to_others, node_sequence, edge_type_masks, edge_type_labels, local_stop, edge_masks, edge_labels, overlapped_edge_features\n\n\n##################  evaluate.py\n\n# !/usr/bin/env/python\n\"\"\"\nUsage:\n    evaluate.py --dataset zinc|qm9|cep\n\nOptions:\n    -h --help                Show this screen.\n    --dataset NAME           Dataset name: zinc, qm9, cep\n\"\"\"\n\n# if __name__ == '__main__':\n#     args = docopt(__doc__)\n#     dataset=args.get('--dataset')\n#     logpscorer, logp_score_per_molecule=utils.check_logp(dataset)\n#     qedscorer, qed_score_per_molecule=utils.check_qed(dataset)\n#     novelty=utils.novelty_metric(dataset)\n#     total, nonplanar=utils.check_planar(dataset)\n#     total, atom_counter, atom_per_molecule =utils.count_atoms(dataset)\n#     total, edge_type_counter, edge_type_per_molecule=utils.count_edge_type(dataset)\n#     total, shape_count, shape_count_per_molecule=utils.shape_count(dataset)\n#     total, tree_count=utils.check_cyclic(dataset)    \n#     sascorer, sa_score_per_molecule=utils.check_sascorer(dataset)\n#     total, validity=utils.check_validity(dataset)\n# \n#     print(\"------------------------------------------\")\n#     print(\"Metrics\")\n#     print(\"------------------------------------------\")\n#     print(\"total molecule\")\n#     print(total)\n#     print(\"------------------------------------------\")\n#     print(\"percentage of nonplanar:\")\n#     print(nonplanar/total)\n#     print(\"------------------------------------------\")\n#     print(\"avg atom:\")\n#     for atom_type, c in atom_counter.items():\n#         print(dataset_info(dataset)['atom_types'][atom_type])\n#         print(c/total)\n#     print(\"standard deviation\")\n#     print(np.std(atom_per_molecule, axis=0))\n#     print(\"------------------------------------------\")\n#     print(\"avg edge_type:\")\n#     for edge_type, c in edge_type_counter.items():\n#         print(edge_type+1)\n#         print(c/total)\n#     print(\"standard deviation\")\n#     print(np.std(edge_type_per_molecule, axis=0))\n#     print(\"------------------------------------------\")\n#     print(\"avg shape:\")\n#     for shape, c in zip(utils.geometry_numbers, shape_count):\n#         print(shape)\n#         print(c/total)\n#     print(\"standard deviation\")\n#     print(np.std(shape_count_per_molecule, axis=0))\n#     print(\"------------------------------------------\")\n#     print(\"percentage of tree:\")\n#     print(tree_count/total)\n#     print(\"------------------------------------------\")\n#     print(\"percentage of validity:\")\n#     print(validity/total)\n#     print(\"------------------------------------------\")\n#     print(\"avg sa_score:\")\n#     print(sascorer)\n#     print(\"standard deviation\")\n#     print(np.std(sa_score_per_molecule))\n#     print(\"------------------------------------------\")\n#     print(\"avg logp_score:\")\n#     print(logpscorer)\n#     print(\"standard deviation\")\n#     print(np.std(logp_score_per_molecule))\n#     print(\"------------------------------------------\")\n#     print(\"percentage of novelty:\")\n#     print(novelty)\n#     print(\"------------------------------------------\")\n#     print(\"avg qed_score:\")\n#     print(qedscorer)\n#     print(\"standard deviation\")\n#     print(np.std(qed_score_per_molecule))\n#     print(\"------------------------------------------\")\n#     print(\"uniqueness\")\n#     print(utils.check_uniqueness(dataset))\n#     print(\"------------------------------------------\")\n#     print(\"percentage of SSSR\")\n#     print(utils.sssr_metric(dataset))\n\n\n################  CGNN_core.py\n\n\n# !/usr/bin/env/python\n\nfrom typing import List, Any\nimport time\nimport json\nimport random\nimport utils\nfrom utils import MLP, dataset_info, ThreadedIterator, SMALL_NUMBER, LARGE_NUMBER, graph_to_adj_mat\n\n\nclass ChemModel(object):\n    @classmethod\n    def default_params(cls):\n        return {\n\n        }\n\n    def __init__(self, args):\n        self.args = args\n\n        # Collect argument things:\n        data_dir = ''\n        if '--data_dir' in args and args['--data_dir'] is not None:\n            data_dir = args['--data_dir']\n        self.data_dir = data_dir\n\n        # Collect parameters:\n        params = self.default_params()\n        config_file = args.get('--config-file')\n        if config_file is not None:\n            with open(config_file, 'r') as f:\n                params.update(json.load(f))\n        config = args.get('--config')\n        if config is not None:\n            params.update(json.loads(config))\n        self.params = params\n\n        # Get which dataset in use\n        self.params['dataset'] = dataset = args.get('--dataset')\n        # Number of atom types of this dataset\n        self.params['num_symbols'] = len(dataset_info(dataset)[\"atom_types\"])\n\n        self.run_id = \"_\".join([time.strftime(\"%Y-%m-%d-%H-%M-%S\"), str(os.getpid())])\n        log_dir = args.get('--log_dir') or '.'\n        self.log_file = os.path.join(log_dir, \"%s_log_%s.json\" % (self.run_id, dataset))\n        self.best_model_file = os.path.join(log_dir, \"%s_model.pickle\" % self.run_id)\n\n        with open(os.path.join(log_dir, \"%s_params_%s.json\" % (self.run_id, dataset)), \"w\") as f:\n            json.dump(params, f)\n        print(\"Run %s starting with following parameters:\\n%s\" % (self.run_id, json.dumps(self.params)))\n        random.seed(params['random_seed'])\n        np.random.seed(params['random_seed'])\n\n        # Load data:\n        self.max_num_vertices = 0\n        self.num_edge_types = 0\n        self.annotation_size = 0\n        self.train_data = self.load_data(params['train_file'], is_training_data=True)\n        self.valid_data = self.load_data(params['valid_file'], is_training_data=False)\n\n        # Build the actual model\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        self.graph = tf.Graph()\n        self.sess = tf.Session(graph=self.graph, config=config)\n        with self.graph.as_default():\n            tf.set_random_seed(params['random_seed'])\n            self.placeholders = {}\n            self.weights = {}\n            self.ops = {}\n            self.make_model()\n            self.make_train_step()\n\n            # Restore/initialize variables:\n            restore_file = args.get('--restore')\n            if restore_file is not None:\n                self.restore_model(restore_file)\n            else:\n                self.initialize_model()\n\n    def load_data(self, file_name, is_training_data: bool):\n        full_path = os.path.join(self.data_dir, file_name)\n\n        print(\"Loading data from %s\" % full_path)\n        with open(full_path, 'r') as f:\n            data = json.load(f)\n\n        restrict = self.args.get(\"--restrict_data\")\n        if restrict is not None and restrict > 0:\n            data = data[:restrict]\n\n        # Get some common data out:\n        num_fwd_edge_types = len(utils.bond_dict) - 1\n        for g in data:\n            self.max_num_vertices = max(self.max_num_vertices, max([v for e in g['graph'] for v in [e[0], e[2]]]))\n\n        self.num_edge_types = max(self.num_edge_types, num_fwd_edge_types * (1 if self.params['tie_fwd_bkwd'] else 2))\n        self.annotation_size = max(self.annotation_size, len(data[0][\"node_features\"][0]))\n\n        return self.process_raw_graphs(data, is_training_data, file_name)\n\n    @staticmethod\n    def graph_string_to_array(graph_string: str) -> List[List[int]]:\n        return [[int(v) for v in s.split(' ')]\n                for s in graph_string.split('\\n')]\n\n    def process_raw_graphs(self, raw_data, is_training_data, file_name, bucket_sizes=None):\n        raise Exception(\"Models have to implement process_raw_graphs!\")\n\n    def make_model(self):\n        self.placeholders['target_values'] = tf.placeholder(tf.float32, [len(self.params['task_ids']), None],\n                                                            name='target_values')\n        self.placeholders['target_mask'] = tf.placeholder(tf.float32, [len(self.params['task_ids']), None],\n                                                          name='target_mask')\n        self.placeholders['num_graphs'] = tf.placeholder(tf.int64, [], name='num_graphs')\n        self.placeholders['out_layer_dropout_keep_prob'] = tf.placeholder(tf.float32, [],\n                                                                          name='out_layer_dropout_keep_prob')\n        # whether this session is for generating new graphs or not\n        self.placeholders['is_generative'] = tf.placeholder(tf.bool, [], name='is_generative')\n\n        with tf.variable_scope(\"graph_model\"):\n            self.prepare_specific_graph_model()\n\n            # Initial state: embedding\n            initial_state = self.get_node_embedding_state(self.placeholders['initial_node_representation'])\n\n            # This does the actual graph work:\n            if self.params['use_graph']:\n                if self.params[\"residual_connection_on\"]:\n                    self.ops['final_node_representations'] = self.compute_final_node_representations_with_residual(\n                        initial_state,\n                        tf.transpose(self.placeholders['adjacency_matrix'], [1, 0, 2, 3]),\n                        \"_encoder\")\n                else:\n                    self.ops['final_node_representations'] = self.compute_final_node_representations_without_residual(\n                        initial_state,\n                        tf.transpose(self.placeholders['adjacency_matrix'], [1, 0, 2, 3]),\n                        self.weights['edge_weights_encoder'],\n                        self.weights['edge_biases_encoder'], self.weights['node_gru_encoder'], \"gru_scope_encoder\")\n            else:\n                self.ops['final_node_representations'] = initial_state\n\n        # Calculate p(z|x)'s mean and log variance\n        self.ops['mean'], self.ops['logvariance'] = self.compute_mean_and_logvariance()\n        # Sample from a gaussian distribution according to the mean and log variance\n        self.ops['z_sampled'] = self.sample_with_mean_and_logvariance()\n        # Construct logit matrices for both edges and edge types\n        self.construct_logit_matrices()\n\n        # Obtain losses for edges and edge types\n        self.ops['qed_loss'] = []\n        self.ops['loss'] = self.construct_loss()\n\n    def make_train_step(self):\n        trainable_vars = self.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n        if self.args.get('--freeze-graph-model'):\n            graph_vars = set(self.sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"graph_model\"))\n            filtered_vars = []\n            for var in trainable_vars:\n                if var not in graph_vars:\n                    filtered_vars.append(var)\n                else:\n                    print(\"Freezing weights of variable %s.\" % var.name)\n            trainable_vars = filtered_vars\n\n        optimizer = tf.train.AdamOptimizer(self.params['learning_rate'])\n        grads_and_vars = optimizer.compute_gradients(self.ops['loss'], var_list=trainable_vars)\n        clipped_grads = []\n        for grad, var in grads_and_vars:\n            if grad is not None:\n                clipped_grads.append((tf.clip_by_norm(grad, self.params['clamp_gradient_norm']), var))\n            else:\n                clipped_grads.append((grad, var))\n        grads_for_display = []\n        for grad, var in grads_and_vars:\n            if grad is not None:\n                grads_for_display.append((tf.clip_by_norm(grad, self.params['clamp_gradient_norm']), var))\n        self.ops['grads'] = grads_for_display\n        self.ops['train_step'] = optimizer.apply_gradients(clipped_grads)\n        # Initialize newly-introduced variables:\n        self.sess.run(tf.local_variables_initializer())\n\n    def gated_regression(self, last_h, regression_gate, regression_transform):\n        raise Exception(\"Models have to implement gated_regression!\")\n\n    def prepare_specific_graph_model(self) -> None:\n        raise Exception(\"Models have to implement prepare_specific_graph_model!\")\n\n    def compute_mean_and_logvariance(self):\n        raise Exception(\"Models have to implement compute_mean_and_logvariance!\")\n\n    def sample_with_mean_and_logvariance(self):\n        raise Exception(\"Models have to implement sample_with_mean_and_logvariance!\")\n\n    def construct_logit_matrices(self):\n        raise Exception(\"Models have to implement construct_logit_matrices!\")\n\n    def construct_loss(self):\n        raise Exception(\"Models have to implement construct_loss!\")\n\n    def make_minibatch_iterator(self, data: Any, is_training: bool):\n        raise Exception(\"Models have to implement make_minibatch_iterator!\")\n\n    \"\"\"\n    def save_intermediate_results(self, adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance):\n        with open('intermediate_results_%s' % self.params[\"dataset\"], 'wb') as out_file:\n            pickle.dump([adjacency_matrix, edge_type_prob, edge_type_label, node_symbol_prob, node_symbol, edge_prob, edge_prob_label, qed_prediction, qed_labels, mean, logvariance], out_file, pickle.HIGHEST_PROTOCOL)\n    \"\"\"\n\n    def save_probs(self, all_results):\n        with open('epoch_prob_matices_%s' % self.params[\"dataset\"], 'wb') as out_file:\n            pickle.dump([all_results], out_file, pickle.HIGHEST_PROTOCOL)\n\n    def run_epoch(self, epoch_name: str, epoch_num, data, is_training: bool):\n        loss = 0\n        start_time = time.time()\n        processed_graphs = 0\n        batch_iterator = ThreadedIterator(self.make_minibatch_iterator(data, is_training), max_queue_size=5)\n\n        for step, batch_data in enumerate(batch_iterator):\n            num_graphs = batch_data[self.placeholders['num_graphs']]\n            processed_graphs += num_graphs\n            batch_data[self.placeholders['is_generative']] = False\n            # Randomly sample from normal distribution\n            batch_data[self.placeholders['z_prior']] = utils.generate_std_normal( \\\n                self.params['batch_size'], batch_data[self.placeholders['num_vertices']], self.params['hidden_size'])\n            if is_training:\n                batch_data[self.placeholders['out_layer_dropout_keep_prob']] = self.params[\n                    'out_layer_dropout_keep_prob']\n                fetch_list = [self.ops['loss'], self.ops['train_step'],\n                              self.ops[\"edge_loss\"], self.ops['kl_loss'],\n                              self.ops['node_symbol_prob'], self.placeholders['node_symbols'],\n                              self.ops['qed_computed_values'], self.placeholders['target_values'],\n                              self.ops['total_qed_loss'],\n                              self.ops['mean'], self.ops['logvariance'],\n                              self.ops['grads'], self.ops['mean_edge_loss'], self.ops['mean_node_symbol_loss'],\n                              self.ops['mean_kl_loss'], self.ops['mean_total_qed_loss']]\n            else:\n                batch_data[self.placeholders['out_layer_dropout_keep_prob']] = 1.0\n                fetch_list = [self.ops['mean_edge_loss'], self.ops['accuracy_task0']]\n            result = self.sess.run(fetch_list, feed_dict=batch_data)\n\n            \"\"\"try:\n                if is_training:\n                    self.save_intermediate_results(batch_data[self.placeholders['adjacency_matrix']], \n                        result[11], result[12], result[4], result[5], result[9], result[10], result[6], result[7], result[13], result[14])\n            except IndexError:\n                pass\"\"\"\n\n            batch_loss = result[0]\n            loss += batch_loss * num_graphs\n\n            print(\"Running %s, batch %i (has %i graphs). Loss so far: %.4f\" % (epoch_name,\n                                                                               step,\n                                                                               num_graphs,\n                                                                               loss / processed_graphs), end='\\r')\n        loss = loss / processed_graphs\n        instance_per_sec = processed_graphs / (time.time() - start_time)\n        return loss, instance_per_sec\n\n    def generate_new_graphs(self, data):\n        raise Exception(\"Models have to implement generate_new_graphs!\")\n\n    def train(self):\n        log_to_save = []\n        total_time_start = time.time()\n        with self.graph.as_default():\n            for epoch in range(1, self.params['num_epochs'] + 1):\n                if not self.params['generation']:\n                    print(\"== Epoch %i\" % epoch)\n\n                    train_loss, train_speed = self.run_epoch(\"epoch %i (training)\" % epoch, epoch,\n                                                             self.train_data, True)\n                    print(\"\\r\\x1b[K Train: loss: %.5f| instances/sec: %.2f\" % (train_loss, train_speed))\n\n                    valid_loss, valid_speed = self.run_epoch(\"epoch %i (validation)\" % epoch, epoch,\n                                                             self.valid_data, False)\n\n                    print(\"\\r\\x1b[K Valid: loss: %.5f | instances/sec: %.2f\" % (valid_loss, valid_speed))\n\n                    epoch_time = time.time() - total_time_start\n\n                    log_entry = {\n                        'epoch': epoch,\n                        'time': epoch_time,\n                        'train_results': (train_loss, train_speed),\n                    }\n                    log_to_save.append(log_entry)\n                    with open(self.log_file, 'w') as f:\n                        json.dump(log_to_save, f, indent=4)\n                    self.save_model(str(epoch) + (\"_%s.pickle\" % (self.params[\"dataset\"])))\n                # Run epoches for graph generation\n                if epoch >= self.params['epoch_to_generate']:\n                    self.generate_new_graphs(self.train_data)\n\n    def save_model(self, path: str) -> None:\n        weights_to_save = {}\n        for variable in self.sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n            assert variable.name not in weights_to_save\n            weights_to_save[variable.name] = self.sess.run(variable)\n\n        data_to_save = {\n            \"params\": self.params,\n            \"weights\": weights_to_save\n        }\n\n        with open(path, 'wb') as out_file:\n            pickle.dump(data_to_save, out_file, pickle.HIGHEST_PROTOCOL)\n\n    def initialize_model(self) -> None:\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n        self.sess.run(init_op)\n\n    def restore_model(self, path: str) -> None:\n        print(\"Restoring weights from file %s.\" % path)\n        with open(path, 'rb') as in_file:\n            data_to_load = pickle.load(in_file)\n\n        variables_to_initialize = []\n        with tf.name_scope(\"restore\"):\n            restore_ops = []\n            used_vars = set()\n            for variable in self.sess.graph.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n                used_vars.add(variable.name)\n                if variable.name in data_to_load['weights']:\n                    restore_ops.append(variable.assign(data_to_load['weights'][variable.name]))\n                else:\n                    print('Freshly initializing %s since no saved value was found.' % variable.name)\n                    variables_to_initialize.append(variable)\n            for var_name in data_to_load['weights']:\n                if var_name not in used_vars:\n                    print('Saved weights for %s not used by model.' % var_name)\n            restore_ops.append(tf.variables_initializer(variables_to_initialize))\n            self.sess.run(restore_ops)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Imports\nimport tensorflow as tf\nfrom typing import Sequence, Any\nfrom docopt import docopt\nfrom collections import defaultdict, deque\nimport sys, traceback\nimport pdb\n# from CGVAE.CGVAE import DenseGGNNChemModel\n# from CGVAE.GGNN_core import ChemModel\n# import CGVAE.utils\n# from CGVAE.utils import *\n# from CGVAE.data_augmentation import *\nfrom numpy import linalg as LA\nfrom copy import deepcopy\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Prepare the dataset\ndef train_valid_split(download_path):\n    # load validation dataset\n    with open(\"/kaggle/working/2019-nCov/Data/valid_idx_zinc.json\", 'r') as f:\n        valid_idx = json.load(f)\n\n    print('reading data...')\n    raw_data = {'train': [], 'valid': []} # save the train, valid dataset.\n    with open(download_path, 'r') as f:\n        all_data = list(csv.DictReader(f))\n\n    file_count=0\n    for i, data_item in enumerate(all_data):\n        smiles = data_item['smiles'].strip()\n        QED = float(data_item['qed'])\n        if i not in valid_idx:\n            raw_data['train'].append({'smiles': smiles, 'QED': QED})\n        else:\n            raw_data['valid'].append({'smiles': smiles, 'QED': QED})\n        file_count += 1\n        if file_count % 2000 ==0:\n            print('finished reading: %d' % file_count, end='\\r')\n    return raw_data\n\ndef preprocess(raw_data, dataset):\n    print('parsing smiles as graphs...')\n    processed_data = {'train': [], 'valid': []}\n    \n    file_count = 0\n    for section in ['train', 'valid']:\n        all_smiles = [] # record all smiles in training dataset\n        for i,(smiles, QED) in enumerate([(mol['smiles'], mol['QED']) \n                                          for mol in raw_data[section]]):\n            nodes, edges = to_graph(smiles, dataset)\n            if len(edges) <= 0:\n                continue\n            processed_data[section].append({\n                'targets': [[(QED)]],\n                'graph': edges,\n                'node_features': nodes,\n                'smiles': smiles\n            })\n            all_smiles.append(smiles)\n            if file_count % 2000 == 0:\n                print('finished processing: %d' % file_count, end='\\r')\n            file_count += 1\n        print('%s: 100 %%      ' % (section))\n        # save the dataset\n        with open('/kaggle/working/2019-nCov/Data/molecules_%s_%s.json' % (section, dataset), 'w') as f:\n            json.dump(processed_data[section], f)\n        # save all molecules in the training dataset\n        if section == 'train':\n            CGVAE.utils.dump('/kaggle/working/2019-nCov/Data/smiles_%s.pkl' % dataset, all_smiles)  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path = '/kaggle/working/2019-nCov/Data/250k_rndm_zinc_drugs_clean_3.csv'\nraw_data = train_valid_split(path)\npreprocess(raw_data, 'zinc')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# The various arguments for the CGVAE - see the implementation for explanations.\n# Keep in mind I also trained this model for 10 epochs ahead of time and the weights are found in \n# CGVAE/10_zinc.pickle (remove from arguments blow if you want to train it yourself!) - you can\n# more parameters and defaults are found in CGVAE.py\nargs = {'--config': None,\n '--config-file': None,\n '--data_dir': '/kaggle/working/2019-nCov/Data/',\n '--dataset': 'zinc',\n '--freeze-graph-model': False,\n '--help': False,\n '--log_dir': '/kaggle/working/2019-nCov/CGVAE/',\n '--restore': '/kaggle/working/2019-nCov/CGVAE/10_zinc.pickle'}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Implementation here is quit straightforawrd due to some lucky implementation choices by the original authors\n# I highly recommend checking out their implementation in the CGVAE/ directory, it's great stuff!\n# Also, I only have a graphics card able to do a mini-batch size of ONE! so be sure to raise that if\n# you've got a better card!\n# Also, model hyperparameters can be found in \nmodel = DenseGGNNChemModel(args)\nmodel.train()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 6. Validation - Molecular Docking Studies Using Autodock Vina"},{"metadata":{},"cell_type":"markdown","source":"### Re-docking the N3 Ligand as a baseline"},{"metadata":{},"cell_type":"markdown","source":"First step in the validation of proposed structures is to re-dock the N3 ligand into the protease structure, to get a baseline for the energy score associated with their binding. It is important to note that the N3 ligand shown in the X-Ray structure is a covalent inhibitor, which means it actually reacts with the active site of the protein. This results in a much stronger bond between ligand and target than non-covalent inhibition. \n\nThis means that the re-docked structure may not be the same as the x-ray, since it lacks the covalent bond to the protein. The following in a procedure for the docking of the N3 ligand into the protein receptor. The same procedure is used for all fo the alter dockings of the candidate molecules. The procedure requires 3 programs:\n\nPymol - https://pymol.org/2/ or https://github.com/schrodinger/pymol-open-source\n\nAutodock Vina - http://vina.scripps.edu/download.html\n\nAutodock Tools, found in MGL tools - http://mgltools.scripps.edu/downloads\n\nTwo excellent video tutorials are found here: https://youtu.be/-GVZP0X0Tg8 and https://youtu.be/blxSn3Lhdec"},{"metadata":{},"cell_type":"markdown","source":"Docking procedure with Autodock Vina:\n\n1. Open the structure of the protein and ligand complex (.cif crystallographic information file)\n\n2. Select the ligand chain (in the bottom right, click \"residues\" so that it swtiches to \"chains\" to be able to select a chain)\n\n3. Delete the ligand, and save the file as a .pdb\n\n4. re-load the original file and this time select the protein and delete, saving only the ligand, also as a .pdb file\n\n5. Open autodock tools, load the protein target molecule with File>Read Molecule (.pdb file)\n\n6. Add hydrogens (Edit>Hydrogens>Add>Polar_only>Okay)\n\n7. View Mol surface (mention binding site)\n\n8. Select the 10 residues involved in the binding site: \n    THR26,LEU27,HIS41,MET49,ASN142,CYS145, HIS163\n\tGLU166, HIS172, GLN189\n    \n9. Go to Grid>Gridbox and show the gridbox, then manipulate it by changing the center and size so that it's completely enclosing the selected sidechains. Remember the coordinates. They should be: center: x=-11.963,y=15.683,z=69.193, spacing: 1A, points: x=20, y=24, z=22\n\n9. Flexible Residues>choose molecule\n\n10. Flexible Residues>Choose Torsions in selected residues> then accept the defaults. Should see various bonds on the 10 selected residues be different colours. THIS IS A VERY IMPORTANT STEP - NOT CONSIDERING FLEXIBILITY IN THE PROTEIN WILL AFFECT ACCURACY OF THE SCORING\n\n11. Flexible Residues>Output>SaveRigid. Save the rigid part of the protein as a .pdbqt file\n\n12. Flexible Residues>Output>Saveflexible. Save the flexible part of the protein as a .pdbqt file\n\n13. Now delete or hide the receptor and load the ligand with Ligand>input>open>ligand.pdb>ok\n\n14. add hydrogens to the ligand edit>hydrogens>add>polar_only\n\n15. export this as a pdb file (with hydrogens now)\n\n16. re-load the updated pdb file\n\n17. Define the rotatable bonds Ligand>TorsionTree>ChooseTorsions>okay\n\n18. Ligand>Output> Save as PDBQT\n\n19. Close autodock tools\n\n20. Create a configuation file for vina that matches the structure of conf.txt found in the Docking/ directory of this repo. Exhaustiveness is proportional to time and is how thoroughly the conformational space is searched.\n21. Run vina using ./vina --config conf.txt"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Visualizing the N3 ligand\nChem.MolFromSmiles(\"CC(C)C[C@H](NC(=O)[C@@H](NC(=O)[C@H](C)NC(=O)c1cc(C)on1)C(C)C)C(=O)N[C@@H](C[C@@H]2CCNC2=O)\\C=C/C(=O)OCc3ccccc3\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Following this procedure for the N3 ligand, we end up with a final lowest energy minimum of around -7.9kcal/mol. The exact value doesn't tell us much, because the specific parameters of the docking scoring function can vary, but this serves as a baseline for comparison of later candidates. The following is the lowest energy stucture. You can see that it is in fact very different from the X-ray structure due to the lack of the covalent bond to the protein, with the N3 ligand sort of \"bending back\" in this conformation"},{"metadata":{},"cell_type":"markdown","source":"Now for docking the candidates. The same procedure as above was followed for each of the candidates, with the additional step below of loading the structures and saving them as PDB files, to be opened in AutoDockTools"},{"metadata":{},"cell_type":"markdown","source":"### Preparing the high scoring and generated compounds for docking"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_predicted = pickle.load(open(\"/kaggle/working/2019-nCov/Data/best_predicted_smiles.pkl\", \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_predicted_mols = [Chem.MolFromSmiles(x) for x in best_predicted]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdkit.Chem.Draw.MolsToGridImage(best_predicted_mols, molsPerRow=2, maxMols=100, subImgSize=(800, 800))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These compounds are visually very similar to the N3 ligand (see visualization above). Maybe searching for similar compounds was a poor move."},{"metadata":{"trusted":true},"cell_type":"code","source":"def write_to_pdb(m, name):\n    m = Chem.AddHs(m)\n    Chem.EmbedMolecule(m)\n    w = Chem.rdmolfiles.PDBWriter(open(\"/kaggle/working/2019-nCov/Docking/\"+ str(name) + \".pdb\", \"w\"))\n    w.write(m)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"for i in range(len(best_predicted_mols)):\n    write_to_pdb(best_predicted_mols[i], \"bp_\" + str(i+1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"generated = pickle.load(open(\"/kaggle/working/2019-nCov/Data/first_generated_smiles_zinc\",\n                             \"rb\"))\ngenerated = [Chem.MolFromSmiles(x) for x in generated]\nrdkit.Chem.Draw.MolsToGridImage(generated, molsPerRow=2, maxMols=100, subImgSize=(800, 800))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The generated compounds from this method are VERY strange for the most part, many with large strange rings. This is interesting because this papaer is a realtively early example of generating molecular graphs and in the past little while have used a penalty on large rings such as the ones seen in these compounds. However, all is not lost, because several of these compounds still have interesting structur and are small and comparable to the N3 ligand."},{"metadata":{},"cell_type":"markdown","source":"In these 50 generated compounds, the ones that appeared at least the most visually similar to the n3 ligand (mainly just the small ones, which there aren't many) are: indexes: [5,6,9,32,33,34,36,38,44]"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"for i in [5,6,9,32,33,34,36,38,43,44]:\n    write_to_pdb(generated[i], str(i))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"After being prepared, the ligands were docked using autodock vina and the script multi_dock.sh to automate the process of docking many compounds. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Preparation of the config files for the command line running of autodock vina\n# this is only for the \"best predicted\" compounds - I already docked the other ones before\n# I made this script but they could easily be changed.\nnames = [\"bp_1\", \"bp_2\", \"bp_3\", \"bp_4\", \"bp_5\",\"bp_6\", \"bp_7\", \"bp_8\", \"bp_9\", \"bp_10\"]\nfor name in names:\n    f = open(\"/kaggle/working/2019-nCov/Docking/conf_\" + name + \".txt\", \"w+\")\n    f.write(\"receptor = /u/macdougt/Research/2019-nCov/Docking/6LU7_receptor_rigid.pdbqt\\n\")\n    f.write(\"flex = /u/macdougt/Research/2019-nCov/Docking/6LU7_receptor_flex.pdbqt\\n\")\n    f.write(\"ligand = /u/macdougt/Research/2019-nCov/Docking/\" + name + \".pdbqt\\n\")\n\n    f.write(\"out = /u/macdougt/Research/2019-nCov/Docking/out_\" + name + \".pdbqt\\n\")\n    f.write(\"log = /u/macdougt/Research/2019-nCov/Docking/log_\" + name + \".txt\\n\")\n\n    f.write(\"center_x = -11.963\\n\")\n    f.write(\"center_y = 15.683\\n\")\n    f.write(\"center_z = 69.193\\n\")\n\n    f.write(\"size_x = 20\\n\")\n    f.write(\"size_y = 24\\n\")\n    f.write(\"size_z = 22\\n\")\n\n    f.write(\"exhaustiveness = 80\\n\")\n\n    f.write(\"cpu = 7\\n\")\n    f.close()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Scores of various compounds"},{"metadata":{},"cell_type":"markdown","source":"### best predicted compounds"},{"metadata":{"trusted":true},"cell_type":"code","source":"rdkit.Chem.Draw.MolsToGridImage(best_predicted_mols, molsPerRow=2, maxMols=100, subImgSize=(800, 800), legends=[\"-8.2\", \"-7.5\", \"-8.0\", \"-7.6\", \"-8.5\", \"-7.5\", \"-8.2\", \"-8.7\", \"-7.6\", \"-8.4\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"For the generated compounds"},{"metadata":{},"cell_type":"markdown","source":"## docking score generated"},{"metadata":{"trusted":true},"cell_type":"code","source":"docked_generated = [generated[i] for i in [5,6,9,32,33,34,36,38,43,44]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rdkit.Chem.Draw.MolsToGridImage(docked_generated, molsPerRow=2, maxMols=100, subImgSize=(800, 800), legends=[\"-7.3\", \"-8.2\", \"-8.8\", \"-9.8\", \"-8.4\", \"-6.9\", \"-7.6\", \"-7.9\", \"-6.7\", \"-9.4\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Chosing the best scoring compound from these two scemes, we get the following compound from the prediction method, with a score of -8.7kcal/mol"},{"metadata":{"trusted":true},"cell_type":"code","source":"best_predicted_mols[7]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The best one from the generative mthod is shown below, with a score of -9.8 kcal/mol"},{"metadata":{"trusted":true},"cell_type":"code","source":"generated[32]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing the high scoring compounds in the active site"},{"metadata":{},"cell_type":"markdown","source":"The following is the highest scoring predicted compound mentioned above."},{"metadata":{},"cell_type":"markdown","source":"The following is the highest scoring generated compound mentioned above."},{"metadata":{},"cell_type":"markdown","source":"## Discussion and Conclusion"},{"metadata":{},"cell_type":"markdown","source":"A predictive deep learning model was trained on a self-generated set of protease inhibitors, and the the pubchem literature was searched for 183 compounds that are somilar to the n3 ligand. Predictions were made on these compounds and those with the 10 best predictive scores were docked to the ligand. The highest scoring compound is shown above and has a score of -8.7kcal/mol\n\nA generative deep learning model was trained on a self-generated set of protease inhibitors, and 50 new compounds were sampled from the latent space of the model. The 10 most promising compounds were docked to the ligand. The highest scoring compound is shown above and has a score of -9.8kcal/mol\n\nThe best compounds from each method show signicant gains over the baseline score of -7.9kcal/mol for the n3 ligand.\n\nI would say that the high scoring compound from the predictive model should be investigated first, because since it was predicted using a test test of compounds from pubchem, this means that it is a chemically feasible compound, which is very important, which means that it could be obtained or made quickly, to be used right away. The generated compound did have a higher binding score, but it's a generated compound that might be difficult to make, even for an experienced chemist, it's difficult to say. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"nCov_env","language":"python","name":"ncov_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.6"}},"nbformat":4,"nbformat_minor":4}